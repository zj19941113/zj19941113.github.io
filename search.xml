<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[基于RGBD相机的人脸实时三维重建过程解析（ElasticFusion实现）]]></title>
    <url>%2F2019%2F06%2F17%2F18%2F</url>
    <content type="text"><![CDATA[主要是在ElasticFusion基础上做了部分修改，进行人脸实时重建。PPT及相关论文地址：https://pan.baidu.com/s/1Zr1uCFj7YqgEWWof2gp-Lw ，提取码：7fnw相关算法实现链接：BundleFusion在win10+vs2013+cuda8.0上的实现DynamicFusion在ubuntu16.04+显卡GT740+cuda8.0上的实现ElasticFusion在ubuntu16.04+cuda8.0上的实现 参考链接：KinectFusion 解析KinectFusion 论文精析kinect fusion 3D重建基本算法KinectFusion中用到的TSDF FusionBundleFusion 解析ElasticFusion 解析 （这个博主其他文章也非常值得参考）ElasticFusion]]></content>
      <categories>
        <category>三维重建</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>ElasticFusion</tag>
        <tag>RGBD</tag>
        <tag>人脸实时三维重建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vue+iview+Echarts+electron 桌面应用]]></title>
    <url>%2F2019%2F04%2F29%2F17%2F</url>
    <content type="text"><![CDATA[一、Vue项目创建1、安装node.js#下载安装包：https://nodejs.org/en/cmd输入node -v 和npm -v查看是否安装成功 2、Vue开发环境#命令行下载淘宝镜像命令工具 cnpm 1npm install cnpm -g --registry=http://registry.npm.taobao.org #用 cnpm 命令全局安装vue-cli脚手架 1cnpm install --global vue-cli 输入vue，出来vue的信息说明安装成功 3、搭建Vue项目（1）项目初始化 使用命令创建项目，一步步选择之后开始等等项目创建完成 1vue init webpack app （2）项目预览12cd demonpm run dev 打开浏览器，复制粘贴命令行显示的地址（ http://localhost:8080），跳出页面 二、引入ivew+Echarts1、ivew+Echarts安装1cnpm install ivew -save 1cnpm install echarts -save 2、引入ivew+Echarts项目src/main.js中引入ivew+Echarts 1234import iView from 'iview';import 'iview/dist/styles/iview.css';Vue.use(iView); 12import Echarts from 'echarts'Vue.prototype.echarts = Echarts 3、Echarts组件将src/components/HelloWorld修改为如下内容 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192&lt;style scoped&gt;.layout&#123; border: 1px solid #d7dde4; background: #f5f7f9; position: relative; border-radius: 4px; overflow: hidden;&#125;.layout-logo&#123; width: 100px; height: 30px; background: #5b6270; border-radius: 3px; float: left; position: relative; top: 15px; left: 20px;&#125;.layout-nav&#123; width: 420px; margin: 0 auto; margin-right: 20px;&#125;&lt;/style&gt;&lt;template&gt; &lt;div class="layout"&gt; &lt;Layout&gt; &lt;Header&gt; &lt;Menu mode="horizontal" theme="dark" active-name="1"&gt; &lt;div class="layout-logo"&gt;&lt;/div&gt; &lt;div class="layout-nav"&gt; &lt;MenuItem name="1"&gt; &lt;Icon type="ios-navigate"&gt;&lt;/Icon&gt; Item 1 &lt;/MenuItem&gt; &lt;MenuItem name="2"&gt; &lt;Icon type="ios-keypad"&gt;&lt;/Icon&gt; Item 2 &lt;/MenuItem&gt; &lt;MenuItem name="3"&gt; &lt;Icon type="ios-analytics"&gt;&lt;/Icon&gt; Item 3 &lt;/MenuItem&gt; &lt;MenuItem name="4"&gt; &lt;Icon type="ios-paper"&gt;&lt;/Icon&gt; Item 4 &lt;/MenuItem&gt; &lt;/div&gt; &lt;/Menu&gt; &lt;/Header&gt; &lt;Layout&gt; &lt;Sider hide-trigger :style="&#123;background: '#fff'&#125;"&gt; &lt;Menu active-name="1-2" theme="light" width="auto" :open-names="['1']"&gt; &lt;Submenu name="1"&gt; &lt;template slot="title"&gt; &lt;Icon type="ios-navigate"&gt;&lt;/Icon&gt; Item 1 &lt;/template&gt; &lt;MenuItem name="1-1"&gt;Option 1&lt;/MenuItem&gt; &lt;MenuItem name="1-2"&gt;Option 2&lt;/MenuItem&gt; &lt;MenuItem name="1-3"&gt;Option 3&lt;/MenuItem&gt; &lt;/Submenu&gt; &lt;Submenu name="2"&gt; &lt;template slot="title"&gt; &lt;Icon type="ios-keypad"&gt;&lt;/Icon&gt; Item 2 &lt;/template&gt; &lt;MenuItem name="2-1"&gt;Option 1&lt;/MenuItem&gt; &lt;MenuItem name="2-2"&gt;Option 2&lt;/MenuItem&gt; &lt;/Submenu&gt; &lt;Submenu name="3"&gt; &lt;template slot="title"&gt; &lt;Icon type="ios-analytics"&gt;&lt;/Icon&gt; Item 3 &lt;/template&gt; &lt;MenuItem name="3-1"&gt;Option 1&lt;/MenuItem&gt; &lt;MenuItem name="3-2"&gt;Option 2&lt;/MenuItem&gt; &lt;/Submenu&gt; &lt;/Menu&gt; &lt;/Sider&gt; &lt;Layout :style="&#123;padding: '0 24px 24px'&#125;"&gt; &lt;Breadcrumb :style="&#123;margin: '24px 0'&#125;"&gt; &lt;BreadcrumbItem&gt;Home&lt;/BreadcrumbItem&gt; &lt;BreadcrumbItem&gt;Components&lt;/BreadcrumbItem&gt; &lt;BreadcrumbItem&gt;Layout&lt;/BreadcrumbItem&gt; &lt;/Breadcrumb&gt; &lt;Content :style="&#123;padding: '24px', minHeight: '280px', background: '#fff'&#125;"&gt; &lt;div id="echart" style="width: 940px;height:500px;"&gt;&lt;/div&gt; &lt;/Content&gt; &lt;/Layout&gt; &lt;/Layout&gt; &lt;/Layout&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt; import echarts from 'echarts' export default &#123; data() &#123; return &#123;&#125; &#125;, mounted() &#123; // 基于准备好的dom，初始化echarts实例 var myChart = echarts.init(document.getElementById('echart')); // 绘制图表 var option = &#123; title : &#123; text: '保有储量变化图', subtext: '一次能源' &#125;, tooltip : &#123; trigger: 'axis' &#125;, legend: &#123; data:['煤','石油','天然气'] &#125;, toolbox: &#123; show : true, feature : &#123; dataView : &#123;show: true, readOnly: false&#125;, magicType : &#123;show: true, type: ['line', 'bar']&#125;, restore : &#123;show: true&#125;, saveAsImage : &#123;show: true&#125; &#125; &#125;, calculable : true, xAxis : [ &#123; type : 'category', data : ['2009','2010','2011','2012','2013','2014','2015','2016','2017'], splitNumber: 10 &#125; ], yAxis : [ &#123; type : 'value' &#125; ], series : [ &#123; name:'煤', type:'bar', data:[1683.47,1654.23,1640.12,1641.6,1639.68,1642.7,1610.41,1639.45,1722.2], markPoint : &#123; data : [ &#123;type : 'max', name: '最大值'&#125;, &#123;type : 'min', name: '最小值'&#125; ] &#125;, markLine : &#123; data : [ &#123;type : 'average', name: '平均值'&#125; ] &#125; &#125;, &#123; name:'石油', type:'bar', data:[22490.2,24947.67,29844.34,31397.94,33713,36300.8,38445.3,38375.6,38158.7], markPoint : &#123; data : [ &#123;type : 'max', name: '最大值'&#125;, &#123;type : 'min', name: '最小值'&#125; ] &#125;, markLine : &#123; data : [ &#123;type : 'average', name : '平均值'&#125; ] &#125; &#125;, &#123; name:'天然气', type:'bar', data:[5502.54,5628.11,5478,6376.26,6231.14,8047.88,7857.1,7802.5,8695.01], markPoint : &#123; data : [ &#123;type : 'max', name: '最大值'&#125;, &#123;type : 'min', name: '最小值'&#125; ] &#125;, markLine : &#123; data : [ &#123;type : 'average', name : '平均值'&#125; ] &#125; &#125; ] &#125;; myChart.setOption(option); &#125; &#125; &lt;/script&gt; 1npm run dev 打开http://localhost:8080 ，显示页面 4、生成项目执行生成命令 1npm run build 出现如下效果项目下回多出一个dist的文件夹，里面是打包好的东西，双击index.html显示网页内容则证明生成成功 （1）打包页面显示空白修改index.js 中的生成路径，第46行改为assetsPublicPath: &#39;./&#39;,，否则后面程序运行打包页面会显示空白，找不到路径 （2）页面部分空白，router-view内容无法显示// mode: ‘history’,//将这个模式关闭就好 （3）iview 打包之后，图标无法显示在 build/webpack.prod.conf.js 中,将extract改为false接下来一切操作都在dist文件夹目录下。 三、electron打包1、安装 electron用 cnpm 命令安装 electron 1cnpm install electron -g cmd输入electron -v查看是否安装成功 2、 package.json 和 main.js在dist文件夹下，复制粘贴下面的 package.json 和 main.js文件，最终目录如图在package.json中： 12345&#123; "name" : "your-app", "version" : "0.1.0", "main" : "main.js"&#125; 在main.js中 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455const &#123;app, BrowserWindow&#125; = require('electron')const path = require('path')const url = require('url')// Keep a global reference of the window object, if you don't, the window will// be closed automatically when the JavaScript object is garbage collected.let winfunction createWindow () &#123; // Create the browser window. win = new BrowserWindow(&#123;width: 800, height: 600&#125;) // and load the index.html of the app. win.loadURL(url.format(&#123; pathname: path.join(__dirname, 'index.html'), protocol: 'file:', slashes: true &#125;)) // Open the DevTools. // win.webContents.openDevTools() // Emitted when the window is closed. win.on('closed', () =&gt; &#123; // Dereference the window object, usually you would store windows // in an array if your app supports multi windows, this is the time // when you should delete the corresponding element. win = null &#125;)&#125;// This method will be called when Electron has finished// initialization and is ready to create browser windows.// Some APIs can only be used after this event occurs.app.on('ready', createWindow)// Quit when all windows are closed.app.on('window-all-closed', () =&gt; &#123; // On macOS it is common for applications and their menu bar // to stay active until the user quits explicitly with Cmd + Q if (process.platform !== 'darwin') &#123; app.quit() &#125;&#125;)app.on('activate', () =&gt; &#123; // On macOS it's common to re-create a window in the app when the // dock icon is clicked and there are no other windows open. if (win === null) &#123; createWindow() &#125;&#125;)// In this file you can include the rest of your app's specific main process// code. You can also put them in separate files and require them here. 运行 1electron . 3、electron-packager打包（1）全局安装electron-packager1npm install electron-packager -g （2）运行打包命令1electron-packager . demo --win --out outApp --arch=x64 --app-version 1.0.0 --electron-version 5.0.0 --overwrite --ignore=node_modules 在dist\outApp\demo-win32-x64文件夹下生成可执行文件demo.exe,双击运行程序嫌每次输入太长，可以将在package.json中添加 1234"scripts": &#123; "start": "electron .", "build": "electron-packager . myApp --win --out outApp --arch=x64 --app-version 1.0.0 --electron-version 5.0.0 --overwrite --ignore=node_modules" &#125; 这样输入npm run start相当于执行了electron .,测试打包效果输入npm run build相当于执行了打包命令，在dist\outApp\myApp-win32-x64文件夹下生成可执行文件myApp.exe 4、源码加密在outApp\demo-win32-x64\resources\app里有写的源码。写的代码完全暴露在用户电脑上是非常不安全的，可以通过electron 自带的加密功能解决这个问题。 （1）全局安装 asar1npm install asar -g （2）使用asar指令进行加密在resources目录下使用asar指令进行加密 1asar pack ./app app.asar 将原来的app文件夹删除，这样生成的app.asar就加密了之前的源代码双击demo.exe重新运行程序]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>electron</tag>
        <tag>Vue</tag>
        <tag>iview</tag>
        <tag>Echarts</tag>
        <tag>桌面应用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一步步配置腾讯云服务器Ubuntu 通过域名访问自己的网页tomcat（详细基础）]]></title>
    <url>%2F2019%2F04%2F26%2F22%2F</url>
    <content type="text"><![CDATA[STEP1:云服务器+域名1、购买云服务器、域名购买学生优惠套餐 10元/月：https://cloud.tencent.com/act/campus?fromSource=gwzcw.1088196.1088196.1088196系统选ubuntu，购买3个月加购域名共38元（3个月以下没法添加域名解析），买了域名以后可以直接通过域名访问 收到服务器信息，最好把这些复制到一个txt里，以后会经常用。 2、添加解析在控制台进入云解析，添加解析，填域名，点新手快速添加， 网站解析，输入公网ip（无需备案）完成后： 3、设置安全组控制台-云服务器-安全组-新建，选择 放通22，80，443，3389端口和ICMP协议，新建安全组点击关联实例数，新增关联与自己的服务器进行关联，点击修改规则，放通8080端口（tomcat要用） STEP2:连接服务器，配置JDK+tomcat1、连接服务器下载安装PuTTY：https://www.chiark.greenend.org.uk/~sgtatham/putty/latest.html下载安装WinSCP：https://winscp.net/eng/download.php （1）PuTTY连接服务器输入公网ip输入默认账户ubuntu，输入初始密码**，登陆成功如上图。 putty的复制粘贴技巧复制密码后，在putty直接右键就是粘贴，密码不显示注意不要多复制。（如果想复制putty中的文字，左键选取就已经复制上了） 创建root账号1、设置root密码sudo passwd root输入密码2、获取root权限输入su输入密码以后再次登录，输入su再输入密码就可以了，成功后： root@VM-0-16-ubuntu:/home/ubuntu# （2）winSCP连接服务器主机名为公网ip，点击登录 尝试新建文件夹test，提示permission denied 使用root登录winSCP：打开winSCP填写登录信息 ，点击高级，设置SFTP如下：sudo /usr/lib/openssh/sftp-server重新登录后，就可以将本地文件拖到服务器中了。 2、配置JDK+tomcat（ubuntu云服务器）下载jdk8：http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html下载Tomcat：https://tomcat.apache.org/download-90.cgiroot登录winSCP后，将下载的 jdk-8u191-linux-x64.tar.gz 和 jdk-11.0.1_linux-x64_bin.tar.gz拖到/usr/local/src/目录下 （1）安装JDK1.8root登录putty后，输入cd /usr/local/src/ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // 进入存放安装包的位置tar zxvf jdk-8u144-linux-x64.tar.gz &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // 解压下载的数据包mv jdk1.8.0_144 /usr/local/jdk1.8 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // 给解压完成后的数据包更换一个其他目录并且改名vi /etc/profile &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // 编辑/etc/profile 环境变量文件在最后面添加 JAVA_HOME=/usr/local/jdk1.8JAVA_BIN=/usr/local/jdk1.8/binJRE_HOME=/usr/local/jdk1.8/jrePATH=$PATH:/usr/local/jdk1.8/bin:/usr/local/jdk1.8/jre/binCLASSPATH=/usr/local/jdk1.8/jre/lib:/usr/local/jdk1.8/lib:/usr/local/jdk1.8/jre/lib/charsets.jar vi使用技巧输入 i ，退出命令模式，进入INSERT模式开始修改内容……按 esc 键，退出INSERT模式，进入命令模式再输入 :wq，保存文件，退出vi编辑器 键 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 功能:wq &nbsp;&nbsp;&nbsp;&nbsp; 保存文件，退出vi编辑器:w &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 保存文件，但不退出vi编辑器:q &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 退出vi编辑器:q! &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 不保存文件，退出vi编辑器ZZ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 保存文件，退出vi编辑器或者在winSCP中直接打开/etc/profile文件，修改后保存。 putty中输入source /etc/profile &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // source重启环境变量配置文件java -version &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // 检测jdk环境是否配置好注：每次登录，碰到java -version不显示版本号就，先source /etc/profile，再java -version，就好了。 （2）安装Tomcat9root登录putty后，输入cd /usr/local/src/ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // 进入存放安装包的位置tar zxvf apache-tomcat-9.0.14.tar.gz &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // 解压下载的数据包mv apache-tomcat-9.0.14 /usr/local/tomcat &nbsp;&nbsp; // 给解压完成后的数据包更换一个其他目录并且改名/usr/local/tomcat/bin/startup.sh &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; // 启动Tomcat Tomcat启用成功了显示： Using CATALINA_BASE: /usr/local/tomcatUsing CATALINA_HOME: /usr/local/tomcatUsing CATALINA_TMPDIR: /usr/local/tomcat/tempUsing JRE_HOME: /usr/local/jdk1.8Using CLASSPATH: /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jarTomcat started. 如果要关闭Tomcat/usr/local/tomcat/bin/shutdown.sh启动Tomcat后，确定服务器8080端口放通，则在浏览器输入 服务器公网ip:8080，一只熟悉的猫出现了~ 注：如果添加了域名解析，可以用域名代替ip STEP3:本地网页布置到tomcat希望ip:8080访问自己的网页首先有自己的网页，如自己写的跳一跳网页，下载地址：https://github.com/zj19941113/You_Jump_I_Jump将项目文件夹zjgame放到tomcat/webapps/文件夹中，从ROOT中将WEB-INF文件夹复制到zjgame，访问ip:8080/zjgame/zjtest.html，就能看到项目了如果添加了域名解析，可以访问 域名:8080/zjgame/zjtest.html （1）去掉项目名a. 将zjgame中的网页和资源文件复制到ROOT中，zjtest.html重命名为index.html浏览器输入 ip:8080 就能访问了或者 b. 修改 /usr/local/tomcat/conf/server.xml 文件在&lt;Host&gt;&lt;/Host&gt;里加上 &lt;Context path=”” docBase=”项目名” reloadable=”true” /&gt; （2）去掉:8080修改 /usr/local/tomcat/conf/server.xml 文件，将默认端口port=”8080”改为”80” Connector port=”80” protocol=”HTTP/1.1” connectionTimeout=”20000” redirectPort=”8443” /&gt; 关闭Tomcat/usr/local/tomcat/bin/shutdown.sh过一会再重启/usr/local/tomcat/bin/startup.sh因为之前已经进行过域名解析，所以直接访问 http://zhoujie1994.cn 就能看到自己的网页了备案后不会再提示不安全：https://zhoujie1994.cn/three/]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>腾讯云服务器</tag>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kinect v1实时dlib(GPU版)人脸识别与活体检测]]></title>
    <url>%2F2019%2F04%2F22%2F16%2F</url>
    <content type="text"><![CDATA[一、配置1、opencv#安装依赖 12sudo apt-get install libvtk5-devsudo apt-get install libgtk2.0-dev #安装OpenCV 2.4.13 1234567git clone https://github.com/opencv/opencvcd opencv/git checkout 2.4.13.3mkdir -p build &amp;&amp; cd buildcmake -DWITH_VTK=ON -DWITH_GTK=ON -DBUILD_opencv_calib3d=ON -DBUILD_opencv_imgproc=ON -DWITH_CUDA=OFF ..make -j4sudo make install 注：opencv的下载过程非常慢 2、dlib（GPU版）需要提前安装驱动、cuda 与 cudnn进入官网：http://dlib.net/ ，点击左下角Download dlib ver.19.17 ，下载后解压。进入dlib根目录下 1234mkdir build &amp;&amp; cd buildcmake .. cmake --build . --config Releasesudo make install 注：会自动检查满足安装gpu版条件，注意命令行提示信息 二、Kinect v1接入1、安装OpenNI2123git clone https://github.com/occipital/OpenNI2.gitcd OpenNI2make -j4 2、安装libFreenect12git clone https://github.com/OpenKinect/libfreenectcd libfreenect 打开libfreenect/CMakeLists.txt，在33行cmake_minimum_required(VERSION 2.8.12)下一行添加 1add_definitions(-std=c++11) 保存后关闭，命令行继续执行 1234mkdir build &amp;&amp; cd build cmake .. -DBUILD_OPENNI2_DRIVER=ON make -j4cp -L lib/OpenNI2-FreenectDriver/libFreenectDriver.so $&#123;OPENNI2_DIR&#125;/Bin/x64-Release/OpenNI2/Drivers #OPENNI2_DIR需要修改 注：${OPENNI2_DIR}是OpenNI2的解压文件夹，比如我的在ElasticFusion文件夹，则打开libfreenect文件夹，运行 1sudo cp platform/linux/udev/51-kinect.rules /etc/udev/rules.d 重启后，插上kinect后，命令行运行clsusb，查看是否包含：Xbox camera，Xbox motor，Xbox audio 二、代码运行1、CMakeList.txt修改#CMakeList.txt 1234567891011121314151617181920212223242526272829303132333435363738cmake_minimum_required(VERSION 2.8.4) project(face_dlib) SET(CMAKE_CXX_FLAGS "$&#123;CMAKE_CXX_FLAGS&#125; -O2 -DDLIB_JPEG_SUPPORT")IF(CMAKE_CXX_COMPILER_ID STREQUAL "Clang") SET(CMAKE_CXX_FLAGS "$&#123;CMAKE_CXX_FLAGS&#125; -Weverything")ELSEIF(CMAKE_CXX_COMPILER_ID STREQUAL "GNU") SET(CMAKE_CXX_FLAGS "$&#123;CMAKE_CXX_FLAGS&#125; -Wall -Wextra")ENDIF()# opencvfind_package(OpenCV REQUIRED)include_directories($&#123;OpenCV_INCLUDE_DIRS&#125;)# OpenNI2set(OPENNI2_PATH "/home/zhoujie/fusion/ElasticFusion/OpenNI2") set(OPENNI2_INCLUDE_DIR $&#123;OPENNI2_PATH&#125;/Include)set(OPENNI2_LIBRARY $&#123;OPENNI2_PATH&#125;/Bin/x64-Release)include_directories ($&#123;OPENNI2_INCLUDE_DIR&#125;)link_directories ($&#123;OPENNI2_LIBRARY&#125;)# dlibinclude(/home/zhoujie/dlib-19.17/dlib/cmake)add_executable(this_is_who_kinect_one src/this_is_who_kinect_one.cpp)target_link_libraries(this_is_who_kinect_one dlib $&#123;OpenCV_LIBS&#125; libOpenNI2.so)macro(add_face_dlib name) add_executable($&#123;name&#125; src/$&#123;name&#125;.cpp) target_link_libraries($&#123;name&#125; dlib )endmacro()add_face_dlib(train_candidate)add_face_dlib(this_is_who) 将OPENNI2_PATH与dlib路径修改为自己的。 2、数据+模型下载下载 shape_predictor_68_face_landmarks.dat 和 dlib_face_recognition_resnet_model_v1.dat 放置到model文件夹中 链接：https://pan.baidu.com/s/1jIoW6BSa5nkGWNipL7sxVQ其中包括： candidate-face.zip（人脸库：包含29个正面人脸红外图） allface.zip（测试人脸集：包括29个人，每人13种脸部姿态下的红外图与深度图） shape_predictor_68_face_landmarks.dat（人脸68关键点检测器） dlib_face_recognition_resnet_model_v1.dat（人脸识别模型） 3、代码运行将train_candidate.cpp 、this_is_who 和 this_is_who_kinect_one.cpp置于src文件夹中。 #build 123mkdir build &amp;&amp; cd buildcmake ..make -j4 cmake时检查是否适用dlib(GPU)版最终在build文件夹中生成多个可执行文件将人脸图片重命名为人的名字，放在data文件夹中。 1、运行train_candidate 获得人脸库特征信息，存储在candidates_descriptors.dat 与 candidates.dat 中，同时生成candidates.txt，便于查看候选人信息。每次修改人脸库，只需运行train_candidate，完成人脸信息的更新。 2、运行this_is_who_kinect_one,实时获取Kinect深度图与彩色图，并进行人脸识别与活体检测。3、 不使用kinect v1实时接入，在离线数据上测试，运行this_is_who,使用data/allface中的深度图与红外图，并进行人脸识别与活体检测。代码部分： #train_candidate 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137#include &lt;dlib/dnn.h&gt;#include &lt;dlib/image_processing/frontal_face_detector.h&gt;#include &lt;dlib/image_processing.h&gt;#include &lt;dlib/gui_widgets.h&gt;#include &lt;dlib/image_io.h&gt;#include &lt;iostream&gt;#include &lt;fstream&gt;#include &lt;time.h&gt;#include &lt;dirent.h&gt;#include &lt;string.h&gt;using namespace dlib;using namespace std;/* 函数声明 *//* 人脸库训练 */int candidates_train(const char *facesFile,std::vector&lt;matrix&lt;float,0,1&gt;&gt;&amp;candidates_descriptors,std::vector&lt;string&gt;&amp;candidates);template &lt;template &lt;int,template&lt;typename&gt;class,int,typename&gt; class block, int N, template&lt;typename&gt;class BN, typename SUBNET&gt;using residual = add_prev1&lt;block&lt;N,BN,1,tag1&lt;SUBNET&gt;&gt;&gt;;template &lt;template &lt;int,template&lt;typename&gt;class,int,typename&gt; class block, int N, template&lt;typename&gt;class BN, typename SUBNET&gt;using residual_down = add_prev2&lt;avg_pool&lt;2,2,2,2,skip1&lt;tag2&lt;block&lt;N,BN,2,tag1&lt;SUBNET&gt;&gt;&gt;&gt;&gt;&gt;;template &lt;int N, template &lt;typename&gt; class BN, int stride, typename SUBNET&gt; using block = BN&lt;con&lt;N,3,3,1,1,relu&lt;BN&lt;con&lt;N,3,3,stride,stride,SUBNET&gt;&gt;&gt;&gt;&gt;;template &lt;int N, typename SUBNET&gt; using ares = relu&lt;residual&lt;block,N,affine,SUBNET&gt;&gt;;template &lt;int N, typename SUBNET&gt; using ares_down = relu&lt;residual_down&lt;block,N,affine,SUBNET&gt;&gt;;template &lt;typename SUBNET&gt; using alevel0 = ares_down&lt;256,SUBNET&gt;;template &lt;typename SUBNET&gt; using alevel1 = ares&lt;256,ares&lt;256,ares_down&lt;256,SUBNET&gt;&gt;&gt;;template &lt;typename SUBNET&gt; using alevel2 = ares&lt;128,ares&lt;128,ares_down&lt;128,SUBNET&gt;&gt;&gt;;template &lt;typename SUBNET&gt; using alevel3 = ares&lt;64,ares&lt;64,ares&lt;64,ares_down&lt;64,SUBNET&gt;&gt;&gt;&gt;;template &lt;typename SUBNET&gt; using alevel4 = ares&lt;32,ares&lt;32,ares&lt;32,SUBNET&gt;&gt;&gt;;using anet_type = loss_metric&lt;fc_no_bias&lt;128,avg_pool_everything&lt; alevel0&lt; alevel1&lt; alevel2&lt; alevel3&lt; alevel4&lt; max_pool&lt;3,3,2,2,relu&lt;affine&lt;con&lt;32,7,7,2,2, input_rgb_image_sized&lt;150&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;;int main(int argc, char** argv)&#123; if (argc == 1) &#123; cout &lt;&lt; "\nCall this program like this:" &lt;&lt; endl; cout &lt;&lt; "./train_candidate ../data/candidate-face/" &lt;&lt; endl; return 0; &#125; const char *facesFile = argv[1]; std::vector&lt;matrix&lt;float,0,1&gt;&gt; candidates_descriptors; std::vector&lt;string&gt; candidates; /* 人脸库训练 */ candidates_train(facesFile,candidates_descriptors,candidates);&#125;/* 人脸库训练 */int candidates_train(const char *facesFile,std::vector&lt;matrix&lt;float,0,1&gt;&gt;&amp;candidates_descriptors,std::vector&lt;string&gt;&amp;candidates)&#123; DIR *dir; struct dirent *ptr; char base[30]; const char *pick=".jpg"; //需要的子串; char IRfile[100]; char *name; int face_num = 0; std::vector&lt;matrix&lt;rgb_pixel&gt;&gt; faces; frontal_face_detector detector = get_frontal_face_detector(); // 人脸正脸检测器 shape_predictor sp; //人脸关键点检测器 anet_type net; // 人脸识别模型 deserialize("../model/shape_predictor_68_face_landmarks.dat") &gt;&gt; sp; deserialize("../model/dlib_face_recognition_resnet_model_v1.dat") &gt;&gt; net; clock_t start,finish; double totaltime; start=clock(); ofstream mycout("../candidates.txt"); cout &lt;&lt; "TRAINING START！" &lt;&lt; endl; if ((dir=opendir(facesFile)) == NULL) &#123; perror("Open dir error..."); exit(1); &#125; while ((ptr=readdir(dir)) != NULL) &#123; strcpy(base, ptr-&gt;d_name); if(strstr(base,pick)) &#123; cout &lt;&lt; "training image:" &lt;&lt; base &lt;&lt; endl; strcpy(IRfile, facesFile); strcat(IRfile, base); name = strtok(base, "_"); string candidate = name; cout &lt;&lt; "candidate: " &lt;&lt; candidate &lt;&lt; endl; mycout &lt;&lt; candidate &lt;&lt; endl; candidates.push_back(candidate); matrix&lt;rgb_pixel&gt; img; load_image(img, IRfile); std::vector&lt;rectangle&gt; dets = detector(img); full_object_detection shape = sp(img, dets[0]); matrix&lt;rgb_pixel&gt; face_chip; extract_image_chip(img, get_face_chip_details(shape,150,0.25), face_chip); faces.push_back(move(face_chip)); face_num += 1; &#125; &#125; candidates_descriptors = net(faces); mycout.close(); cout &lt;&lt; "TRAINING END！" &lt;&lt; endl; cout &lt;&lt; "\nTRAIN RESULT：" &lt;&lt; endl; cout &lt;&lt; "Training " &lt;&lt; face_num &lt;&lt; " face(s)！" &lt;&lt; endl; finish=clock(); totaltime=(double)(finish-start)/CLOCKS_PER_SEC; cout&lt;&lt;"TRAINING TIME： " &lt;&lt; totaltime &lt;&lt; " S！"&lt;&lt;endl; cout &lt;&lt; "\nFace database updating……" &lt;&lt; endl; serialize("../candidates_descriptors.dat") &lt;&lt; candidates_descriptors; serialize("../candidates.dat") &lt;&lt; candidates; cout &lt;&lt; "Face has been updated！" &lt;&lt; endl; closedir(dir); return 0;&#125; #this_is_who_kinect_one.cpp 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392#include &lt;iostream&gt;#include &lt;opencv2/core.hpp&gt;#include &lt;opencv2/imgproc.hpp&gt;#include &lt;opencv2/highgui.hpp&gt;#include &lt;opencv2/opencv.hpp&gt;#include &lt;OpenNI.h&gt;#include &lt;dlib/dnn.h&gt;#include &lt;dlib/image_processing/frontal_face_detector.h&gt;#include &lt;dlib/image_processing/render_face_detections.h&gt;#include &lt;dlib/image_processing.h&gt;#include &lt;dlib/gui_widgets.h&gt;#include &lt;dlib/image_io.h&gt;#include &lt;dlib/opencv.h&gt;#include &lt;time.h&gt;#include &lt;dirent.h&gt;#include &lt;string.h&gt;#include &lt;math.h&gt;#include &lt;sstream&gt; using namespace std;using namespace openni;using namespace dlib;using namespace cv;template &lt;template &lt;int,template&lt;typename&gt;class,int,typename&gt; class block, int N, template&lt;typename&gt;class BN, typename SUBNET&gt;using residual = add_prev1&lt;block&lt;N,BN,1,tag1&lt;SUBNET&gt;&gt;&gt;;template &lt;template &lt;int,template&lt;typename&gt;class,int,typename&gt; class block, int N, template&lt;typename&gt;class BN, typename SUBNET&gt;using residual_down = add_prev2&lt;avg_pool&lt;2,2,2,2,skip1&lt;tag2&lt;block&lt;N,BN,2,tag1&lt;SUBNET&gt;&gt;&gt;&gt;&gt;&gt;;template &lt;int N, template &lt;typename&gt; class BN, int stride, typename SUBNET&gt; using block = BN&lt;con&lt;N,3,3,1,1,relu&lt;BN&lt;con&lt;N,3,3,stride,stride,SUBNET&gt;&gt;&gt;&gt;&gt;;template &lt;int N, typename SUBNET&gt; using ares = relu&lt;residual&lt;block,N,affine,SUBNET&gt;&gt;;template &lt;int N, typename SUBNET&gt; using ares_down = relu&lt;residual_down&lt;block,N,affine,SUBNET&gt;&gt;;template &lt;typename SUBNET&gt; using alevel0 = ares_down&lt;256,SUBNET&gt;;template &lt;typename SUBNET&gt; using alevel1 = ares&lt;256,ares&lt;256,ares_down&lt;256,SUBNET&gt;&gt;&gt;;template &lt;typename SUBNET&gt; using alevel2 = ares&lt;128,ares&lt;128,ares_down&lt;128,SUBNET&gt;&gt;&gt;;template &lt;typename SUBNET&gt; using alevel3 = ares&lt;64,ares&lt;64,ares&lt;64,ares_down&lt;64,SUBNET&gt;&gt;&gt;&gt;;template &lt;typename SUBNET&gt; using alevel4 = ares&lt;32,ares&lt;32,ares&lt;32,SUBNET&gt;&gt;&gt;;using anet_type = loss_metric&lt;fc_no_bias&lt;128,avg_pool_everything&lt; alevel0&lt; alevel1&lt; alevel2&lt; alevel3&lt; alevel4&lt; max_pool&lt;3,3,2,2,relu&lt;affine&lt;con&lt;32,7,7,2,2, input_rgb_image_sized&lt;150&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;;const int ITER = 5000; // 随机取点次数const float PLANE_OR_NOT = 0.2; // 判断是否为平面的分界线const int SIGMA = 1;int main()&#123; frontal_face_detector detector = get_frontal_face_detector(); // 人脸正脸检测器 shape_predictor sp; //人脸关键点检测器 anet_type net; // 人脸识别模型 deserialize("../model/shape_predictor_68_face_landmarks.dat") &gt;&gt; sp; deserialize("../model/dlib_face_recognition_resnet_model_v1.dat") &gt;&gt; net; std::vector&lt;matrix&lt;float,0,1&gt;&gt; candidates_descriptors; deserialize("../candidates_descriptors.dat") &gt;&gt; candidates_descriptors; std::vector&lt;string&gt; candidates; deserialize("../candidates.dat") &gt;&gt; candidates; // 1. Initial OpenNI if( OpenNI::initialize() != STATUS_OK ) &#123; cerr &lt;&lt; "OpenNI Initial Error: " &lt;&lt; OpenNI::getExtendedError() &lt;&lt; endl; return -1; &#125; // 2. Open Device Device mDevice; if( mDevice.open( ANY_DEVICE ) != STATUS_OK ) &#123; cerr &lt;&lt; "Can't Open Device: " &lt;&lt; OpenNI::getExtendedError() &lt;&lt; endl; return -1; &#125; // 3. Create depth stream VideoStream mDepthStream; if( mDevice.hasSensor( SENSOR_DEPTH ) ) &#123; if( mDepthStream.create( mDevice, SENSOR_DEPTH ) == STATUS_OK ) &#123; // 3a. set video mode VideoMode mMode; mMode.setResolution( 640, 480 ); mMode.setFps( 30 ); mMode.setPixelFormat( PIXEL_FORMAT_DEPTH_1_MM ); if( mDepthStream.setVideoMode( mMode) != STATUS_OK ) &#123; cout &lt;&lt; "Can't apply VideoMode: " &lt;&lt; OpenNI::getExtendedError() &lt;&lt; endl; &#125; &#125; else &#123; cerr &lt;&lt; "Can't create depth stream on device: " &lt;&lt; OpenNI::getExtendedError() &lt;&lt; endl; return -1; &#125; &#125; else &#123; cerr &lt;&lt; "ERROR: This device does not have depth sensor" &lt;&lt; endl; return -1; &#125; // 4. Create color stream VideoStream mColorStream; if( mDevice.hasSensor( SENSOR_COLOR ) ) &#123; if( mColorStream.create( mDevice, SENSOR_COLOR ) == STATUS_OK ) &#123; // 4a. set video mode VideoMode mMode; mMode.setResolution( 640, 480 ); mMode.setFps( 30 ); mMode.setPixelFormat( PIXEL_FORMAT_RGB888 ); if( mColorStream.setVideoMode( mMode) != STATUS_OK ) &#123; cout &lt;&lt; "Can't apply VideoMode: " &lt;&lt; OpenNI::getExtendedError() &lt;&lt; endl; &#125; // 4b. image registration if( mDevice.isImageRegistrationModeSupported( IMAGE_REGISTRATION_DEPTH_TO_COLOR ) ) &#123; mDevice.setImageRegistrationMode( IMAGE_REGISTRATION_DEPTH_TO_COLOR ); &#125; &#125; else &#123; cerr &lt;&lt; "Can't create color stream on device: " &lt;&lt; OpenNI::getExtendedError() &lt;&lt; endl; return -1; &#125; &#125; // 5. create OpenCV Window cv::namedWindow( "Depth Image", CV_WINDOW_AUTOSIZE); cv::namedWindow( "Color Image", CV_WINDOW_AUTOSIZE); // 6. start VideoFrameRef mColorFrame; VideoFrameRef mDepthFrame; mDepthStream.start(); mColorStream.start(); cv::Mat cImageBGR; cv::Mat DepthMat; int iMaxDepth = mDepthStream.getMaxPixelValue(); while( true ) &#123; // 8a. get depth frame if( mDepthStream.readFrame( &amp;mDepthFrame ) == STATUS_OK ) &#123; // 8b. convert data to OpenCV format const cv::Mat mImageDepth( mDepthFrame.getHeight(), mDepthFrame.getWidth(), CV_16UC1, (void*)mDepthFrame.getData() ); DepthMat = mImageDepth.clone(); // 8c. re-map depth data [0,Max] to [0,255] cv::Mat mScaledDepth; mImageDepth.convertTo( mScaledDepth, CV_8U, 255.0 / iMaxDepth ); // 8d. show image cv::imshow( "Depth Image", mScaledDepth ); &#125; // 7a. get color frame if( mColorStream.readFrame( &amp;mColorFrame ) == STATUS_OK ) &#123; // 7b. convert data to OpenCV format const cv::Mat mImageRGB( mColorFrame.getHeight(), mColorFrame.getWidth(), CV_8UC3, (void*)mColorFrame.getData() ); // 7c. convert form RGB to BGR cv::cvtColor( mImageRGB, cImageBGR, CV_RGB2BGR ); // 7d. show image //dlib cv_image&lt;bgr_pixel&gt; cimg(cImageBGR); // Detect faces std::vector&lt;dlib::rectangle&gt; dets = detector(cimg); if(dets.size() != 0)&#123; int livedetector; string who,isface; float who_probability; float pretotal_ary; if(dets.size() != 1)&#123; livedetector = -1; for (unsigned long j = 0; j &lt; dets.size(); ++j)&#123; Rect rect(dets[j].left(),dets[j].top(),dets[j].right() -dets[j].left(), dets[j].bottom() -dets[j].top());//左上坐标（x,y）和矩形的长(x)宽(y) cv::rectangle(cImageBGR, rect, Scalar(0, 255, 0),1,8,0); &#125; &#125; else&#123; Rect rect(dets[0].left(),dets[0].top(),dets[0].right() -dets[0].left(), dets[0].bottom() -dets[0].top());//左上坐标（x,y）和矩形的长(x)宽(y) cv::rectangle(cImageBGR, rect, Scalar(0, 255, 0),1,8,0); full_object_detection shape = sp(cimg, dets[0]); std::vector&lt;matrix&lt;rgb_pixel&gt;&gt; faces; matrix&lt;rgb_pixel&gt; face_chip; extract_image_chip(cimg, get_face_chip_details(shape,150,0.25), face_chip); faces.push_back(move(face_chip)); std::vector&lt;matrix&lt;float,0,1&gt;&gt; face_descriptors = net(faces); float distance; float best_distance = length(face_descriptors[0]-candidates_descriptors[0]); size_t candidates_num = candidates_descriptors.size(); int candidates_num_int = static_cast&lt;int&gt;(candidates_num); int best_k = 0; for (int k = 1; k &lt; candidates_num_int; k++) &#123; distance = length(face_descriptors[0]-candidates_descriptors[k]); if (distance &lt; best_distance) &#123; best_distance = distance; best_k = k; &#125; &#125; if (best_distance &lt; 0.6) &#123; who = candidates[best_k]; who_probability = (1.05-0.66*best_distance)*100 &lt; 100?(1.05-0.66*best_distance)*100:99; &#125; else&#123; who = "Unknow"; &#125; // liveness_detection(DepthMat,locates,livedetector); int COL ,ROW ,FACE_WIDTH ,FACE_HEIGHT; int faceno0_num; int FaceDATA[3][100000]; int k; int pretotal; int x[3],y[3],z[3]; // 随机取三个点 float a,b,c; // 拟合平面方程 z=ax+by+c int rand_num[3]; float check,distance2; int total; COL = rect.x; ROW = rect.y; FACE_WIDTH = rect.width; FACE_HEIGHT = rect.height; faceno0_num = FACE_HEIGHT*FACE_WIDTH -1; k = 0; for(int m = 1;m&lt; FACE_HEIGHT+1;m++) &#123; for(int n= 1;n&lt; FACE_WIDTH+1;n++) &#123; ushort tmp = DepthMat.at&lt;ushort&gt;(COL+n-1,ROW+m-1); if (tmp == 0) &#123; faceno0_num -= 1; // 非零深度点个数为 faceno0_num+1 continue; &#125; FaceDATA[0][k] = n; FaceDATA[1][k] = m; FaceDATA[2][k] = tmp; k += 1; &#125; &#125; if(faceno0_num &gt; 2000)&#123; pretotal = 0; // 符合拟合模型的数据的个数 srand((unsigned)time(NULL)); total = 0; for(k = 0; k &lt; ITER; k++) &#123; do&#123; rand_num[0] = std::rand()%faceno0_num; rand_num[1] = std::rand()%faceno0_num; rand_num[2] = std::rand()%faceno0_num; &#125;while(rand_num[0] == rand_num[1] || rand_num[0] == rand_num[2] || rand_num[1] == rand_num[2]); for(int n = 0; n &lt; 3; n++ ) &#123; x[n] = FaceDATA[0][rand_num[n]]; y[n] = FaceDATA[1][rand_num[n]]; z[n] = FaceDATA[2][rand_num[n]]; &#125; check = (x[0]-x[1])*(y[0]-y[2]) - (x[0]-x[2])*(y[0]-y[1]); if ( check == 0) // 防止提示浮点数例外 (核心已转储) &#123; k -= 1; continue; &#125; a = ( (z[0]-z[1])*(y[0]-y[2]) - (z[0]-z[2])*(y[0]-y[1]) )*1.0/( (x[0]-x[1])*(y[0]-y[2]) - (x[0]-x[2])*(y[0]-y[1]) ); if (y[0] == y[2]) // 防止提示浮点数例外 (核心已转储) &#123; k -= 1; continue; &#125; b = ((z[0] - z[2]) - a * (x[0] - x[2]))*1.0/(y[0]-y[2]); c = z[0]- a * x[0] - b * y[0]; total = 0; for(int n = 0; n &lt; faceno0_num +1 ; n++ ) &#123; distance2 = fabs(a*FaceDATA[0][n] + b*FaceDATA[1][n] - 1*FaceDATA[2][n] + c*1); if (distance2 &lt; SIGMA) &#123; total +=1; &#125; &#125; if (total &gt; pretotal) // 找到符合拟合平面数据最多的拟合平面 &#123; pretotal=total; &#125; &#125; pretotal_ary = pretotal *1.0/ faceno0_num; if (pretotal_ary &lt; PLANE_OR_NOT)&#123; livedetector = 1; &#125; else&#123; livedetector = 0; &#125; &#125; else&#123; livedetector = -2; &#125; &#125; if(livedetector == -1)&#123; who = "More than one face"; isface = "Please detect one face"; &#125; if(livedetector == 1)&#123; isface = "Is FACE"; &#125; if(livedetector == 0)&#123; isface = "Is not FACE"; &#125; if(livedetector == -2)&#123; isface = "Lack of depth information"; pretotal_ary = 0; &#125; stringstream strStream1; strStream1 &lt;&lt; who &lt;&lt; " , " &lt;&lt; who_probability &lt;&lt; "%"; string str1 = strStream1.str(); stringstream strStream2; strStream2 &lt;&lt; isface &lt;&lt; " , " &lt;&lt; "pretotal_ary:" &lt;&lt; pretotal_ary; string str2 = strStream2.str(); cv::putText(cImageBGR, "RECOGNITION RESULT: ", cv::Point(20,20), cv::FONT_HERSHEY_COMPLEX,0.4,Scalar(255, 255, 255),1,8,0); cv::putText(cImageBGR, str1, cv::Point(20,40), cv::FONT_HERSHEY_COMPLEX,0.5,Scalar(0, 255, 0),1,8,0); cv::putText(cImageBGR, "LIVENESS DETECTION RESULT: ", cv::Point(20,60), cv::FONT_HERSHEY_COMPLEX,0.4,Scalar(255, 255, 255),1,8,0); cv::putText(cImageBGR, str2, cv::Point(20,80), cv::FONT_HERSHEY_COMPLEX,0.5,Scalar(0, 255, 0),1,8,0); &#125; cv::imshow( "Color Image", cImageBGR); &#125; // 6a. check keyboard if( cv::waitKey( 1 ) == 'q' )&#123; break; &#125; &#125; // 9. stop mDepthStream.destroy(); mColorStream.destroy(); mDevice.close(); OpenNI::shutdown(); return 0;&#125; #this_is_who.cpp 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236#include &lt;dlib/dnn.h&gt;#include &lt;dlib/image_processing/frontal_face_detector.h&gt;#include &lt;dlib/image_processing.h&gt;#include &lt;dlib/gui_widgets.h&gt;#include &lt;dlib/image_io.h&gt;#include &lt;iostream&gt;#include &lt;time.h&gt;#include &lt;dirent.h&gt;#include &lt;string.h&gt;#include &lt;math.h&gt;using namespace dlib;using namespace std;/* 函数声明 *//* 输出人脸位置 返回识别结果 */string face_location(const char *imgFile,std::vector&lt;int&gt;&amp;locates, std::vector&lt;matrix&lt;float,0,1&gt;&gt;&amp;candidates_descriptors,std::vector&lt;string&gt;&amp;candidates);/* 判断是否为活体 */bool liveness_detection(const char *DeepFile,std::vector&lt;int&gt;&amp;locates); const int IMG_HEIGHT = 720;const int IMG_WIDTH = 1280;template &lt;template &lt;int,template&lt;typename&gt;class,int,typename&gt; class block, int N, template&lt;typename&gt;class BN, typename SUBNET&gt;using residual = add_prev1&lt;block&lt;N,BN,1,tag1&lt;SUBNET&gt;&gt;&gt;;template &lt;template &lt;int,template&lt;typename&gt;class,int,typename&gt; class block, int N, template&lt;typename&gt;class BN, typename SUBNET&gt;using residual_down = add_prev2&lt;avg_pool&lt;2,2,2,2,skip1&lt;tag2&lt;block&lt;N,BN,2,tag1&lt;SUBNET&gt;&gt;&gt;&gt;&gt;&gt;;template &lt;int N, template &lt;typename&gt; class BN, int stride, typename SUBNET&gt; using block = BN&lt;con&lt;N,3,3,1,1,relu&lt;BN&lt;con&lt;N,3,3,stride,stride,SUBNET&gt;&gt;&gt;&gt;&gt;;template &lt;int N, typename SUBNET&gt; using ares = relu&lt;residual&lt;block,N,affine,SUBNET&gt;&gt;;template &lt;int N, typename SUBNET&gt; using ares_down = relu&lt;residual_down&lt;block,N,affine,SUBNET&gt;&gt;;template &lt;typename SUBNET&gt; using alevel0 = ares_down&lt;256,SUBNET&gt;;template &lt;typename SUBNET&gt; using alevel1 = ares&lt;256,ares&lt;256,ares_down&lt;256,SUBNET&gt;&gt;&gt;;template &lt;typename SUBNET&gt; using alevel2 = ares&lt;128,ares&lt;128,ares_down&lt;128,SUBNET&gt;&gt;&gt;;template &lt;typename SUBNET&gt; using alevel3 = ares&lt;64,ares&lt;64,ares&lt;64,ares_down&lt;64,SUBNET&gt;&gt;&gt;&gt;;template &lt;typename SUBNET&gt; using alevel4 = ares&lt;32,ares&lt;32,ares&lt;32,SUBNET&gt;&gt;&gt;;using anet_type = loss_metric&lt;fc_no_bias&lt;128,avg_pool_everything&lt; alevel0&lt; alevel1&lt; alevel2&lt; alevel3&lt; alevel4&lt; max_pool&lt;3,3,2,2,relu&lt;affine&lt;con&lt;32,7,7,2,2, input_rgb_image_sized&lt;150&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;;int main(int argc, char** argv)&#123; if (argc == 1) &#123; cout &lt;&lt; "\nCall this program like this:" &lt;&lt; endl; cout &lt;&lt; "./this_is_who ../data/allface/0004_IR_allleft.jpg ../data/allface/0004_raw_allleft.raw" &lt;&lt; endl; return 0; &#125; const char *imgFile = argv[1]; const char *DeepFile = argv[2]; std::vector&lt;matrix&lt;float,0,1&gt;&gt; candidates_descriptors; deserialize("../candidates_descriptors.dat") &gt;&gt; candidates_descriptors; std::vector&lt;string&gt; candidates; deserialize("../candidates.dat") &gt;&gt; candidates; std::vector&lt;int&gt; locates; /* 输出人脸位置 返回识别结果 */ string who = face_location(imgFile, locates, candidates_descriptors,candidates); cout &lt;&lt; "\nRECOGNITION RESULT：" &lt;&lt; endl; cout &lt;&lt; "This is " &lt;&lt; who &lt;&lt; endl; //深度图与红外图是水平翻转的 locates[0] = IMG_WIDTH - locates[0] -locates[2]; /* 判断是否为活体 */ liveness_detection( DeepFile, locates);&#125;/* 函数 输出人脸位置 返回识别结果 */string face_location(const char* imgFile,std::vector&lt;int&gt;&amp;locates, std::vector&lt;matrix&lt;float,0,1&gt;&gt;&amp;candidates_descriptors,std::vector&lt;string&gt;&amp;candidates)&#123; frontal_face_detector detector = get_frontal_face_detector(); // 人脸正脸检测器 shape_predictor sp; //人脸关键点检测器 anet_type net; // 人脸识别模型 deserialize("../model/shape_predictor_68_face_landmarks.dat") &gt;&gt; sp; deserialize("../model/dlib_face_recognition_resnet_model_v1.dat") &gt;&gt; net; cout &lt;&lt; "\nprocessing image " &lt;&lt; imgFile &lt;&lt; endl; matrix&lt;rgb_pixel&gt; img; load_image(img, imgFile); std::vector&lt;rectangle&gt; dets = detector(img); // cout &lt;&lt; "Number of faces detected: " &lt;&lt; dets.size() &lt;&lt; endl; locates.push_back(dets[0].left()); locates.push_back(dets[0].top()); locates.push_back(dets[0].right() - dets[0].left()); locates.push_back(dets[0].bottom() - dets[0].top()); full_object_detection shape = sp(img, dets[0]); std::vector&lt;matrix&lt;rgb_pixel&gt;&gt; faces; matrix&lt;rgb_pixel&gt; face_chip; extract_image_chip(img, get_face_chip_details(shape,150,0.25), face_chip); faces.push_back(move(face_chip)); std::vector&lt;matrix&lt;float,0,1&gt;&gt; face_descriptors = net(faces); float distance; float best_distance = length(face_descriptors[0]-candidates_descriptors[0]); size_t candidates_num = candidates_descriptors.size(); int candidates_num_int = static_cast&lt;int&gt;(candidates_num); int best_k = 0; for (int k = 1; k &lt; candidates_num_int; k++) &#123; distance = length(face_descriptors[0]-candidates_descriptors[k]); if (distance &lt; best_distance) &#123; best_distance = distance; best_k = k; &#125; &#125; string who; if (best_distance &lt; 0.6) &#123; who = candidates[best_k]; &#125; else&#123; who = "Unknow"; &#125; return who;&#125;/* 函数判断是否为活体 */bool liveness_detection(const char *DeepFile,std::vector&lt;int&gt;&amp;locates)&#123; const int ITER = 5000; // 随机取点次数 const float PLANE_OR_NOT = 0.2; // 判断是否为平面的分界线 const int SIGMA = 1; typedef unsigned short UNIT16; // 从.raw读取二进制16位数据到MatDATA UNIT16 MatDATA[IMG_HEIGHT*IMG_WIDTH]; FILE *fp = NULL; fp = fopen( DeepFile, "rb" ); size_t sizeRead = fread(MatDATA,sizeof(UNIT16),IMG_HEIGHT*IMG_WIDTH,fp); if (sizeRead != IMG_HEIGHT*IMG_WIDTH) &#123; cout &lt;&lt; "DeepFile open error!" &lt;&lt; endl; return 0; &#125; fclose(fp); int n = 0; int i,j; int COL = locates[0],ROW = locates[1],FACE_WIDTH = locates[2],FACE_HEIGHT = locates[3]; //位置信息 // txt :157 66 172 198 , 取行66：66+198,列取157：157+172 int faceno0_num = FACE_HEIGHT*FACE_WIDTH -1; int FaceDATA[3][100000]; n = 0; for(i = 1;i&lt; FACE_HEIGHT+1;i++) &#123; for(j= 1;j&lt; FACE_WIDTH+1;j++) &#123; if (MatDATA[IMG_WIDTH*(ROW+i-2)+COL+j-2] == 0) &#123; faceno0_num -= 1; // 非零深度点个数为 faceno0_num+1 continue; &#125; FaceDATA[1][n] = i; FaceDATA[0][n] = j; FaceDATA[2][n] = MatDATA[IMG_WIDTH*(ROW+i-2)+COL+j-2]; n += 1; &#125; &#125; int pretotal = 0; // 符合拟合模型的数据的个数 int x[3],y[3],z[3]; // 随机取三个点 srand((unsigned)time(NULL)); float a,b,c; // 拟合平面方程 z=ax+by+c // float besta,bestb,bestc; // 最佳参数 int rand_num[3]; float check,distance; int total = 0; for(i = 0; i &lt; ITER; i++) &#123; do&#123; rand_num[0] = std::rand()%faceno0_num; rand_num[1] = std::rand()%faceno0_num; rand_num[2] = std::rand()%faceno0_num; &#125;while(rand_num[0] == rand_num[1] || rand_num[0] == rand_num[2] || rand_num[1] == rand_num[2]); for(n = 0; n &lt; 3; n++ ) &#123; x[n] = FaceDATA[0][rand_num[n]]; y[n] = FaceDATA[1][rand_num[n]]; z[n] = FaceDATA[2][rand_num[n]]; &#125; check = (x[0]-x[1])*(y[0]-y[2]) - (x[0]-x[2])*(y[0]-y[1]); if ( check == 0) // 防止提示浮点数例外 (核心已转储) &#123; i -= 1; continue; &#125; a = ( (z[0]-z[1])*(y[0]-y[2]) - (z[0]-z[2])*(y[0]-y[1]) )*1.0/( (x[0]-x[1])*(y[0]-y[2]) - (x[0]-x[2])*(y[0]-y[1]) ); if (y[0] == y[2]) // 防止提示浮点数例外 (核心已转储) &#123; i -= 1; continue; &#125; b = ((z[0] - z[2]) - a * (x[0] - x[2]))*1.0/(y[0]-y[2]); c = z[0]- a * x[0] - b * y[0]; total = 0; for(n = 0; n &lt; faceno0_num +1 ; n++ ) &#123; distance = fabs(a*FaceDATA[0][n] + b*FaceDATA[1][n] - 1*FaceDATA[2][n] + c*1); if (distance &lt; SIGMA) &#123; total +=1; &#125; &#125; if (total &gt; pretotal) // 找到符合拟合平面数据最多的拟合平面 &#123; pretotal=total; // besta = a; // bestb = b; // bestc = c; &#125; &#125; float pretotal_ary = pretotal *1.0/ faceno0_num ; cout &lt;&lt; "\nLIVENESS DETECTION RESULT：" &lt;&lt; endl; bool IS_FACE; if (pretotal_ary &lt; PLANE_OR_NOT) &#123; IS_FACE = true; cout &lt;&lt; "pretotal_ary = " &lt;&lt; pretotal_ary &lt;&lt; " , Is FACE!" &lt;&lt; endl; &#125; else &#123; IS_FACE = false; cout &lt;&lt; "pretotal_ary = " &lt;&lt; pretotal_ary &lt;&lt; " , Is not FACE!" &lt;&lt; endl; &#125; return IS_FACE;&#125; 代码比较粗糙，应该以后也不会再改了……]]></content>
      <categories>
        <category>算法</category>
        <category>实时人脸识别</category>
        <category>活体检测</category>
      </categories>
      <tags>
        <tag>dlib</tag>
        <tag>opencv</tag>
        <tag>Kinect</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticFusion在ubuntu16.04+cuda8.0上的实现]]></title>
    <url>%2F2019%2F04%2F12%2F15%2F</url>
    <content type="text"><![CDATA[一、驱动与cuda安装#安装驱动与cuda8.0参考这篇：https://blog.csdn.net/ffcjjhv/article/details/89151382 二、配置1、依赖包 123sudo apt-get install cmake cmake-qt-gui git build-essential libusb-1.0-0-dev libudev-dev sudo apt-get install freeglut3-dev libglew-dev libsuitesparse-dev libeigen3-dev sudo apt-get install zlib1g-dev libjpeg-dev doxygen graphviz 2、openjdk-7-jdk123sudo add-apt-repository ppa:openjdk-r/ppasudo apt-get updatesudo apt-get install openjdk-7-jdk 3、OpenGL123sudo apt-get install build-essential libgl1-mesa-devsudo apt-get install freeglut3-devsudo apt-get install libglew-dev libsdl2-dev libsdl2-image-dev libglm-dev libfreetype6-dev 三、代码编译#download ElasticFusionGitHub地址：https://github.com/mp3guy/ElasticFusion ，解压到ElasticFusion/ElasticFusion-master 1cd ElasticFusion 1、安装 Pangolin123456git clone https://github.com/stevenlovegrove/Pangolin.gitcd Pangolinmkdir build &amp;&amp; cd buildcmake ../ -DAVFORMAT_INCLUDE_DIR="" -DCPP11_NO_BOOST=ONmake -j4cd ../.. 2、安装 OpenNI21234git clone https://github.com/occipital/OpenNI2.gitcd OpenNI2make -j4cd .. 3、配置 ElasticFusion（1）build Core12345cd ElasticFusion-master/Coremkdir build &amp;&amp; cd buildcmake ../srcmake -j4cd ../../ （2）build GPUTest12345cd GPUTestmkdir build &amp;&amp; cd buildcmake ../srcmake -j4cd ../../ （3）build GUI12345cd GUImkdir build &amp;&amp; cd buildcmake ../srcmake -j4cd ../../ 四、 运行1、数据集离线运行下载数据集： http://www.doc.ic.ac.uk/~sleutene/datasets/elasticfusion/dyson_lab.klg ，放到ElasticFusion/ElasticFusion-master/GUI/build，并运行 1./ElasticFusion -l dyson_lab.klg #运行结果 2、在Kinect v1上实时运行（1）build libFreenect123cd ElasticFusiongit clone https://github.com/OpenKinect/libfreenectcd libfreenect 打开libfreenect/CMakeLists.txt，在33行cmake_minimum_required(VERSION 2.8.12)下一行添加 1add_definitions(-std=c++11) 保存后关闭，命令行继续执行 1234mkdir build &amp;&amp; cd build cmake .. -DBUILD_OPENNI2_DRIVER=ON make -j4cp -L lib/OpenNI2-FreenectDriver/libFreenectDriver.so $&#123;OPENNI2_DIR&#125;/Bin/x64-Release/OpenNI2/Drivers #OPENNI2_DIR需要修改 注：${OPENNI2_DIR}是OpenNI2的解压文件夹，比如我的在ElasticFusion文件夹，则 （2）Kinect v1接入打开libfreenect文件夹，运行 1sudo cp platform/linux/udev/51-kinect.rules /etc/udev/rules.d 重启后，插上kinect后，命令行运行lsusb，查看是否包含：Xbox camera，Xbox motor，Xbox audio （3）修改GPUConfig.h如果代码运行很慢且出现如下提示，需要修改修改GPUConfig.h在ElasticFusion/ElasticFusion-master/GPUtest/build文件夹运行 1./GPUTest ../ 将最后4行添加到/ElasticFusion/ElasticFusion-master/Core/src/Utils/GPUConfig.h （4）运行打开ElasticFusion/ElasticFusion-master/GUI/build，运行 1./ElasticFusion #运行结果]]></content>
      <categories>
        <category>三维重建</category>
        <category>配置</category>
        <category>算法</category>
      </categories>
      <tags>
        <tag>ElasticFusion</tag>
        <tag>三维重建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BundleFusion在win10+vs2013+cuda8.0上的实现]]></title>
    <url>%2F2019%2F04%2F12%2F21%2F</url>
    <content type="text"><![CDATA[介绍在深度相机室内实时稠密三维重建领域，BundleFusion是目前效果最好的开源算法框架。论文地址：https://arxiv.org/pdf/1604.01093.pdf演示视频截取片断： 视频地址：https://www.youtube.com/watch?v=keIirXrRb1k在演示视频中，structure sensor是卡在iPad上，将采集到的数据通过无线网络传给台式机（带GPU），匹配、优化和重建工作都是在台式机上运行，重建的结果再通过无线网络传到iPad上显示。重建效果图： 一、配置1、cuda8.0安装#建议安装：cuda8.0，vs2013 #卸载cuda10.0在控制面板/卸载程序，卸载掉所有带有NVIDIA的程序，卸载顺序不影响 #下载cuda8.0：https://developer.nvidia.com/cuda-80-ga2-download-archive ，并运行安装。 #验证安装成功，输入nvcc --version 2、DirectX SDK安装下载地址：http://download.microsoft.com/download/A/E/7/AE743F1F-632B-4809-87A9-AA1BB3458E31/DXSDK_Jun10.exe ，运行安装。 #若运行安装出现 错误 ErrorCode:s1023解决办法1：控制面板卸载以上两项，再重新安装DXSDK_Jun10.exe即可。 大部分通过这个方法可以解决，如果以上两项不存在或者卸载后仍不能解决问题（比如我），可能Microsoft Visual C++ 运行库安装有问题，得删除并重新安装 Microsoft Visual C++ 运行库，步骤如下： 1、控制面板卸载名称包含“Microsoft Visual C++”和“Redistributable”的所有程序2、下载并安装以下所有软件： Microsoft Visual C++ 2005 SP1 Redistributable (x86) Microsoft Visual C++ 2005 SP1 Redistributable (x64) Microsoft Visual C++ 2008 SP1 Redistributable (x86) Microsoft Visual C++ 2008 SP1 Redistributable (x64)Microsoft Visual C++ 2010 SP1 Redistributable (x86)Microsoft Visual C++ 2010 SP1 Redistributable (x64)Microsoft Visual C++ 2012 Update 4 Redistributable(x86 和 x64)Microsoft Visual C++ 2013 Update 5 Redistributable Package(x86 和 x64)Microsoft Visual C++ 2015 Update 3 Redistributable(x86 和 x64) 注意： 每个下载的文件名均相同，因此在安装它之前，请确保不覆盖之前的文件。下载一个安装一个，然后再下载下一个并安装，依此类推。 64 位系统上需要 x86 和 x64 版本。32 位系统只需要 x86 版本。 3、重启后，控制面板卸载2010两项，再重新安装DXSDK_Jun10.exe，即可解决。 二、vs2013代码生成BundleFusion GitHub地址：https://github.com/niessner/BundleFusion配置过程主要参考这里 从GitHub上下载该BundleFusion，并解压，得到工程 BundleFusioin-master 1、mLib与 mLib external配置mLib下载地址：https://github.com/niessner/mLib ，解压后，替换掉目录 BundleFusion-master\external\mLib mLib external下载地址：https://www.dropbox.com/s/fve3uen5mzonidx/mLibExternal.zip?dl=0 ，解压后，放到 BundleFusioin-master 同级目录,网址访问不了的话可以用我传到网盘的mLibExternal.zip：链接：https://pan.baidu.com/s/1CJysQCHl4t7RPfvqliwfew 提取码：0x4t 最终目录结构： 123456789101112131415bundlefusion BundleFusion-master/ external/ mLib/ # this is the submodule you replaced data/ src/ [...] FriedLiver/ [...] FriedLiver.sln [...] mLibExternal/ # you downloaded this from Dropbox include libsWindows [...] 2、cuda设置修改cuda版本用VS2013打开 BundleFusion-master\FriedLiver\FriedLiver.sln 工程，如果cuda版本不是原代码设置的cuda7.0，就会出现如下错误：修改 BundleFusion-master\FriedLiver\FriedLiver.vcxproj 文件，用vs code打开该文件，并修改如下两行：再次打开就不会报错了。 显卡计算能力设置选择适合你显卡的compute能力，显卡计算能力在这里查询我的笔记本显卡NVIDIA GeForce MX150，计算能力与GeForce 940M等同，为5.0，在工程的配置选项中设置如下： 3、相机类型设置（以apt0.sens离线数据为例）输入类型输入数据可以为Kinect v1，Kinect v2，PrimeSenseSensor，IntelSensor，RealSense，StructureSensor以及SensorDataReader（离线数据）具体可以查看FriedLiver.cpp中7-103行的内容 没有深度相机的，以离线数据office3.sens为例，下载地址：http://graphics.stanford.edu/projects/bundlefusion/ 修改GlobalAppState.h因为使用的是离线数据，将GlobalAppState.h中深度相机全部注释掉。注：如果用深度相机（如：Kinect v1）实时运行，用哪个将哪个取消注释，并安装对应SDK。 4、生成FriedLiver.exe修改为 Release 版本，然后选择 生成-&gt;生成解决方法编译通过后，如下图：在BundleFusion-master\FriedLiver\x64\Release中生成可执行文件FriedLiver.exe 三、运行与结果1、配置文件修改将BundleFusion-master\FriedLiver 目录下的 zParametersBundlingDefault.txt 和 zParametersDefault.txt 拷贝到 BundleFusion-master\FriedLiver\x64\Release 修改zParametersDefault.txt用vs code打开 zParametersDefault.txt 配置文件，选择输入类型，这里使用离线数据，修改第2行，设置s_sensorIdx = 8 。如果用深度相机（如：Kinect v1）实时运行，修改为对应的序列号。修改第49行，设置s_hashNumSDFBlocks = 100000修改第58行，设置输入路径s_binaryDumpSensorFile = &quot;../data/office3.sens&quot;将下载的office3.sens放置到BundleFusion-master\FriedLiver\x64\data 修改zParametersBundlingDefault.txt双击运行后，若出现窗口崩溃或者没有响应直接跳出，如下图：用vs code打开 zParametersBundlingDefault.txt 配置文件，修改第30行，设置s_maxNumImages = 400 2、运行结果配置修改后，双击运行FriedLiver.exe按下tab键，如下图所示，按照提示与软件进行交互，按下ctrl+2显示带颜色深度，按下ctrl+3显示彩色视频流，按下ctrl+9导出当前模型到FriedLiver\x64\Release\scans\scan.ply ，具体实现代码查看FriedLiver\Source\DepthSensing\DepthSensing.cpp 3、模型生成运行结束后，BundleFusion-master\FriedLiver\x64\data\office3.sens同目录下生成office3.ply，用meshlab打开 4、在Kinect v1上的实时运行安装 Kinect SDK v1.8 Kinect for Windows SDK v1.8 Kinect for Windows Developer Toolkit v1.8 下载后默认安装 设置Kinect v1输入 重新打开 BundleFusion-master\FriedLiver\FriedLiver.sln 工程，将GlobalAppState.h中第三行取消注释 选择 生成-&gt;生成解决方法，即可编译通过。 zParametersDefault.txt 配置文件，选择输入类型，这里使用Kinect v1，修改第2行，设置s_sensorIdx = 0 实时运行插上Kinect v1，双击运行FriedLiver.exe]]></content>
      <categories>
        <category>三维重建</category>
        <category>算法</category>
        <category>配置</category>
      </categories>
      <tags>
        <tag>BundleFusion</tag>
        <tag>win10</tag>
        <tag>室内重建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DynamicFusion在ubuntu16.04+显卡GT740+cuda8.0上的实现]]></title>
    <url>%2F2019%2F04%2F09%2F14%2F</url>
    <content type="text"><![CDATA[介绍DynamicFusion构建了一个可以实时重建 非刚性变形 动态场景 的系统，随着新的数据融合进模型当中，模型得到降噪、细节更加精细。 论文地址：http://grail.cs.washington.edu/projects/dynamicfusion/papers/DynamicFusion.pdfGitHub地址：https://github.com/mihaibujanca/dynamicfusion注意：这个项目是对论文的复现，但是目前尚未准确复现论文内容，运行速度仍然有待优化，10s/frame不满足实时运行，建议转换成离线数据，再进行运行。 这里记录DynamicFusion在ubuntu16.04+显卡GT740+cuda8.0上的实现。（虽然可以实现，但是GT740真的是太慢慢慢慢了…电脑是刚重装完系统的机子） 1、驱动与cuda安装（1）驱动安装a.驱动下载#查看显卡型号 1lspci | grep -i nvidia nvidia官网 https://www.geforce.cn/drivers 搜索显卡型号对应的驱动，下载（别用最新的）后放到home文件夹 b.禁用nouveau驱动1sudo gedit /etc/modprobe.d/blacklist.conf 在文本最后添加： 12blacklist nouveauoptions nouveau modeset=0 然后执行： 1sudo update-initramfs -u 重启后，屏幕分辨率可能不对不用管，命令行执行：lsmod | grep nouveau ，如果没有屏幕输出，说明禁用nouveau成功。 c.禁用X-Window服务#注意：这会关闭图形界面，提前用手机或者笔记本看下面的步骤 1sudo service lightdm stop 按Ctrl-Alt+F1进入命令行界面，输入用户名和密码登录。 d.命令行安装驱动#给驱动run文件赋予执行权限： 1sudo chmod +x NVIDIA-Linux-x86_64-384.59.run #后面的参数不可省略： 1sudo ./NVIDIA-Linux-x86_64-384.59.run –no-opengl-files 按照提示安装，回车确定，成功安装后在命令行输入：sudo service lightdm start，恢复到图形界面，此时屏幕分辨率恢复正常，重启。 e.驱动测试1sudo nvidia-smi 若列出GPU的信息列表，表示驱动安装成功。 （2）cuda7.5安装cuda7.5可以使用命令行安装，到了cuda8.0以及往上就不行了 12sudo apt-get updatesudo apt-get install nvidia-cuda-dev nvidia-cuda-toolkit nvidia-nsight nvidia-prime 等待安装完成后，输入nvcc -V，检查是否安装成功。 2、环境配置下载DynamicFusion代码：https://github.com/mihaibujanca/dynamicfusion整个配置过程参考dynamicfusion-master/build.sh 文件（不要直接运行build.sh会有各种报错） （1）apt-get install 依赖安装123sudo apt-get install cmake libvtk5-dev libsuitesparse-dev liblapack-dev --fix-missingsudo apt-get install libblas-dev libgtk2.0-dev pkg-config libopenni-dev --fix-missingsudo apt-get install libusb-1.0-0-dev wget zip clang --fix-missing （2）安装gflags、glog、Eigen、Ceres进入dynamicfusion-master文件夹的上一目录 安装gflags、glog、Eigen#安装 gflags 12345git clone https://github.com/gflags/gflags.gitcd gflagsmkdir -p build/ &amp;&amp; cd buildcmake .. &amp;&amp; make cd ../../ #安装 glog 12345git clone https://github.com/google/glog.gitcd glogmkdir -p build/ &amp;&amp; cd build/cmake .. &amp;&amp; makecd ../../ #安装 Eigen 3.3.4 1234567wget http://bitbucket.org/eigen/eigen/get/3.3.4.tar.gztar -xf 3.3.4.tar.gzcd eigen-eigen-5a0156e40febmkdir -p build &amp;&amp; cd buildcmake ..sudo make installcd ../../ 配置ceres-solver#下载ceres-solver：https://github.com/ceres-solver/ceres-solver 并解压到dynamicfusion-master文件夹的上一目录 ，将ceres-solver-master重命名为ceres-solver #配置 Ceres 123456cd ceres-solvermkdir -p build/ &amp;&amp; cd build/cmake ..make -j4sudo make installcd ../../ （3）安装OpenCV 2.4.13注：opencv的下载过程非常非常慢，建议前天晚上下载，第二天再接着配置 12345678git clone https://github.com/opencv/opencvcd opencv/git checkout 2.4.13.3mkdir -p build &amp;&amp; cd buildcmake -DWITH_VTK=ON -DBUILD_opencv_calib3d=ON -DBUILD_opencv_imgproc=ON -DWITH_CUDA=OFF ..make -j4sudo make installcd ../../ （4）安装Boost1234567wget https://dl.bintray.com/boostorg/release/1.64.0/source/boost_1_64_0.tar.gztar -xf boost_1_64_0.tar.gzcd boost_1_64_0sudo ./bootstrap.sh./b2sudo ./b2 installcd .. 删除掉无用的压缩包，最终目录如图 3、代码修改与dataset获取（1）配置deps/terra12345cd dynamicfusion-master/depswget https://github.com/zdevito/terra/releases/download/release-2016-03-25/terra-Linux-x86_64-332a506.zipunzip terra-Linux-x86_64-332a506.ziprm terra-Linux-x86_64-332a506.zipmv terra-Linux-x86_64-332a506 terra （2）配置deps/Opt#下载Opt：https://github.com/niessner/Opt/tree/c6012e7e4c67fa3bea96161ba24fe88a2b79deed ，并解压到deps，重命名为Opt 12cd Opt/API/make -j4 （3）修改kfusion/src/warp_field.cpp为了解决之后运行代码会出现的报错 12.../dynamicfusion/kfusion/src/warp_field.cpp:158:10: error: ‘struct ceres::Solver::Options’ has no member named ‘num_threads_used’options.num_threads_used = 8; 将第157行options.num_linear_solver_threads = 8;注释掉 （4）dataset下载下载地址 https://www.dropbox.com/sh/qgy2n9bmioofqnj/AABUnT7pi2ECpxSi80EmXOXna?dl=0 （给的数据挺多的，如果网站上不去可以下载我传到百度云的数据，只传了作为demo的umbrella_data.zip） umbrella_data.zip下载链接：https://pan.baidu.com/s/1PRf7-xl5vgj2SUQVNiuk-g ，提取码：25s1下载后放置到dynamicfusion-master文件夹中，运行 12345678910mkdir -p data/umbrella/depthmkdir -p data/umbrella/colormv umbrella_data.zip data/umbrellacd data/umbrellaunzip umbrella_data.ziprm *.txtmv *color*.png color/mv *depth*.png depth/rm umbrella_data.zip 4、运行（1）编译进入dynamicfusion-master文件夹，运行 123mkdir -p build &amp;&amp; cd buildcmake -DOpenCV_DIR=~/opencv/build -DBOOST_ROOT=~/boost_1_64_0/ -DOPENNI_INCLUDE_DIR=/usr/include/ni -DOpenCV_FOUND=TRUE ..make -j4 （2）运行与报错进入dynamicfusion-master文件夹，运行 1./build/bin/dynamicfusion data/umbrella 运行后报错 llvm: No such file or directory ,查看这里：https://github.com/mihaibujanca/dynamicfusion/issues/54升级cuda7.5到8.0能够解决 （3）升级cuda7.5到8.0卸载cuda7.51sudo apt-get autoremove nvidia-cuda-toolkit 安装cuda8.0#下载cuda8.0：https://developer.nvidia.com/cuda-80-ga2-download-archive #安装依赖 1sudo apt-get install freeglut3-dev build-essential libx11-dev libxmu-dev libxi-dev libgl1-mesa-glx libglu1-mesa libglu1-mesa-dev #安装cuda8.0 12sudo chmod 777 cuda_8.0.61_375.26_linux.run sudo ./cuda_8.0.61_375.26_linux.run 执行后会先出现一个声明，需要阅读到100%才会开始安装，直接ctr+c跳过按照提示输入回答就行，位置全部选择默认，注：第二个选择是否安装nvidia驱动时，一定要选择否安装完依赖后，如果仍然提示 12Installing the CUDA Toolkit in /usr/local/cuda-8.0 … Missing recommended library: libXmu.so 不用管 #添加环境变量 1sudo gedit ~/.bashrc 将以下内容写入到~/.bashrc尾部： 12export PATH=/usr/local/cuda-8.0/bin$&#123;PATH:+:$&#123;PATH&#125;&#125;export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64$&#123;LD_LIBRARY_PATH:+:$&#123;LD_LIBRARY_PATH&#125;&#125; 重启后，输入nvcc --version 测试是否安装成功 （4）运行与结果1sudo apt-get install nvidia-cuda-dev nvidia-cuda-toolkit nvidia-nsight nvidia-prime #编译进入dynamicfusion-master文件夹，运行 123mkdir -p build &amp;&amp; cd buildcmake -DOpenCV_DIR=~/opencv/build -DBOOST_ROOT=~/boost_1_64_0/ -DOPENNI_INCLUDE_DIR=/usr/include/ni -DOpenCV_FOUND=TRUE ..make -j4 #运行进入dynamicfusion-master文件夹，运行 1./build/bin/dynamicfusion data/umbrella #结果DynamicFusion效果（项目视频）：视频地址：https://www.youtube.com/watch?v=i1eZekcc_lM]]></content>
      <categories>
        <category>三维重建</category>
        <category>算法</category>
        <category>配置</category>
      </categories>
      <tags>
        <tag>三维重建</tag>
        <tag>DynamicFusion</tag>
        <tag>非刚体</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[leetcode 精选50题 JavaScript答案汇总]]></title>
    <url>%2F2019%2F03%2F11%2F13%2F</url>
    <content type="text"><![CDATA[题目来源 腾讯精选练习（50 题）信息更新时间：2019-3-11，因为要准备面前端，就用js做了遍来熟悉JavaScript这门语言，50道题均已提交通过。 GitHub地址：https://github.com/zj19941113/LeetCode-50-JavaScript-Answers 2两数相加-33.0%-中等https://leetcode-cn.com/problems/add-two-numbers/ 1234567891011121314151617181920212223242526272829303132333435363738/** * Definition for singly-linked list. * function ListNode(val) &#123; * this.val = val; * this.next = null; * &#125; *//** * @param &#123;ListNode&#125; l1 * @param &#123;ListNode&#125; l2 * @return &#123;ListNode&#125; */var addTwoNumbers = function(l1, l2) &#123; var re = new ListNode(0); //0随便取，不会用到，只为了初始化 var r=re; //浅拷贝 var carry=0; while(l1!=null || l2!=null)&#123; var x = l1==null? 0:l1.val; var y = l2==null? 0:l2.val; r.next=new ListNode((carry+x+y)%10); r=r.next; if(carry+x+y&gt;9)&#123; carry=1; &#125;else&#123; carry=0; &#125; if(l1!=null)&#123; l1=l1.next; &#125; if(l2!=null)&#123; l2=l2.next; &#125; &#125; if(carry&gt;0)&#123; r.next=new ListNode(1); //最高位进1 &#125; return re.next;&#125;; 4寻找两个有序数组的中位数-33.8%-困难https://leetcode-cn.com/problems/median-of-two-sorted-arrays/ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * @param &#123;number[]&#125; nums1 * @param &#123;number[]&#125; nums2 * @return &#123;number&#125; */var findMedianSortedArrays = function(a, b) &#123; // 谁小谁移，移动k次，k中间 var n = a.length; var m = b.length; var nm = n + m; var ni = 0,mi = 0,x,y; while(ni + mi &lt; nm )&#123; if(ni &lt; n )&#123; while(mi == m || a[ni] &lt;= b[mi])&#123; ni=ni+1; if(ni+mi == (nm+1)/2)&#123; //总长奇 return a[ni-1]; &#125; if(ni+mi == nm/2)&#123;//总长偶 x = a[ni-1]; break; &#125; if(ni+mi == (nm/2 + 1))&#123; y = a[ni-1]; return (x+y)/2 &#125; &#125; &#125; if(mi &lt; m)&#123; while(ni == n || b[mi] &lt;= a[ni])&#123; mi = mi + 1; if(ni+mi == (nm+1)/2)&#123; return b[mi-1]; &#125; if(ni+mi == nm/2)&#123; x = b[mi-1]; &#125; if(ni+mi == (nm/2 + 1))&#123; y = b[mi-1]; return (x+y)/2 &#125; &#125; &#125; &#125; return -1;&#125;; 5最长回文子串-24.7%-中等https://leetcode-cn.com/problems/longest-palindromic-substring/submissions/ 1234567891011121314151617181920212223242526272829/** * @param &#123;string&#125; s * @return &#123;string&#125; */var longestPalindrome = function(s) &#123; if (s==null&amp;&amp;s.length&lt;1)&#123; return ""; &#125; var fir = 0; var las = 0; for(var i=0;i&lt;s.length;i++)&#123; var lenl = getsub(s,i,i); var lenr = getsub(s,i,i+1); var len = Math.max(lenl,lenr); if(len&gt;las-fir)&#123; fir = i - Math.floor((len-1)/2); las = i + Math.floor(len/2); &#125; &#125; return s.slice(fir,las+1);&#125;;function getsub(s,l,r)&#123; while(l&gt;=0&amp;&amp;r&lt;=s.length-1&amp;&amp;s.charAt(l)==s.charAt(r))&#123; l--; r++; &#125; return r-l-1;&#125; 7整数反转-31.8%-简单https://leetcode-cn.com/problems/reverse-integer/ 1234567891011121314151617181920/** * @param &#123;number&#125; x * @return &#123;number&#125; */var reverse = function(x) &#123; var result = 0; var strx; var mayAns; if(x&gt;0)&#123; strx = String(x); mayAns = Number(strx.split("").reverse().join("")); return mayAns&lt;=Math.pow(2,31)-1?mayAns:0; &#125; else&#123; strx = String(-x); mayAns = -Number(strx.split("").reverse().join("")); return mayAns&gt;=-Math.pow(2,31) ?mayAns:0; &#125; return result;&#125;; 8字符串转换整数 (atoi)-16.6%-中等https://leetcode-cn.com/problems/median-of-two-sorted-arrays 123456789101112131415161718/** * @param &#123;string&#125; str * @return &#123;number&#125; */var myAtoi = function(str) &#123; str = str.trim(); var word = str.split(" ")[0]; if(parseInt(word))&#123; if(parseInt(word)&gt;Math.pow(2,31)-1)&#123; return Math.pow(2,31)-1; &#125;; if(parseInt(word)&lt;-Math.pow(2,31))&#123; return -Math.pow(2,31); &#125;; return parseInt(word); &#125; return 0;&#125;; 9回文数-56.0%-简单https://leetcode-cn.com/problems/palindrome-number/ 1234567891011/** * @param &#123;number&#125; x * @return &#123;boolean&#125; */var isPalindrome = function(x) &#123; if(x &gt;= 0)&#123; var strx = String(x); return strx == strx.split("").reverse().join(""); &#125; return false;&#125;; 11盛最多水的容器-53.5%-中等https://leetcode-cn.com/problems/container-with-most-water/ 12345678910111213141516/** * @param &#123;number[]&#125; height * @return &#123;number&#125; */var maxArea = function(height) &#123; var maxs = 0; for (var i=0;i&lt;height.length;i++)&#123; for (var j=i+1;j&lt;height.length;j++)&#123; var tmp = Math.min(height[i],height[j])*(j-i); if(tmp&gt;maxs)&#123; maxs = tmp; &#125; &#125; &#125; return maxs;&#125;; 14最长公共前缀-32.2%-简单https://leetcode-cn.com/problems/longest-common-prefix/ 12345678910111213141516171819202122232425/** * @param &#123;string[]&#125; strs * @return &#123;string&#125; */var longestCommonPrefix = function(strs) &#123; if(strs.length == 0)&#123; return ""; &#125; var mini = 0; for(var i=1;i&lt;strs.length;i++)&#123; if (strs[i].length&lt;strs[mini].length)&#123; mini = i; &#125; &#125; var result = ""; for(let j=0;j&lt;strs[mini].length;j++)&#123; for( i=0;i&lt;strs.length;i++)&#123; if(strs[mini].slice(0,j+1) != strs[i].slice(0,j+1))&#123; return result; &#125; &#125; result = strs[mini].slice(0,j+1); &#125; return result;&#125;; 15三数之和-21.2%-中等https://leetcode-cn.com/problems/3sum 12345678910111213141516171819202122232425/** * @param &#123;number[]&#125; nums * @return &#123;number[][]&#125; */var threeSum = function(nums) &#123; nums.sort(function(a,b)&#123;return a-b&#125;); var ls = []; for (var i=0;i&lt;nums.length-2;i++) &#123; if(nums[i]==nums[i-1])&#123; //i和前面一样，跳过 continue; &#125; for (var j=i+1;j&lt;nums.length-1;j++) &#123; if(nums[j]==nums[j-1]&amp;&amp;j!=i+1)&#123; //j和前面一样，且前面不是i，跳过 continue; &#125; for (var k=j+1;k&lt;nums.length;k++) &#123; if(nums[k]+nums[j]+nums[i] == 0)&#123; ls.push([nums[i],nums[j],nums[k]]); //找到k跳出，不再往后 break; &#125; &#125; &#125; &#125; return ls;&#125;; 16最接近的三数之和-39.0%-中等https://leetcode-cn.com/problems/3sum-closest/ 1234567891011121314151617181920/** * @param &#123;number[]&#125; nums * @param &#123;number&#125; target * @return &#123;number&#125; */var threeSumClosest = function(nums, target) &#123; var len = nums.length; var minl = nums[0]+nums[1]+nums[2]; for(var i=0;i&lt;len-2;i++)&#123; for(var j=i+1;j&lt;len-1;j++)&#123; for(var k=j+1;k&lt;len;k++)&#123; var tmp = nums[i]+nums[j]+nums[k]; if(Math.abs(tmp-target)&lt;Math.abs(minl-target))&#123; minl = tmp; &#125; &#125; &#125; &#125; return minl;&#125;; 20有效的括号-36.7%-简单https://leetcode-cn.com/problems/valid-parentheses/ 1234567891011121314151617181920212223/** * @param &#123;string&#125; s * @return &#123;boolean&#125; */var isValid = function(s) &#123; if(s == null||s.length &lt;1)&#123; return true; &#125; var stack = []; for(var i=0;i&lt;s.length;i++)&#123; if(s[i] == "")&#123; continue; &#125; if(stack.leng&lt;1)&#123; stack.push(s[i]); &#125;else if(stack[stack.length-1]=="("&amp;&amp;s[i]==")" ||stack[stack.length-1]=="&#123;"&amp;&amp;s[i]=="&#125;" ||stack[stack.length-1]=="["&amp;&amp;s[i]=="]")&#123; stack.pop(); &#125;else&#123; stack.push(s[i]); &#125; &#125; return stack.length&lt;1;&#125;; 21合并两个有序链表-52.9%-简单https://leetcode-cn.com/problems/merge-two-sorted-lists/ 12345678910111213141516171819202122232425262728/** * Definition for singly-linked list. * function ListNode(val) &#123; * this.val = val; * this.next = null; * &#125; *//** * @param &#123;ListNode&#125; l1 * @param &#123;ListNode&#125; l2 * @return &#123;ListNode&#125; */var mergeTwoLists = function(l1, l2) &#123; var l3 = new ListNode(0); var l4 = l3; while(l1!=null &amp;&amp; l2!=null)&#123; if(l1.val&lt;l2.val)&#123; l4.next = l1; l1 = l1.next; &#125;else&#123; l4.next = l2; l2 = l2.next; &#125; l4 = l4.next; &#125; l4.next = l1==null?l2:l1; return l3.next;&#125;; 23合并K个排序链表-43.8%-困难https://leetcode-cn.com/problems/merge-k-sorted-lists/ 1234567891011121314151617181920212223242526272829303132333435/** * Definition for singly-linked list. * function ListNode(val) &#123; * this.val = val; * this.next = null; * &#125; *//** * @param &#123;ListNode[]&#125; lists * @return &#123;ListNode&#125; */var mergeKLists = function(lists) &#123; var rl = null; for(var i=0;i&lt;lists.length;i++)&#123; rl = mergeTwoLists(lists[i],rl); &#125; return rl;&#125;;var mergeTwoLists = function(l1, l2) &#123; var l3 = new ListNode(0); var l4 = l3; while(l1!=null &amp;&amp; l2!=null)&#123; if(l1.val&lt;l2.val)&#123; l4.next = l1; l1 = l1.next; &#125;else&#123; l4.next = l2; l2 = l2.next; &#125; l4 = l4.next; &#125; l4.next = l1==null?l2:l1; return l3.next;&#125;; 26删除排序数组中的重复项-42.9%-简单https://leetcode-cn.com/problems/remove-duplicates-from-sorted-array/ 12345678910111213141516/** * @param &#123;number[]&#125; nums * @return &#123;number&#125; */var removeDuplicates = function(nums) &#123; for(var i=0;i&lt;nums.length;i++)&#123; var j=1; while(nums[i] == nums[i+j])&#123; ++j; &#125; if(j!=1)&#123; nums.splice(i,j-1); &#125; &#125; return nums.length;&#125;; 33搜索旋转排序数组-35.8%-中等https://leetcode-cn.com/problems/search-in-rotated-sorted-array/ 1234567891011121314151617181920212223242526272829303132/** * @param &#123;number[]&#125; nums * @param &#123;number&#125; target * @return &#123;number&#125; */var search = function(nums, target) &#123; return searchs(nums, 0, nums.length-1, target);&#125;;function searchs(nums,l,h,target)&#123; //搜不到返回-1，搜到了返回target if (l &gt; h)&#123; return -1; &#125; var mid = Math.floor((l + h)/2);//[0,1,2,4,5,6,7] if(nums[mid] == target)&#123; return mid; &#125; if(nums[mid] &lt; nums[h])&#123; //旋转在左 [6,7,0,1,2,4,5] if(nums[mid]&lt;target&amp;&amp;target&lt;=nums[h])&#123; return searchs(nums,mid+1,h,target);//在右，右顺序 &#125;else&#123; return searchs(nums,l,mid-1,target);//不在右 &#125; &#125;else&#123; //旋转在右 [2,4,5,6,7,0,1] if(nums[l]&lt;=target&amp;&amp;target&lt;nums[mid])&#123; return searchs(nums,l,mid-1,target);//在左，左顺序 &#125;else&#123; return searchs(nums,mid+1,h,target);//不在右 &#125; &#125;&#125; 43字符串相乘-37.9%-中等https://leetcode-cn.com/problems/multiply-strings/ 123456789101112131415161718192021222324252627282930313233343536373839404142/** * @param &#123;string&#125; num1 * @param &#123;string&#125; num2 * @return &#123;string&#125; */var multiply = function(num1, num2) &#123; var res = ""; var len1 = num1.length; var len2 = num2.length; for(var i=0;i&lt;len1;i++)&#123; for(var j=0;j&lt;len2;j++)&#123; var base = Array(i+j+1).join(0);//i+j个0 eg:"00" //逐位相乘 var tmp = String(Number(num1[len1-1-i])*Number(num2[len2-1-j])) + base; res = addstr(res,tmp); &#125; &#125; while(res[0] == 0 &amp;&amp; res.length &gt; 1)&#123;//"002" -&gt; "2" res = res.slice(1); &#125; return res;&#125;;function addstr(s1, s2) &#123; //字符串相加 var re = ""; if(s1.length&gt;=s2.length)&#123; s2 = Array(s1.length-s2.length+1).join(0) + s2; &#125;else&#123; s1 = Array(s2.length-s1.length+1).join(0) + s1; &#125; var flag = 0; for(let i=0;i&lt;s1.length;i++)&#123; var t = Number(s1[s1.length - 1 - i]) + Number(s2[s1.length - 1 - i]) + flag; if(t &lt; 10)&#123; re = String(t).concat(re); flag = 0; &#125;else&#123; re = String(t-10).concat(re); flag = 1; &#125; &#125; return flag == 0? re:"1"+re;&#125;; 46全排列-66.7%-中等https://leetcode-cn.com/problems/permutations/ 12345678910111213141516171819202122232425/** * @param &#123;number[]&#125; nums * @return &#123;number[][]&#125; */var permute = function(nums) &#123; if(nums.length == 0)&#123; return [[]]; &#125; if(nums.length == 1)&#123; var re = []; re.push(nums); return re; &#125; var nums1 = permute(nums.slice(1)); var num0 = nums[0]; var result = []; for(var i=0;i&lt;nums1.length;i++)&#123; for(var j=0;j&lt;nums1[i].length+1;j++)&#123; var tmp = nums1[i].concat(); tmp.splice(j,0,num0); result.push(tmp); &#125; &#125; return result;&#125;; 53最大子序和-42.9%-简单https://leetcode-cn.com/problems/maximum-subarray/ 123456789101112131415161718/** * @param &#123;number[]&#125; nums * @return &#123;number&#125; */var maxSubArray = function(nums) &#123; var sum = 0; var maxsum = nums[0]; for(var i=0;i&lt;nums.length;i++)&#123; sum +=nums[i]; if(sum &gt; maxsum)&#123; maxsum = sum; &#125; if(sum &lt; 0)&#123; sum = 0; &#125; &#125; return maxsum;&#125;; 54螺旋矩阵-34.0%-中等https://leetcode-cn.com/problems/spiral-matrix/ 12345678910111213141516171819202122232425262728293031/** * @param &#123;number[][]&#125; matrix * @return &#123;number[]&#125; */var spiralOrder = function(matrix) &#123; var arr = []; while(!(matrix.length == 0||matrix[0].length == 0))&#123; var tmp = matrix.shift(); //剪第一行 arr = arr.concat(tmp); if(matrix.length == 0||matrix[0].length == 0)&#123; //[] [[],[]] return arr; &#125; for(var i=0;i&lt;matrix.length;i++)&#123;//剪最后一列 var tmp = matrix[i].pop(); arr.push(tmp); &#125; if(matrix.length == 0||matrix[0].length == 0)&#123; return arr; &#125; var tmp = matrix.pop().reverse();//剪最后一行 arr = arr.concat(tmp); if(matrix.length == 0||matrix[0].length == 0)&#123; return arr; &#125; for(var i=0;i&lt;matrix.length;i++)&#123;//剪第一列 var tmp = matrix[matrix.length-1-i].shift(); arr.push(tmp); &#125; &#125; return arr;&#125;; 59螺旋矩阵II-71.1%-中等https://leetcode-cn.com/problems/spiral-matrix-ii/ 12345678910111213141516171819202122232425262728/** * @param &#123;number&#125; n * @return &#123;number[][]&#125; */var generateMatrix = function(n) &#123; var arr = []; for(var i = 0; i &lt; n; i++) &#123; arr.push(new Array(n).fill(0));//创建n*n零矩阵 &#125; var c = 1; var i = 0; while(c &lt;= n*n)&#123; for(var j=i;j&lt;n-i;j++)&#123;//第一行 arr[i][j] = c++; &#125; for(var j=i+1;j&lt;n-i;j++)&#123;//最后一列 arr[j][n-i-1] = c++; &#125; for(var j=i+1;j&lt;n-i;j++)&#123;//最后一行 arr[n-i-1][n-j-1] = c++; &#125; for(var j=i+1;j&lt;n-i-1;j++)&#123;//第一列 arr[n-j-1][i] = c++; &#125; i++; &#125; return arr;&#125;; 61旋转链表-37.5%-中等https://leetcode-cn.com/problems/rotate-list/ 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** * Definition for singly-linked list. * function ListNode(val) &#123; * this.val = val; * this.next = null; * &#125; *//** * @param &#123;ListNode&#125; head * @param &#123;number&#125; k * @return &#123;ListNode&#125; */var rotateRight = function(head, k) &#123; if(head == null||k==0)&#123; return head; &#125; var h = head; var len = 1;//总长 while(h.next !=null )&#123; ++len; h = h.next; &#125; if(k % len == 0)&#123;//不用旋转 head本身 return head; &#125; if(k &gt; len)&#123;//需旋转 k = k % len; &#125; len = len - k; var ln = new ListNode(0);//0-&gt;1-&gt;2-&gt;3-&gt;NULL var n = ln; while(len != 0)&#123; n.next = head; n = n.next; head = head.next; --len; &#125; n.next = null; var r = head; var lr = r; while(head.next != null)&#123; lr = head; head = head.next; lr = lr.next; &#125; lr.next = ln.next return r;&#125;; 62不同路径-52.6%-中等https://leetcode-cn.com/problems/unique-paths/ 12345678910111213141516171819202122/** * @param &#123;number&#125; m * @param &#123;number&#125; n * @return &#123;number&#125; */var uniquePaths = function(m, n) &#123; var dp = []; for(var i=0;i&lt;m;i++)&#123; //创建m*n零矩阵 dp.push(new Array(n).fill(0)); &#125; for(var i=0;i&lt;m;i++)&#123; for(var j=0;j&lt;n;j++)&#123; if(i==0||j==0)&#123; dp[i][j] = 1; &#125; else&#123; dp[i][j] = dp[i-1][j]+dp[i][j-1] &#125; &#125; &#125; return dp[m-1][n-1];&#125;; 70爬楼梯-44.5%-简单https://leetcode-cn.com/problems/climbing-stairs/ 1234567891011121314151617181920/** * @param &#123;number&#125; n * @return &#123;number&#125; */var climbStairs = function(n) &#123; if(n==1)&#123; return 1; &#125; if(n==2)&#123; return 2; &#125; var i = 1; var j = 2; for(var k=0;k&lt;n-2;k++)&#123; var tmp = i + j; i = j; j = tmp; &#125; return j;&#125;; 78子集-71.6%-中等https://leetcode-cn.com/problems/subsets/ 12345678910111213141516171819/** * @param &#123;number[]&#125; nums * @return &#123;number[][]&#125; */var subsets = function(nums) &#123; var arr = []; arr.push([]); for(var i=0;i&lt;nums.length;i++)&#123; var tmp = []; //tmp深拷贝arr for(var k=0;k&lt;arr.length;k++)&#123; tmp[k] = arr[k].slice(0); &#125; for(var j=0;j&lt;tmp.length;j++)&#123; tmp[j].push(nums[i]); arr.push(tmp[j]); &#125; &#125; return arr;&#125;; 88合并两个有序数组-43.0%-简单https://leetcode-cn.com/problems/merge-two-sorted-lists/ 12345678910111213141516171819202122232425262728/** * Definition for singly-linked list. * function ListNode(val) &#123; * this.val = val; * this.next = null; * &#125; *//** * @param &#123;ListNode&#125; l1 * @param &#123;ListNode&#125; l2 * @return &#123;ListNode&#125; */var mergeTwoLists = function(l1, l2) &#123; var l3 = new ListNode(0); var l4 = l3; while(l1!=null &amp;&amp; l2!=null)&#123; if(l1.val&lt;l2.val)&#123; l4.next = l1; l1 = l1.next; &#125;else&#123; l4.next = l2; l2 = l2.next; &#125; l4 = l4.next; &#125; l4.next = l1==null?l2:l1; return l3.next;&#125;; 89格雷编码-62.7%-中等https://leetcode-cn.com/problems/gray-code/ 123456789101112131415161718/** * @param &#123;number&#125; n * @return &#123;number[]&#125; */// 000 001 011 010 ——// |// 100 101 111 110 &lt;——var grayCode = function(n) &#123; if(n == 0)&#123; return [0]; &#125; var tmp = grayCode(n-1).concat();//深拷贝 for(var i=0;i&lt;tmp.length;i++)&#123; tmp[i] += Math.pow(2,n-1); &#125; tmp.reverse(); return grayCode(n-1).concat(tmp);&#125;; 104二叉树的最大深度-67.4%-简单https://leetcode-cn.com/problems/maximum-depth-of-binary-tree/ 1234567891011121314151617181920/** * Definition for a binary tree node. * function TreeNode(val) &#123; * this.val = val; * this.left = this.right = null; * &#125; *//** * @param &#123;TreeNode&#125; root * @return &#123;number&#125; */var maxDepth = function(root) &#123; if(root == null)&#123; return 0; &#125; var l_height = maxDepth(root.left); var r_height = maxDepth(root.right); return Math.max(l_height,r_height) + 1;&#125;; 121买卖股票的最佳时机-48.5%-简单https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock/ 123456789101112131415/** * @param &#123;number[]&#125; prices * @return &#123;number&#125; */var maxProfit = function(prices) &#123; var maxtmp = 0; for(var i=0;i&lt;prices.length;i++)&#123; for(var j=i+1;j&lt;prices.length;j++)&#123; if(prices[j]-prices[i]&gt;maxtmp)&#123; maxtmp = prices[j]-prices[i]; &#125; &#125; &#125; return maxtmp;&#125;; 122买卖股票的最佳时机II-51.6%-简单https://leetcode-cn.com/problems/best-time-to-buy-and-sell-stock-ii/ 12345678910111213/** * @param &#123;number[]&#125; prices * @return &#123;number&#125; */var maxProfit = function(prices) &#123; var maxtmp = 0; for(var i=0;i&lt;prices.length-1;i++)&#123; if (prices[i]&lt;prices[i+1])&#123; maxtmp += prices[i+1]-prices[i]; &#125; &#125; return maxtmp;&#125;; 124二叉树中的最大路径和-33.8%-困难https://leetcode-cn.com/problems/binary-tree-maximum-path-sum/ 123456789101112131415161718192021222324252627/** * Definition for a binary tree node. * function TreeNode(val) &#123; * this.val = val; * this.left = this.right = null; * &#125; *//** * @param &#123;TreeNode&#125; root * @return &#123;number&#125; */var maxPathSum = function(root) &#123; var len = -Number.MAX_VALUE; submax(root); function submax(t)&#123; if(t == null)&#123; return 0; &#125; var l = Math.max(submax(t.left),0); var r = Math.max(submax(t.right),0); if(t.val + l + r &gt; len)&#123; len = t.val + l + r; &#125; return Math.max(l,r) + t.val; &#125; return len;&#125;; 136只出现一次的数字-59.1%-简单https://leetcode-cn.com/problems/single-number/ 1234567891011121314/** * @param &#123;number[]&#125; nums * @return &#123;number&#125; */var singleNumber = function(nums) &#123; // 交换律：a ^ b ^ c &lt;=&gt; a ^ c ^ b,俩两相同的移到一起 // 相同的数异或为0: n ^ n =&gt; 0,只剩下单个的了 // 任何数于0异或为任何数 0 ^ n =&gt; n var s = 0; for(var i=0;i&lt;nums.length;i++)&#123; s = s^nums[i]; &#125; return s;&#125;; 141环形链表-35.5%-简单https://leetcode-cn.com/problems/linked-list-cycle 1234567891011121314151617181920212223/** * Definition for singly-linked list. * function ListNode(val) &#123; * this.val = val; * this.next = null; * &#125; */ /** * @param &#123;ListNode&#125; head * @return &#123;boolean&#125; */var hasCycle = function(head) &#123; var slow = head, fast = head; while(fast &amp;&amp; fast.next)&#123; slow = slow.next; fast = fast.next.next; if(slow == fast)&#123; return true; &#125; &#125; return false;&#125;; 142环形链表II-34.8%-中等https://leetcode-cn.com/problems/linked-list-cycle-ii/ 123456789101112131415161718192021222324252627282930313233/** * Definition for singly-linked list. * function ListNode(val) &#123; * this.val = val; * this.next = null; * &#125; *//** * @param &#123;ListNode&#125; head * @return &#123;ListNode&#125; */var detectCycle = function(head) &#123; var slow = head,fast = head; var isloop = false; while(fast &amp;&amp; fast.next)&#123; slow = slow.next; fast = fast.next.next; if(slow == fast)&#123;//相遇 isloop = true; break; &#125; &#125; if(isloop)&#123; var l = head; while(l != slow)&#123; l = l.next; slow = slow.next; &#125; return slow; &#125; return null;&#125;; 146LRU缓存机制-39.0%-困难https://leetcode-cn.com/problems/lru-cache/ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * @param &#123;number&#125; capacity *//**1.新加入的数据 ————&gt; ___ 头 &lt;————— ___ | ___ |2.被访问的数据 ___ ————— ___ ___ &lt;———— ___ 尾3.淘汰的数据*/var LRUCache = function(capacity) &#123; this.size = capacity; this.map = &#123;&#125;; this.list = []; &#125;;/** * @param &#123;number&#125; key * @return &#123;number&#125; */LRUCache.prototype.get = function(key) &#123; if(this.map[key] != null)&#123; this.list.splice(this.list.indexOf(key),1);//更新list this.list.unshift(key); return this.map[key]; &#125; return -1;&#125;;/** * @param &#123;number&#125; key * @param &#123;number&#125; value * @return &#123;void&#125; */LRUCache.prototype.put = function(key, value) &#123; var tmp = this.list.indexOf(key); if(tmp != -1)&#123; //key已存在，更新list this.list.splice(tmp,1); &#125; else&#123; //key不存在 if(this.list.length == this.size)&#123; //栈满 var d = this.list.pop(); this.map[d] = null; &#125; &#125; this.list.unshift(key); this.map[key] = value;&#125;;/** * Your LRUCache object will be instantiated and called as such: * var obj = Object.create(LRUCache).createNew(capacity) * var param_1 = obj.get(key) * obj.put(key,value) */ 148排序链表-58.6%-中等https://leetcode-cn.com/problems/sort-list/ 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * Definition for singly-linked list. * function ListNode(val) &#123; * this.val = val; * this.next = null; * &#125; *//** * @param &#123;ListNode&#125; head * @return &#123;ListNode&#125; */var sortList = function(head) &#123; if (!head || !head.next)&#123; return head; &#125; var slow = head,bk = head,fast = head; while(fast &amp;&amp; fast.next )&#123; bk = slow; slow = slow.next; fast = fast.next.next; &#125; bk.next = null; var l1 = sortList(head);//中分，归并 var l2 = sortList(slow); return merge(l1,l2);&#125;;function merge(l1,l2)&#123;//合并有序链表 var l = new ListNode(0); var rl = l; while(l1 &amp;&amp; l2)&#123; if(l1.val &lt; l2.val)&#123; rl.next = l1; l1 = l1.next; &#125;else&#123; rl.next = l2; l2 = l2.next; &#125; rl = rl.next; &#125; rl.next = l1==null?l2:l1; return l.next;&#125; 155最小栈-47.7%-简单https://leetcode-cn.com/problems/min-stack/ 1234567891011121314151617181920212223242526272829303132333435363738394041424344/** * initialize your data structure here. */var MinStack = function() &#123; this.arr = [];&#125;;/** * @param &#123;number&#125; x * @return &#123;void&#125; */MinStack.prototype.push = function(x) &#123; this.arr.push(x);&#125;;/** * @return &#123;void&#125; */MinStack.prototype.pop = function() &#123; this.arr.pop();&#125;;/** * @return &#123;number&#125; */MinStack.prototype.top = function() &#123; return this.arr[this.arr.length-1];&#125;;/** * @return &#123;number&#125; */MinStack.prototype.getMin = function() &#123; return Math.min.apply( &#123;&#125;, this.arr );&#125;;/** * Your MinStack object will be instantiated and called as such: * var obj = Object.create(MinStack).createNew() * obj.push(x) * obj.pop() * var param_3 = obj.top() * var param_4 = obj.getMin() */ 160相交链表-37.2%-简单https://leetcode-cn.com/problems/intersection-of-two-linked-lists/ 12345678910111213141516171819202122232425/** * Definition for singly-linked list. * function ListNode(val) &#123; * this.val = val; * this.next = null; * &#125; *//** * @param &#123;ListNode&#125; headA * @param &#123;ListNode&#125; headB * @return &#123;ListNode&#125; */var getIntersectionNode = function(headA, headB) &#123; if(headA == null || headB == null)&#123; return null; &#125; var pA = headA; var pB = headB; while(pA != pB) &#123; pA = pA == null ? headB : pA.next; pB = pB == null ? headA : pB.next; &#125; return pA;&#125;; 169求众数-58.0%-简单https://leetcode-cn.com/problems/majority-element/ 12345678/** * @param &#123;number[]&#125; nums * @return &#123;number&#125; */var majorityElement = function(nums) &#123; nums.sort(); return nums[Math.floor(nums.length/2)];&#125;; 206反转链表-58.6%-简单https://leetcode-cn.com/problems/reverse-linked-list/ 12345678910111213141516171819202122/** * Definition for singly-linked list. * function ListNode(val) &#123; * this.val = val; * this.next = null; * &#125; *//** * @param &#123;ListNode&#125; head * @return &#123;ListNode&#125; */var reverseList = function(head) &#123; var l = null; while(head != null)&#123; var newl = new ListNode(head.val); var tmp = l; l = newl; l.next = tmp; head = head.next; &#125; return l; &#125;; 215数组中的第K个最大元素-56.6%-中等https://leetcode-cn.com/problems/kth-largest-element-in-an-array/ 123456789101112131415161718192021222324252627282930313233343536373839/** * @param &#123;number[]&#125; nums * @param &#123;number&#125; k * @return &#123;number&#125; */var findKthLargest = function(nums, k) &#123; return fastkmax(nums,0,nums.length-1,k);&#125;;function fastkmax(n,l,r,k)&#123; var mid = partiton(n,l,r);//排序 左边都是比mid小的，右边都是比mid大的 var rk = r - mid + 1; if(rk == k)&#123; return n[mid]; &#125; if(rk &gt; k)&#123; return fastkmax(n,mid + 1,r,k); &#125; return fastkmax(n,l,mid - 1,k -rk );&#125;function partiton(n,l,r)&#123;//一次快排 var base = n[l]; var i = l,j = r,tmp; while(i &lt; j)&#123; while(n[j] &gt;= base &amp;&amp; i &lt; j)&#123; j--; &#125; while(n[i] &lt;= base &amp;&amp; i &lt; j)&#123; i++; &#125; tmp = n[i]; n[i] = n[j]; n[j] = tmp; &#125; n[l] = n[i]; n[i] = base; return i; &#125; 217存在重复元素-47.0%-简单https://leetcode-cn.com/problems/contains-duplicate/ 12345678910111213/** * @param &#123;number[]&#125; nums * @return &#123;boolean&#125; */var containsDuplicate = function(nums) &#123; nums.sort(); for(var i=0;i&lt;nums.length-1;i++)&#123; if(nums[i]==nums[i+1])&#123; return true; &#125; &#125; return false;&#125;; 230二叉搜索树中第K小的元素-62.0%-中等https://leetcode-cn.com/problems/kth-smallest-element-in-a-bst/ 12345678910111213141516171819202122232425/** * Definition for a binary tree node. * function TreeNode(val) &#123; * this.val = val; * this.left = this.right = null; * &#125; *//** * @param &#123;TreeNode&#125; root * @param &#123;number&#125; k * @return &#123;number&#125; */var kthSmallest = function(root, k) &#123; var list = []; order(root,list); return list[k-1];&#125;;function order(root,list)&#123;//中序遍历 if(!root)&#123; return; &#125; order(root.left,list); list.push(root.val); order(root.right,list);&#125; 231 2的幂-44.3%-简单https://leetcode-cn.com/problems/power-of-two/ 12345678910111213/** * @param &#123;number&#125; n * @return &#123;boolean&#125; */var isPowerOfTwo = function(n) &#123; if(n == 0)&#123; return false; &#125; if(n == 1)&#123; return true; &#125; return n%2==0?isPowerOfTwo(n/2):false;&#125;; 235二叉搜索树的最近公共祖先-56.9%-简单https://leetcode-cn.com/problems/lowest-common-ancestor-of-a-binary-search-tree/ 12345678910111213141516171819202122232425262728/** * Definition for a binary tree node. * function TreeNode(val) &#123; * this.val = val; * this.left = this.right = null; * &#125; *//** * @param &#123;TreeNode&#125; root * @param &#123;TreeNode&#125; p * @param &#123;TreeNode&#125; q * @return &#123;TreeNode&#125; */var lowestCommonAncestor = function(root, p, q) &#123; // 二叉搜索树(二叉排序树):若它的左子树不空，则左子树上所有结点的值均小于它的根结点的值; var res = null; lca(root, p , q); function lca(root, p, q)&#123; if((root.val - p.val)*(root.val - q.val) &lt;= 0)&#123; //root为p、q根节点 res = root; &#125;else if(root.val &lt; p.val &amp;&amp; root.val &lt; q.val)&#123; //都在右子树 lca(root.right, p , q); &#125;else&#123; lca(root.left, p , q); &#125; &#125; return res;&#125;; 236二叉树的最近公共祖先-50.4%-中等https://leetcode-cn.com/problems/lowest-common-ancestor-of-a-binary-tree/ 123456789101112131415161718192021222324252627282930313233/** * Definition for a binary tree node. * function TreeNode(val) &#123; * this.val = val; * this.left = this.right = null; * &#125; *//** * @param &#123;TreeNode&#125; root * @param &#123;TreeNode&#125; p * @param &#123;TreeNode&#125; q * @return &#123;TreeNode&#125; */var lowestCommonAncestor = function(root, p, q) &#123; if(!root)&#123; return null; &#125; if(root == p || root == q)&#123; return root; &#125; var l = lowestCommonAncestor(root.left,p,q)//左支存在一个的位置 var r = lowestCommonAncestor(root.right,p,q) if(l &amp;&amp; r)&#123;//左支有一个，右支有一个 return root; &#125; if(l)&#123;//右没有，都在左，先找到的即为根节点 return l; &#125; if(r)&#123; return r; &#125; return null;&#125;; 237删除链表中的节点-69.4%-简单https://leetcode-cn.com/problems/delete-node-in-a-linked-list/ 123456789101112131415/** * Definition for singly-linked list. * function ListNode(val) &#123; * this.val = val; * this.next = null; * &#125; *//** * @param &#123;ListNode&#125; node * @return &#123;void&#125; Do not return anything, modify node in-place instead. */var deleteNode = function(node) &#123; node.val = node.next.val; node.next = node.next.next;&#125;; 238除自身以外数组的乘积-58.8%-中等https://leetcode-cn.com/problems/product-of-array-except-self/ 1234567891011121314151617/** * @param &#123;number[]&#125; nums * @return &#123;number[]&#125; */var productExceptSelf = function(nums) &#123; var arr = []; var l = 1, r = 1; for(var i=0;i&lt;nums.length;i++)&#123;//arr[i] i左边数乘积 arr[i] = l; l *= nums[i]; &#125; for(var j=nums.length-1;j&gt;=0;j--)&#123; arr[j] *= r; r *= nums[j]; &#125; return arr;&#125;; 292Nim游戏-66.4%-简单https://leetcode-cn.com/problems/nim-game/ 1234567/** * @param &#123;number&#125; n * @return &#123;boolean&#125; */var canWinNim = function(n) &#123; return n%4 == 0?false:true;&#125;; 344反转字符串-65.1%-简单https://leetcode-cn.com/problems/reverse-string/ 12345678910/** * @param &#123;character[]&#125; s * @return &#123;void&#125; Do not return anything, modify s in-place instead. */var reverseString = function(s) &#123; for(var i=1;i&lt;s.length;i++)&#123; var tmp = s.splice(i,1)[0]; s.unshift(tmp); &#125;&#125;; 557反转字符串中的单词III-63.5%-简单https://leetcode-cn.com/problems/reverse-words-in-a-string-iii/ 1234567891011/** * @param &#123;string&#125; s * @return &#123;string&#125; */var reverseWords = function(s) &#123; var arr = s.split(" "); for(var i=0;i&lt;arr.length;i++)&#123; arr[i] = arr[i].split("").reverse().join(""); &#125; return arr.join(" ");&#125;;]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>leetcode</tag>
        <tag>JavaScript</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[setTimeout、Promise、Async/Await 异步机制 事件循环]]></title>
    <url>%2F2019%2F02%2F22%2F12%2F</url>
    <content type="text"><![CDATA[javascript是一门单线程语言 将任务分为两类： 同步任务 异步任务 打开网站时，网页的渲染过程就是一大堆同步任务，比如页面骨架和页面元素的渲染。而像加载图片音乐之类占用资源大耗时久的任务，就是异步任务。 同步的进入主线程，异步的进入Event Table并注册函数。当指定的事情完成时，Event Table会将这个函数移入Event Queue。主线程内的任务执行完毕为空，会去Event Queue读取对应的函数，进入主线程执行。上述过程会不断重复，也就是常说的Event Loop(事件循环)。js引擎存在monitoring process进程，会持续不断的检查主线程执行栈是否为空，一旦为空，就会去Event Queue那里检查是否有等待被调用的函数。 举些例子： 1、setTimeout12345console.log('script start') //1. 打印 script startsetTimeout(function()&#123; console.log('settimeout') // 3. 打印 settimeout&#125;) console.log('script end') //2. 打印 script start 输出顺序：script start-&gt;script end-&gt;settimeout 2、PromisePromise本身是同步的立即执行函数， 当在executor中执行resolve或者reject的时候, 此时是异步操作， 会先执行then/catch等，当主栈完成后，才会去调用resolve/reject中存放的方法执行，打印p的时候，是打印的返回结果，一个Promise实例。 123456789console.log('script start') //1. 打印 script startlet promise1 = new Promise(function (resolve) &#123; console.log('promise1') //2. 打印 promise1 resolve() console.log('promise1 end') //3. 打印 promise1 end&#125;).then(function () &#123; console.log('promise2') //5. 打印 promise2&#125;)console.log('script end') //4. 打印 script end // 输出顺序: script start-&gt;promise1-&gt;promise1 end-&gt;script end-&gt;promise2-&gt;settimeout 3、async await1234567891011async function async1()&#123; console.log('async1 start'); //2. 打印 async1 start await async2(); console.log('async1 end') //5. 打印 async1 end&#125;async function async2()&#123; console.log('async2') //3. 打印 async2&#125;console.log('script start'); //1. 打印 script startasync1();console.log('script end') //4. 打印 script end // 输出顺序：script start-&gt;async1 start-&gt;async2-&gt;script end-&gt;async1 end 同时有setTimeout、Promise、Async/Await ，执行顺序又是什么？引入 macro-task(宏任务)：包括整体代码script，setTimeout，setInterval micro-task(微任务)：Promise，process.nextTick 进入整体代码(宏任务)后，开始第一次循环。接着执行所有的微任务。然后再次从宏任务开始，找到其中一个任务队列执行完毕，再执行所有的微任务。 123456789setTimeout(function() &#123; console.log('setTimeout'); //4. 打印 setTimeout&#125;)new Promise(function(resolve) &#123; console.log('promise'); //1. 打印 promise&#125;).then(function() &#123; console.log('then'); //3. 打印 then&#125;)console.log('console'); //2. 打印 console // 输出顺序：promise-&gt;console-&gt;then-&gt;setTimeout 先遇到setTimeout，那么将其回调函数注册后分发到宏任务Event Queue。 接下来遇到了Promise，new Promise立即执行，then函数分发到微任务Event Queue。 遇到console.log()，立即执行。 整体代码script作为第一个宏任务执行结束，发现了then在微任务Event Queue里面，执行。 第一轮事件循环结束了，从宏任务Event Queue开始第二轮循环。发现了宏任务EventQueue中setTimeout对应的回调函数，立即执行。 结束。 再来分析两个案例： 123456789101112131415161718192021222324252627282930313233343536373839console.log('1'); //1. 打印 1 setTimeout(function() &#123; console.log('2'); //5. 打印 2 process.nextTick(function() &#123; console.log('3'); //7. 打印 3 &#125;) new Promise(function(resolve) &#123; console.log('4'); //6. 打印 4 resolve(); &#125;).then(function() &#123; console.log('5') //8. 打印 5 &#125;)&#125;)process.nextTick(function() &#123; console.log('6'); //3. 打印 6&#125;)new Promise(function(resolve) &#123; console.log('7'); //2. 打印 7 resolve();&#125;).then(function() &#123; console.log('8') //4. 打印 8&#125;) setTimeout(function() &#123; console.log('9'); //9. 打印 9 process.nextTick(function() &#123; console.log('10'); //11. 打印 10 &#125;) new Promise(function(resolve) &#123; console.log('11'); //10. 打印 11 resolve(); &#125;).then(function() &#123; console.log('12') //12. 打印 12 &#125;)&#125;) // 输出顺序：1，7，6，8，2，4，3，5，9，11，10，12 123456789101112131415161718192021async function async1() &#123; console.log('async1 start') //2. 打印 async1 start await async2() console.log('async1 end') //7. 打印 async1 end&#125;async function async2() &#123; console.log('async2') //3. 打印 async2&#125;console.log('script start') //1. 打印 script startsetTimeout(function () &#123; console.log('settimeout') //8. 打印 settimeout&#125;)async1()new Promise(function (resolve) &#123; console.log('promise1') //4. 打印 promise1 resolve()&#125;).then(function () &#123; console.log('promise2') //6. 打印 promise2&#125;)console.log('script end') //5. 打印 script end // 输出顺序：script start-&gt;async1 start-&gt;async2-&gt;promise1-&gt;script end-&gt;promise2-&gt;async1 end-&gt;settimeout]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>js</tag>
        <tag>Promise</tag>
        <tag>异步机制</tag>
        <tag>事件循环</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Three.js+tween.js 基础(一)]]></title>
    <url>%2F2019%2F01%2F26%2F20%2F</url>
    <content type="text"><![CDATA[Three.js简介Three.js是众多WebGL三维引擎框架其中之一，源自github的一个开源项目,项目地址：https://github.com/mrdoob/three.js 。可以利用three.js进行网页上的三维场景（机械、建筑、游戏等）创建，能写出在浏览器上流畅运行的3D程序。如果没有前端基础，最好预先学习一点HTML/JavaScript方面的知识。 官方文档：https://threejs.org/docs/index.html#manual/en/introduction/Creating-a-scene 一、基本概念篇：第一个three.js三维场景在Three.js中，要渲染物体到网页中，需要3个基本对象： 场景（scene） 相机（camera） 渲染器（renderer） 场景对应于整个布景空间，相机是拍摄镜头，渲染器用来把拍摄好的场景转换成胶卷。 123456var scene = new THREE.Scene();var camera = new THREE.PerspectiveCamera( 75, window.innerWidth / window.innerHeight, 0.1, 1000 );var renderer = new THREE.WebGLRenderer();renderer.setSize( window.innerWidth, window.innerHeight );document.body.appendChild( renderer.domElement ); 1、场景（scene）1var scene = new THREE.Scene(); 场景对应于整个布景空间，在Threejs中场景就只有一种，用THREE.Scene来表示。 2、相机（camera）在Threejs中最常用的相机有两种 正投影相机THREE.OrthographicCamera 透视投影相机THREE.PerspectiveCamera (1) 正投影相机THREE.OrthographicCamera 12var camera = new THREE.OrthographicCamera( width / - 2, width / 2, height / 2, height / - 2, 1, 1000 );scene.add( camera ); 函数构造：OrthographicCamera( left : Number, right : Number, top : Number, bottom : Number, near : Number, far : Number )参考：https://threejs.org/docs/index.html#api/zh/cameras/OrthographicCamera 正交投影相机示意图如下：(2) 透视投影相机THREE.PerspectiveCamera 12var camera = new THREE.PerspectiveCamera( 45, width / height, 1, 1000 );scene.add( camera ); 函数构造：PerspectiveCamera( fov : Number, aspect : Number, near : Number, far : Number )参考：https://threejs.org/docs/index.html#api/zh/cameras/PerspectiveCamera 透视投影相机示意图如下： 3、 渲染器（renderer）123var renderer = new THREE.WebGLRenderer();renderer.setSize( window.innerWidth, window.innerHeight );document.body.appendChild( renderer.domElement ); 除了创建renderer实例，还需要设置渲染空间的尺寸，一般使用目标屏幕的宽高（window.innerWidth和window.innerHeight），也可以给定一个尺寸。渲染器renderer的domElement元素，表示渲染器中的画布，所有的渲染都是画在domElement上的，所以这里的appendChild表示将这个domElement挂接在body下面，这样渲染的结果就能够在页面中显示了。 4、 添加对象现在的场景中是空的，我们向场景中加入最简单的立方体。 123456var geometry = new THREE.BoxGeometry( 1, 1, 1 ); var material= new THREE.MeshBasicMaterial( &#123; color: 0x4ca7c1 &#125; ); var cube = new THREE.Mesh( geometry, material ); scene.add( cube );camera.position.z = 5; 创建几何体，创建材质，利用几何体和材质创建对象，将对象加入场景scene中。默认情况下，当我们调用scene.add()的时候，物体将会被添加到坐标为(0,0,0)的位置。但这可能会使得摄像机的位置和立方体相互重叠（摄像机位于立方体中）。为了防止这种情况的发生，需要将摄像机稍微向外移动一些。 5、 渲染场景12345function animate() &#123; requestAnimationFrame( animate ); renderer.render( scene, camera );&#125;animate(); 在这里我们创建了一个循环——这使得渲染器能够在每次屏幕刷新时对场景进行绘制（在大多数屏幕上，刷新率一般是60次/秒）。requestAnimationFrame函数就是让浏览器去执行一次参数中的函数，这样通过上面render中调用requestAnimationFrame()函数，requestAnimationFrame()函数又让rander()再执行一次，就形成了我们通常所说的游戏循环了。 6、使立方体动起来在animate()函数中添加 12cube.rotation.x += 0.01;cube.rotation.y += 0.01; 这一段代码将在每一帧时被渲染时调用（正常情况下是60次/秒），这就让立方体有了一个看起来很不错的旋转动画。除了改变立方体的旋转角度、位置，也可以通过改变相机位置角度达到同样动起来的效果。 完整代码（附详细备注）123456789101112131415161718192021222324252627282930313233343536373839404142&lt;html&gt; &lt;head&gt; &lt;title&gt;My first three.js app&lt;/title&gt; &lt;style&gt; body &#123; margin: 0; &#125; canvas &#123; width: 100%; height: 100% &#125; &lt;/style&gt; &lt;/head&gt; &lt;body&gt; &lt;script src="build/three.js"&gt;&lt;/script&gt; &lt;script&gt; // 场景（scene） var scene = new THREE.Scene(); // 相机（camera） var camera = new THREE.PerspectiveCamera( 75, window.innerWidth/window.innerHeight, 0.1, 1000 ); // 渲染器（renderer） var renderer = new THREE.WebGLRenderer(); // 创建renderer实例 renderer.setSize( window.innerWidth, window.innerHeight ); // 设置渲染空间的尺寸 document.body.appendChild( renderer.domElement ); // 渲染出的画布加入页面 // 添加对象 var geometry = new THREE.BoxGeometry( 1, 1, 1 ); // 创建几何体 var material = new THREE.MeshBasicMaterial( &#123; color: 0x4ca7c1 &#125; ); //创建材质 var cube = new THREE.Mesh( geometry, material ); // 用几何体和材质创建对象 scene.add( cube ); // 对象加入场景scene中 camera.position.z = 5; // 渲染场景 var animate = function () &#123; requestAnimationFrame( animate ); cube.rotation.x += 0.01; cube.rotation.y += 0.01; renderer.render( scene, camera ); &#125;; animate(); &lt;/script&gt; &lt;/body&gt;&lt;/html&gt; 在浏览器中的效果： 二、实际应用篇：跳一跳（You_Jump_I_Jump）有了基本概念后，我们来一步步实现跳一跳的代码复原，GitHub地址：https://github.com/zj19941113/You_Jump_I_Jump最终效果图： 1、创建场景与第一个盒子（1）基本元素场景：设置背景颜色 12scene = new THREE.Scene();scene.background = new THREE.Color( 0x8797a4 ); 渲染器：渲染空间尺寸设置为屏幕的宽高 123renderer = new THREE.WebGLRenderer(&#123; antialias: true &#125; );renderer.setSize( window.innerWidth, window.innerHeight );document.body.appendChild( renderer.domElement ); 相机：选择正交投影相机，设置相机位置与朝向 123camera = new THREE.OrthographicCamera( window.innerWidth / - 2, window.innerWidth / 2, window.innerHeight / 2, window.innerHeight / - 2, -1000, 2000 );camera.position.set( 200 , 180 , 200 );camera.lookAt(new THREE.Vector3(0,0,0)); 光源：添加平行光与环境光，平行光让几何体更层次分明，环境光提升整体亮度 123456light = new THREE.AmbientLight( 0xFFFFFF,0.4 );scene.add( light );light2 = new THREE.DirectionalLight(0xFFFFFF,1);light2.position.set(3,4,2);scene.add(light2); 环境光 函数构造：AmbientLight( color : Integer, intensity : Float )color - (参数可选）颜色的rgb数值。缺省值为 0xffffff(白色)。intensity - (参数可选)光照的强度。缺省值为 1。参考：https://threejs.org/docs/index.html#api/zh/lights/AmbientLight 平行光 函数构造：DirectionalLight( color : Integer, intensity : Float )参考：https://threejs.org/docs/index.html#api/zh/lights/DirectionalLight （2）创建地面与盒子12ground = creatGround(0,0,0x8797a4)cube01 = creatcube01(0,0); 函数 creatGround（），创建5000*5000大小，颜色为#8797a4的地面 123456789 function creatGround(x,z,color)&#123; var geometry = new THREE.PlaneGeometry( 5000, 5000, 1, 1 ); var material = new THREE.MeshLambertMaterial(&#123; color:color&#125;); mesh = new THREE.Mesh( geometry,material ); mesh.rotation.x = -Math.PI / 2; mesh.position.set(x,-0.01,z); scene.add(mesh); return mesh;&#125; 函数 creatcube01（），创建一个其中一面有纹理的盒子 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869function creatcube01(x,z) &#123; // 绘制纹理 var canvas = document.createElement('canvas'); canvas.width=100; canvas.height=50; var ctx = canvas.getContext('2d'); ctx.rect(0,0,100,50); ctx.fillStyle="#e86014"; ctx.fill(); ctx.beginPath(); ctx.arc(40,25,18,0,2*Math.PI); ctx.fillStyle="#ffffff"; ctx.fill(); ctx.beginPath(); ctx.arc(45,20,6,0,2*Math.PI); ctx.fillStyle="#e86014"; ctx.fill(); ctx.beginPath(); ctx.arc(65,10,6,0,2*Math.PI); ctx.fillStyle="#ffffff"; ctx.fill(); var texture = new THREE.Texture(canvas); group = canvasOneFace(x,z,texture,0xe86014); return group;&#125;// 创建一个纯色盒子，将纹理贴在平面上覆盖在盒子一面function canvasOneFace(x,z,texture,color) &#123; // 创建纯色盒子，高50，长宽100 var geometry = new THREE.CubeGeometry( 100, 50, 100); var material = new THREE.MeshLambertMaterial( &#123; color:color&#125; ); mesh = new THREE.Mesh( geometry,material); mesh.position.set(x,25,z); // 默认中心在(0,0,0)向上抬25，使盒子在地面上 // 创建100*50的平面，和盒子侧面一样大，材料使用刚绘制的画布作纹理，而不是颜色 var geometry = new THREE.PlaneGeometry( 100,50 ); var material = new THREE.MeshLambertMaterial(&#123;map:texture&#125;); texture.needsUpdate = true; mesh1 = new THREE.Mesh( geometry,material); // 使平面和盒子侧面基本重合，差了0.01的距离，纹理能遮盖住盒子本身颜色 mesh1.rotation.y = Math.PI / 2; mesh1.position.set(x+50.01,25,z); // 创建阴影 Shadow = makeShadow(); Shadow.position.set( x-30 , 0 , z+8 ); // 组合起来方便使用 group = new THREE.Object3D(); group.add(mesh,mesh1,Shadow); scene.add(group); return group;&#125;// 创建阴影function makeShadow(x,z)&#123; // 创建平面，贴上图片作为纹理 vargeometry = new THREE.PlaneGeometry( 116, 160, 1, 1 ); // 加载图片作为纹理 var texture = new THREE.TextureLoader().load( "source/shadow.png" ); var material = new THREE.MeshBasicMaterial(&#123;map:texture&#125;); material.transparent = true; // 材质透明 meshShadow = new THREE.Mesh( geometry,material); // 调整阴影位置 meshShadow.rotation.x = -Math.PI / 2; meshShadow.rotation.z = Math.PI / 2 ; meshShadow.position.set( x-30 , 0 , z+6 ); return meshShadow;&#125; 效果： 因为之后要创建各种盒子，所以这里用了函数，方便之后调用。用画布绘制右侧面的纹理，创建橙色的纯色盒子，将纹理贴在平面上覆盖在盒子侧面，最后再创建阴影，同样是创建平面贴上阴影图片作为纹理。 画布作为纹理： 12345678910var canvas = document.createElement('canvas');canvas.width=100;canvas.height=50;var ctx = canvas.getContext('2d');// 画布的绘制// ……// 画布的绘制var texture = new THREE.Texture(canvas);var material = new THREE.MeshLambertMaterial(&#123;map:texture&#125;);texture.needsUpdate = true; png图片作为纹理： 123 var texture = new THREE.TextureLoader().load( "source/shadow.png" ); var material = new THREE.MeshBasicMaterial(&#123;map:texture&#125;);material.transparent = true; // 材质透明 注：尽量重用geometry，material，使性能优化。 （3）窗口自适应1window.addEventListener( 'resize', onWindowResize, false ); 当窗口大小改变时，触发函数 onWindowResize()，实时改变相机参数。 123456789101112function onWindowResize() &#123; width = document.body.clientWidth; height = document.body.clientHeight; camera.left = width / - 2; camera.right = width / 2; camera.top = height / 2; camera.bottom = height / -2; // 更新相机投影矩阵，在相机任何参数被改变以后必须被调用 camera.updateProjectionMatrix(); renderer.setSize( width,height );&#125; 效果：完整代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;title&gt;&lt;/title&gt; &lt;meta charset="utf-8"&gt;&lt;/head&gt;&lt;body&gt;&lt;script src="build/three.js"&gt;&lt;/script&gt;&lt;script&gt; var camera, scene, renderer; init(); animate(); function init() &#123; scene = new THREE.Scene(); scene.background = new THREE.Color( 0x8797a4 ); renderer = new THREE.WebGLRenderer(&#123; antialias: true &#125; ); renderer.setSize( window.innerWidth, window.innerHeight ); document.body.appendChild( renderer.domElement ); camera = new THREE.OrthographicCamera( window.innerWidth / - 2, window.innerWidth / 2, window.innerHeight / 2, window.innerHeight / - 2, -1000, 2000 ); camera.position.set( 200 , 180 , 200 ); camera.lookAt(new THREE.Vector3(0,0,0)); light = new THREE.AmbientLight( 0xFFFFFF,0.4 ); scene.add( light ); light2 = new THREE.DirectionalLight(0xFFFFFF,1); light2.position.set(3,4,2); scene.add(light2); ground = creatGround(0,0,0x8797a4) cube01 = creatcube01(0,0); window.addEventListener( 'resize', onWindowResize, false ); &#125; function creatGround(x,z,color)&#123; var geometry = new THREE.PlaneGeometry( 5000, 5000, 1, 1 ); var material = new THREE.MeshLambertMaterial(&#123; color:color&#125;); mesh = new THREE.Mesh( geometry,material ); mesh.rotation.x = -Math.PI / 2; mesh.position.set(x,-0.01,z); scene.add(mesh); return mesh; &#125; function creatcube01(x,z) &#123; // 绘制纹理 var canvas = document.createElement('canvas'); canvas.width=100; canvas.height=50; var ctx = canvas.getContext('2d'); ctx.rect(0,0,100,50); ctx.fillStyle="#e86014"; ctx.fill(); ctx.beginPath(); ctx.arc(40,25,18,0,2*Math.PI); ctx.fillStyle="#ffffff"; ctx.fill(); ctx.beginPath(); ctx.arc(45,20,6,0,2*Math.PI); ctx.fillStyle="#e86014"; ctx.fill(); ctx.beginPath(); ctx.arc(65,10,6,0,2*Math.PI); ctx.fillStyle="#ffffff"; ctx.fill(); var texture = new THREE.Texture(canvas); group = canvasOneFace(x,z,texture,0xe86014); return group; &#125; // 创建一个纯色盒子，将纹理贴在平面上覆盖在盒子一面 function canvasOneFace(x,z,texture,color) &#123; // 创建纯色盒子，高50，长宽100 var geometry = new THREE.CubeGeometry( 100, 50, 100); var material = new THREE.MeshLambertMaterial( &#123; color:color&#125; ); mesh = new THREE.Mesh( geometry,material); mesh.position.set(x,25,z); // 默认中心在(0,0,0)向上抬25，使盒子在地面上 // 创建100*50的平面，和盒子侧面一样大，材料使用刚绘制的画布作纹理，而不是颜色 var geometry = new THREE.PlaneGeometry( 100,50 ); var material = new THREE.MeshLambertMaterial(&#123;map:texture&#125;); texture.needsUpdate = true; mesh1 = new THREE.Mesh( geometry,material); // 使平面和盒子侧面基本重合，差了0.01的距离，纹理能遮盖住盒子本身颜色 mesh1.rotation.y = Math.PI / 2; mesh1.position.set(x+50.01,25,z); // 创建阴影 Shadow = makeShadow(); Shadow.position.set( x-30 , 0 , z+8 ); // 组合起来方便使用 group = new THREE.Object3D(); group.add(mesh,mesh1,Shadow); scene.add(group); return group; &#125; // 创建阴影 function makeShadow(x,z)&#123; // 创建平面，贴上图片作为纹理 var geometry = new THREE.PlaneGeometry( 116, 160, 1, 1 ); // 加载图片作为纹理 var texture = new THREE.TextureLoader().load( "source/shadow.png" ); var material = new THREE.MeshBasicMaterial(&#123;map:texture&#125;); material.transparent = true; // 材质透明 meshShadow = new THREE.Mesh( geometry,material); // 调整阴影位置 meshShadow.rotation.x = -Math.PI / 2; meshShadow.rotation.z = Math.PI / 2 ; meshShadow.position.set( x-30 , 0 , z+6 ); return meshShadow; &#125; function onWindowResize() &#123; width = document.body.clientWidth; height = document.body.clientHeight; camera.left = width / - 2; camera.right = width / 2; camera.top = height / 2; camera.bottom = height / -2; // 更新相机投影矩阵，在相机任何参数被改变以后必须被调用 camera.updateProjectionMatrix(); renderer.setSize( width,height ); &#125; function animate() &#123; requestAnimationFrame( animate ); renderer.render( scene, camera ); &#125;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 2、更多盒子的生成所有盒子的生成函数，见 https://github.com/zj19941113/You_Jump_I_Jump/blob/master/zjFirstStep.htmlboxs大合照： 3、性能与调试创建stats对象进行性能监控帧数：图形处理器每秒钟能够刷新几次，通常用fps（Frames Per Second）来表示。帧数越高，画面的感觉就会越好。所以大多数游戏都会有超过30的FPS。我们设置性能监视器来监视FPS。（1）引入Stats.js文件，官方文档：https://github.com/mrdoob/stats.js 1&lt;script src="js/Stats.js"&gt;&lt;/script&gt; （2）将stats对象加入到html网页中 1var stats; 12345stats = new Stats();stats.domElement.style.position = 'absolute';stats.domElement.style.right = '8px';stats.domElement.style.top = '8px';document.body.appendChild( stats.domElement ); （3）在 animate() 中调用stats.update()统计时间和帧数 12345function animate() &#123; requestAnimationFrame( animate ); stats.update(); // 调用stats.update()统计时间和帧数 renderer.render( scene, camera );&#125; 其中FPS表示：上一秒的帧数，这个值越大越好，一般都为60左右。点击后变成另一个视图。MS表示渲染一帧需要的毫秒数，这个数字是越小越好，再次点击又可以回到FPS视图中。效果： OrbitControls控制器辅助调试（1）引入controls/OrbitControls.js文件，官方文档：https://github.com/mrdoob/three.js/blob/017f2a9eb17820772359b1e1dbca2b626c9f32b1/examples/jsm/controls/OrbitControls.js 1&lt;script src="js/controls/OrbitControls.js"&gt;&lt;/script&gt; （2）controls控制器设置 1var controls; 1234567controls = new THREE.OrbitControls( camera, renderer.domElement );controls.enableDamping = true; controls.dampingFactor = 0.25;controls.screenSpacePanning = false;controls.minDistance = 100;controls.maxDistance = 500;controls.maxPolarAngle = Math.PI / 2; （3）在 animate() 中调用controls.update() 123456function animate() &#123; requestAnimationFrame( animate ); stats.update(); // 调用stats.update()统计时间和帧数 controls.update(); // 调用controls.update() renderer.render( scene, camera );&#125; OrbitControls控制器的作用是，鼠标左键进行视角的旋转，右键控制平移，滚轮控制缩放。效果：完整代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889&lt;!DOCTYPE html&gt;&lt;html lang="en"&gt;&lt;head&gt; &lt;title&gt;&lt;/title&gt; &lt;meta charset="utf-8"&gt;&lt;/head&gt;&lt;body&gt;&lt;script src="build/three.js"&gt;&lt;/script&gt;&lt;script src="js/Stats.js"&gt;&lt;/script&gt;&lt;script src="js/controls/OrbitControls.js"&gt;&lt;/script&gt;&lt;script&gt; var camera, scene, renderer; var stats, controls; init(); animate(); function init() &#123; scene = new THREE.Scene(); scene.background = new THREE.Color( 0x8797a4 ); renderer = new THREE.WebGLRenderer(&#123; antialias: true &#125; ); renderer.setSize( window.innerWidth, window.innerHeight ); document.body.appendChild( renderer.domElement ); stats = new Stats(); stats.domElement.style.position = 'absolute'; stats.domElement.style.right = '8px'; stats.domElement.style.top = '8px'; document.body.appendChild( stats.domElement ); camera = new THREE.OrthographicCamera( window.innerWidth / - 2, window.innerWidth / 2, window.innerHeight / 2, window.innerHeight / - 2, -1000, 2000 ); camera.position.set( 200 , 180 , 200 ); camera.lookAt(new THREE.Vector3(0,0,0)); controls = new THREE.OrbitControls( camera, renderer.domElement ); controls.enableDamping = true; controls.dampingFactor = 0.25; controls.screenSpacePanning = false; controls.minDistance = 100; controls.maxDistance = 500; controls.maxPolarAngle = Math.PI / 2; light = new THREE.AmbientLight( 0xFFFFFF,0.4 ); scene.add( light ); light2 = new THREE.DirectionalLight(0xFFFFFF,1); light2.position.set(3,4,2); scene.add(light2); ground = creatGround(0,0,0x8797a4) cube01 = creatcube01(0,0); window.addEventListener( 'resize', onWindowResize, false ); &#125; function creatGround(x,z,color)&#123; // 地面创建函数 // …… // 地面创建函数 &#125; function creatcube01(x,z) &#123; // 盒子创建函数 // …… // 盒子创建函数 &#125; function onWindowResize() &#123; // 窗口自适应函数 // …… // 窗口自适应函数 &#125; function animate() &#123; requestAnimationFrame( animate ); stats.update(); controls.update(); renderer.render( scene, camera ); &#125;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; 有效利用Controls和Helper能使视图展示更加出色，从而方便代码的编写。这里有官方提供的一些实例（包括模型加载、控制器、动画等相关内容）：https://github.com/mrdoob/three.js/tree/017f2a9eb17820772359b1e1dbca2b626c9f32b1/examples 下一篇：Three.js+tween.js 基础(二) 将介绍tween.js 相关知识，完成player的跳动动画，盒子的随机生成，分数统计等功能。]]></content>
      <categories>
        <category>Three</category>
      </categories>
      <tags>
        <tag>tween</tag>
        <tag>Three</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用浏览器本地存储localStorage进行换肤，改变页面字体颜色样式]]></title>
    <url>%2F2019%2F01%2F24%2F11%2F</url>
    <content type="text"><![CDATA[效果刷新页面，界面效果依然不会改变。 相关代码html: 1234567891011121314&lt;div class="setting_tool iconfont"&gt; &lt;a class="back2top" style="display:none;"&gt;&lt;i class="czs-arrow-up-l"&gt;&lt;/i&gt;&lt;/a&gt; &lt;a class="socolor"&gt;&lt;i class="czs-clothes-l"&gt;&lt;/i&gt;&lt;/a&gt; &lt;div class="c"&gt; &lt;ul&gt; &lt;li class="color undefined"&gt;默认&lt;/li&gt; &lt;li class="color sepia"&gt;护眼&lt;/li&gt; &lt;li class="color night"&gt;夜晚&lt;/li&gt; &lt;li class="hr"&gt;&lt;/li&gt; &lt;li class="font serif"&gt;Serif&lt;/li&gt; &lt;li class="font sans"&gt;Sans&lt;/li&gt; &lt;/ul&gt; &lt;/div&gt;&lt;/div&gt; javascrip: 12345678910111213141516171819202122232425$("li.undefined").click(function()&#123; localStorage.removeItem('color_style'); $('.color-style').remove(); if(localStorage.color_style) $('head').append("&lt;style class='color-style'&gt;" + localStorage.color_style + "&lt;/style&gt;"); &#125;); $("li.sepia").click(function()&#123; localStorage.setItem("color_style", ".menu .menu-item a.current-menu-item &#123;color:#704214;&#125;.article-title h2:hover&#123;color:#361e07&#125;【写你需要的改之后的样式】") $('.color-style').remove(); if(localStorage.color_style) $('head').append("&lt;style class='color-style'&gt;" + localStorage.color_style + "&lt;/style&gt;"); &#125;); $("li.night").click(function()&#123; localStorage.setItem("color_style", ".menu .menu-item a.current-menu-item &#123;color:#bdcadb;&#125;.article-title h2:hover&#123;color:white&#125;【写你需要的改之后的样式】") $('.color-style').remove(); if(localStorage.color_style) $('head').append("&lt;style class='color-style'&gt;" + localStorage.color_style + "&lt;/style&gt;"); &#125;); $("li.serif").click(function()&#123; localStorage.setItem("font_style","body&#123;font-family:serif;&#125;") $('.font-style').remove(); if(localStorage.font_style) $('head').append("&lt;style class='font-style'&gt;" + localStorage.font_style + "&lt;/style&gt;"); &#125;); $("li.sans").click(function()&#123; localStorage.removeItem('font_style'); $('.font-style').remove(); if(localStorage.font_style) $('head').append("&lt;style class='font-style'&gt;" + localStorage.font_style + "&lt;/style&gt;"); &#125;); 在页面头部添加 1234&lt;script&gt; if(localStorage.color_style) $('head').append("&lt;style class='color-style'&gt;" + localStorage.color_style + "&lt;/style&gt;"); if(localStorage.font_style) $('head').append("&lt;style class='font-style'&gt;" + localStorage.font_style + "&lt;/style&gt;"); &lt;/script&gt; 原理点击选择相应的颜色，字体后，添加本地存储localStorage.color_style和localStorage.font_style，用添加的本地存储样式覆盖掉现有的样式，在页面头部添加的js保证刷新跳转后页面样式不变。 本地存储localStorage 相关知识1、简介localStorage会可以将数据直接存储到本地，相当于一个5M大小的针对于前端页面的数据库。在IE8以上的IE版本才支持localStorage这个属性。localStorage属于永久性存储，如果存储内容多的话会消耗内存空间，会导致页面变卡。注意：存入的数据只能以字符串形式存入。 2、存储与清除// 存储localStorage.setItem(“font_style”,”body{font-family:serif;}”) // 清除键值对localStorage.removeItem(‘color_style’); // 清空localStoragelocalStorage.clear(); 3、处理JSON数据var obj = {“a”: 1,”b”: 2}; // 转化为JSON字符串obj = JSON.stringify(obj); //保存localStorage.setItem(“temp2”, obj); //JSON字符串转JSON对象obj=JSON.parse(localStorage.getItem(“temp2”));]]></content>
      <categories>
        <category>前端</category>
      </categories>
      <tags>
        <tag>localStorage</tag>
        <tag>换肤</tag>
        <tag>前端</tag>
        <tag>web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[c++ 实现红外图与深度图结合的人脸识别+活体检测（Ubuntu +dlib）]]></title>
    <url>%2F2019%2F01%2F23%2F19%2F</url>
    <content type="text"><![CDATA[准备:1、Ubuntu C++ 编译dlib库https://blog.csdn.net/ffcjjhv/article/details/84660869 2、数据+模型下载https://pan.baidu.com/s/1jIoW6BSa5nkGWNipL7sxVQ 其中包括： candidate-face.zip（人脸库：包含29个正面人脸红外图） allface.zip（测试人脸集：包括29个人，每人13种脸部姿态下的红外图与深度图） shape_predictor_68_face_landmarks.dat（人脸68关键点检测器） dlib_face_recognition_resnet_model_v1.dat（人脸识别模型） 代码分析:主要包含3个函数： 123/* 函数声明 *//* 人脸库训练 */int candidates_train(const char *facesFile,std::vector&lt;matrix&lt;float,0,1&gt;&gt;&amp;candidates_descriptors,std::vector&lt;string&gt;&amp;candidates); 运行candidates_train，遍历人脸库candidate-face文件夹，将候选人名单存入candidates，将候选人人脸特征存入candidates_descriptors。 12/* 输出人脸位置 返回识别结果 */string face_location(const char *imgFile,std::vector&lt;int&gt;&amp;locates, std::vector&lt;matrix&lt;float,0,1&gt;&gt;&amp;candidates_descriptors,std::vector&lt;string&gt;&amp;candidates); 运行face_location，得到要测试图片的人脸特征，计算与每个候选人人脸特征的欧式距离，得到距离最小值的编号，从而在candidates中得到识别结果。在函数运行过程中，将人脸位置信息传入locates，进行活体检测。 12/* 判断是否为活体 */bool liveness_detection(const char *DeepFile,std::vector&lt;int&gt;&amp;locates); 运行liveness_detection，利用深度图与人脸位置信息进行活体检测，主要利用了RANSAC算法。 运行结果：补充：python版的看这里 https://blog.csdn.net/ffcjjhv/article/details/84637986python版的在allface文件夹共375张图片上的识别精度为99.469%，出错的两张是allleft姿态，侧转角度很大。模型算法和这篇c++版是一样的，只是语言不一样。可以看出识别效果还是很好的。this_is_who.py在test-face文件夹中的批量测试结果： 完整代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322#include &lt;dlib/dnn.h&gt;#include &lt;dlib/image_processing/frontal_face_detector.h&gt;#include &lt;dlib/image_processing.h&gt;#include &lt;dlib/gui_widgets.h&gt;#include &lt;dlib/image_io.h&gt;#include &lt;iostream&gt;#include &lt;dirent.h&gt;#include &lt;string.h&gt;#include &lt;stdio.h&gt;#include &lt;math.h&gt;using namespace dlib;using namespace std;/* 函数声明 *//* 人脸库训练 */int candidates_train(const char *facesFile,std::vector&lt;matrix&lt;float,0,1&gt;&gt;&amp;candidates_descriptors,std::vector&lt;string&gt;&amp;candidates);/* 输出人脸位置 返回识别结果 */string face_location(const char *imgFile,std::vector&lt;int&gt;&amp;locates, std::vector&lt;matrix&lt;float,0,1&gt;&gt;&amp;candidates_descriptors,std::vector&lt;string&gt;&amp;candidates);/* 判断是否为活体 */bool liveness_detection(const char *DeepFile,std::vector&lt;int&gt;&amp;locates); const int IMG_HEIGHT = 720;const int IMG_WIDTH = 1280;template &lt;template &lt;int,template&lt;typename&gt;class,int,typename&gt; class block, int N, template&lt;typename&gt;class BN, typename SUBNET&gt;using residual = add_prev1&lt;block&lt;N,BN,1,tag1&lt;SUBNET&gt;&gt;&gt;;template &lt;template &lt;int,template&lt;typename&gt;class,int,typename&gt; class block, int N, template&lt;typename&gt;class BN, typename SUBNET&gt;using residual_down = add_prev2&lt;avg_pool&lt;2,2,2,2,skip1&lt;tag2&lt;block&lt;N,BN,2,tag1&lt;SUBNET&gt;&gt;&gt;&gt;&gt;&gt;;template &lt;int N, template &lt;typename&gt; class BN, int stride, typename SUBNET&gt; using block = BN&lt;con&lt;N,3,3,1,1,relu&lt;BN&lt;con&lt;N,3,3,stride,stride,SUBNET&gt;&gt;&gt;&gt;&gt;;template &lt;int N, typename SUBNET&gt; using ares = relu&lt;residual&lt;block,N,affine,SUBNET&gt;&gt;;template &lt;int N, typename SUBNET&gt; using ares_down = relu&lt;residual_down&lt;block,N,affine,SUBNET&gt;&gt;;template &lt;typename SUBNET&gt; using alevel0 = ares_down&lt;256,SUBNET&gt;;template &lt;typename SUBNET&gt; using alevel1 = ares&lt;256,ares&lt;256,ares_down&lt;256,SUBNET&gt;&gt;&gt;;template &lt;typename SUBNET&gt; using alevel2 = ares&lt;128,ares&lt;128,ares_down&lt;128,SUBNET&gt;&gt;&gt;;template &lt;typename SUBNET&gt; using alevel3 = ares&lt;64,ares&lt;64,ares&lt;64,ares_down&lt;64,SUBNET&gt;&gt;&gt;&gt;;template &lt;typename SUBNET&gt; using alevel4 = ares&lt;32,ares&lt;32,ares&lt;32,SUBNET&gt;&gt;&gt;;using anet_type = loss_metric&lt;fc_no_bias&lt;128,avg_pool_everything&lt; alevel0&lt; alevel1&lt; alevel2&lt; alevel3&lt; alevel4&lt; max_pool&lt;3,3,2,2,relu&lt;affine&lt;con&lt;32,7,7,2,2, input_rgb_image_sized&lt;150&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;;frontal_face_detector detector = get_frontal_face_detector(); // 人脸正脸检测器shape_predictor sp; //人脸关键点检测器anet_type net; // 人脸识别模型int main()&#123; const char *imgFile = "/home/zhoujie/data/allface/0002_IR_allleft.jpg"; const char *facesFile = "/home/zhoujie/cProject/dlib_test/candidate-face/"; deserialize("shape_predictor_68_face_landmarks.dat") &gt;&gt; sp; deserialize("dlib_face_recognition_resnet_model_v1.dat") &gt;&gt; net; std::vector&lt;matrix&lt;float,0,1&gt;&gt; candidates_descriptors; std::vector&lt;string&gt; candidates; /* 人脸库训练 */ candidates_train(facesFile,candidates_descriptors,candidates); std::vector&lt;int&gt; locates; /* 输出人脸位置 返回识别结果 */ string who = face_location(imgFile, locates, candidates_descriptors,candidates); cout &lt;&lt; "识别结果：" &lt;&lt; endl; cout &lt;&lt; "This is " &lt;&lt; who &lt;&lt; endl; //深度图与红外图是水平翻转的 locates[0] = IMG_WIDTH - locates[0] -locates[2]; // printf("%d,%d,%d,%d\n", locates[0],locates[1],locates[2],locates[3]); const char *DeepFile = "/home/zhoujie/data/allface/0002_raw_allleft.raw"; bool IS_FACE; /* 判断是否为活体 */ IS_FACE = liveness_detection( DeepFile, locates); // printf("RESULT : %d\n", IS_FACE);&#125;/* 人脸库训练 */int candidates_train(const char *facesFile,std::vector&lt;matrix&lt;float,0,1&gt;&gt;&amp;candidates_descriptors,std::vector&lt;string&gt;&amp;candidates)&#123; DIR *dir; struct dirent *ptr; char base[30]; const char *pick=".jpg"; //需要的子串; char IRfile[100]; char *name; int face_num = 0; std::vector&lt;matrix&lt;rgb_pixel&gt;&gt; faces; if ((dir=opendir(facesFile)) == NULL) &#123; perror("Open dir error..."); exit(1); &#125; while ((ptr=readdir(dir)) != NULL) &#123; strcpy(base, ptr-&gt;d_name); if(strstr(base,pick)) &#123; printf("training image:%s\n",base); strcpy(IRfile, facesFile); strcat(IRfile, base); name = strtok(base, "_"); // printf("%s\n",name); string candidate = name; cout &lt;&lt; "candidate: " &lt;&lt; candidate &lt;&lt; endl; candidates.push_back(candidate); matrix&lt;rgb_pixel&gt; img; load_image(img, IRfile); std::vector&lt;rectangle&gt; dets = detector(img); full_object_detection shape = sp(img, dets[0]); matrix&lt;rgb_pixel&gt; face_chip; extract_image_chip(img, get_face_chip_details(shape,150,0.25), face_chip); faces.push_back(move(face_chip)); face_num += 1; &#125; &#125; candidates_descriptors = net(faces); printf("训练结果：\n共训练 %d 张人脸\n", face_num); closedir(dir); return 0;&#125;/* 函数 输出人脸位置 返回识别结果 */string face_location(const char* imgFile,std::vector&lt;int&gt;&amp;locates, std::vector&lt;matrix&lt;float,0,1&gt;&gt;&amp;candidates_descriptors,std::vector&lt;string&gt;&amp;candidates)&#123; cout &lt;&lt; "processing image " &lt;&lt; imgFile &lt;&lt; endl; matrix&lt;rgb_pixel&gt; img; load_image(img, imgFile); std::vector&lt;rectangle&gt; dets = detector(img); // cout &lt;&lt; "Number of faces detected: " &lt;&lt; dets.size() &lt;&lt; endl; locates.push_back(dets[0].left()); locates.push_back(dets[0].top()); locates.push_back(dets[0].right() - dets[0].left()); locates.push_back(dets[0].bottom() - dets[0].top()); full_object_detection shape = sp(img, dets[0]); std::vector&lt;matrix&lt;rgb_pixel&gt;&gt; faces; matrix&lt;rgb_pixel&gt; face_chip; extract_image_chip(img, get_face_chip_details(shape,150,0.25), face_chip); faces.push_back(move(face_chip)); std::vector&lt;matrix&lt;float,0,1&gt;&gt; face_descriptors = net(faces); float distance; float best_distance = length(face_descriptors[0]-candidates_descriptors[0]); // printf("k = 0 ,best_distance = %f\n",best_distance); size_t candidates_num = candidates_descriptors.size(); int candidates_num_int = static_cast&lt;int&gt;(candidates_num); // printf("candidates_num_int : %d\n", candidates_num_int); int best_k = 0; for (int k = 1; k &lt; candidates_num_int; k++) &#123; distance = length(face_descriptors[0]-candidates_descriptors[k]); // printf("k = %d ,distance = %f\n",k,distance); if (distance &lt; best_distance) &#123; best_distance = distance; best_k = k; &#125; &#125; string who; if (best_distance &lt; 0.6) &#123; who = candidates[best_k]; &#125; else&#123; who = "Unknow"; &#125; return who;&#125;/* 函数判断是否为活体 */bool liveness_detection(const char *DeepFile,std::vector&lt;int&gt;&amp;locates)&#123; const int ITER = 5000; // 随机取点次数 const float PLANE_OR_NOT = 0.2; // 判断是否为平面的分界线 const int SIGMA = 1; typedef unsigned short UNIT16; // 从.raw读取二进制16位数据到MatDATA UNIT16 MatDATA[IMG_HEIGHT*IMG_WIDTH]; FILE *fp = NULL; fp = fopen( DeepFile, "rb" ); size_t sizeRead = fread(MatDATA,sizeof(UNIT16),IMG_HEIGHT*IMG_WIDTH,fp); if (sizeRead != IMG_HEIGHT*IMG_WIDTH) &#123; printf("error!\n"); &#125; fclose(fp); int n = 0; int i,j; int COL = locates[0],ROW = locates[1],FACE_WIDTH = locates[2],FACE_HEIGHT = locates[3]; //位置信息 // txt :157 66 172 198 , 取行66：66+198,列取157：157+172 int faceno0_num = FACE_HEIGHT*FACE_WIDTH -1; int FaceDATA[3][100000]; n = 0; for(i = 1;i&lt; FACE_HEIGHT+1;i++) &#123; for(j= 1;j&lt; FACE_WIDTH+1;j++) &#123; if (MatDATA[IMG_WIDTH*(ROW+i-2)+COL+j-2] == 0) &#123; faceno0_num -= 1; // 非零深度点个数为 faceno0_num+1 continue; &#125; FaceDATA[1][n] = i; FaceDATA[0][n] = j; FaceDATA[2][n] = MatDATA[IMG_WIDTH*(ROW+i-2)+COL+j-2]; n += 1; &#125; &#125; // int test = 0; // printf("%d,%d,%d,%d\n",test,FaceDATA[0][test],FaceDATA[1][test],FaceDATA[2][test]); int pretotal = 0; // 符合拟合模型的数据的个数 int x[3],y[3],z[3]; // 随机取三个点 srand((unsigned)time(NULL)); float a,b,c; // 拟合平面方程 z=ax+by+c // float besta,bestb,bestc; // 最佳参数 int rand_num[3]; float check,distance; int total = 0; for(i = 0; i &lt; ITER; i++) &#123; do&#123; rand_num[0] = std::rand()%faceno0_num; rand_num[1] = std::rand()%faceno0_num; rand_num[2] = std::rand()%faceno0_num; &#125;while(rand_num[0] == rand_num[1] || rand_num[0] == rand_num[2] || rand_num[1] == rand_num[2]); for(n = 0; n &lt; 3; n++ ) &#123; x[n] = FaceDATA[0][rand_num[n]]; y[n] = FaceDATA[1][rand_num[n]]; z[n] = FaceDATA[2][rand_num[n]]; // printf("%d,%d,%d,%d\n", x[n],y[n],z[n],n); &#125; check = (x[0]-x[1])*(y[0]-y[2]) - (x[0]-x[2])*(y[0]-y[1]); if ( check == 0) // 防止提示浮点数例外 (核心已转储) &#123; i -= 1; continue; &#125; a = ( (z[0]-z[1])*(y[0]-y[2]) - (z[0]-z[2])*(y[0]-y[1]) )*1.0/( (x[0]-x[1])*(y[0]-y[2]) - (x[0]-x[2])*(y[0]-y[1]) ); if (y[0] == y[2]) // 防止提示浮点数例外 (核心已转储) &#123; i -= 1; continue; &#125; b = ((z[0] - z[2]) - a * (x[0] - x[2]))*1.0/(y[0]-y[2]); c = z[0]- a * x[0] - b * y[0]; // printf("%f,%f,%f\n",a,b,c); total = 0; for(n = 0; n &lt; faceno0_num +1 ; n++ ) &#123; distance = fabs(a*FaceDATA[0][n] + b*FaceDATA[1][n] - 1*FaceDATA[2][n] + c*1); if (distance &lt; SIGMA) &#123; total +=1; &#125; &#125; // printf("%d,%f,%d\n",i,distance,total); if (total &gt; pretotal) // 找到符合拟合平面数据最多的拟合平面 &#123; pretotal=total; // besta = a; // bestb = b; // bestc = c; &#125; &#125; float pretotal_ary = pretotal *1.0/ faceno0_num ; printf("活体检测结果：\npretotal_ary=%f,",pretotal_ary); bool IS_FACE; if (pretotal_ary &lt; PLANE_OR_NOT) &#123; IS_FACE = true; printf("是人脸\n"); &#125; else &#123; IS_FACE = false; printf("不是人脸\n"); &#125; // printf("%d\n", IS_FACE); return IS_FACE;&#125;]]></content>
      <categories>
        <category>人脸识别</category>
        <category>活体检测</category>
      </categories>
      <tags>
        <tag>活体检测</tag>
        <tag>dlib</tag>
        <tag>人脸识别</tag>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微信小程序 人脸追踪+人脸识别+视频上传 页面]]></title>
    <url>%2F2019%2F01%2F17%2F10%2F</url>
    <content type="text"><![CDATA[效果图：调用的百度人脸识别api，免费账户就可以，虽然有QPS限制但对于自己学习使用是足够的。 代码：wxml： 12345678910111213141516171819202122232425262728&lt;view class=&quot;page-body&quot;&gt; &lt;view class=&quot;page-body-wrapper&quot;&gt; &lt;camera device-position=&quot;front&quot; flash=&quot;off&quot; binderror=&quot;error&quot; style=&quot;width:100%;height:300px;&quot;&gt;&lt;/camera&gt; &lt;canvas wx:if=&quot;&#123;&#123;canvasshow&#125;&#125;&quot; style=&quot;width: 100%;height:300px;position:absolute;&quot; canvas-id=&quot;canvas&quot;&gt;&lt;/canvas&gt; &lt;view class=&quot;btn-area&quot;&gt; &lt;button type=&quot;primary&quot; bindtap=&quot;track&quot; style=&apos;background-color:#31859c;&apos; data-trackshow=&quot;&#123;&#123;trackshow&#125;&#125;&quot;&gt;&#123;&#123;trackshow&#125;&#125;&lt;/button&gt; &lt;/view&gt; &lt;view class=&quot;btn-area&quot;&gt; &lt;button type=&quot;primary&quot; bindtap=&quot;search&quot; style=&apos;background-color:#31859c;&apos;&gt;进行人脸识别&lt;/button&gt; &lt;/view&gt; &lt;view class=&quot;btn-area&quot;&gt; &lt;button type=&quot;primary&quot; bindtap=&quot;startRecord&quot; style=&apos;background-color:#31859c;&apos;&gt;开始录像&lt;/button&gt; &lt;/view&gt; &lt;view class=&quot;btn-area&quot;&gt; &lt;button type=&quot;primary&quot; bindtap=&quot;stopRecord&quot; style=&apos;background-color:#31859c;&apos;&gt;结束录像&lt;/button&gt; &lt;/view&gt; &lt;view class=&quot;preview-tips&quot;&gt;识别结果:&#123;&#123;who&#125;&#125;&lt;/view&gt; &lt;view wx:if=&quot;&#123;&#123;src&#125;&#125;&quot; style=&apos;display:flex;width:100%&apos;&gt; &lt;image mode=&quot;aspectFit&quot; src=&quot;&#123;&#123;src&#125;&#125;&quot; class=&apos;result-img&apos;&gt;&lt;/image&gt; &lt;canvas style=&quot;width: 100%;height:300px;position:absolute;&quot; canvas-id=&quot;canvasresult&quot;&gt;&lt;/canvas&gt; &lt;/view&gt; &lt;view wx:if=&quot;&#123;&#123;videoSrc&#125;&#125;&quot; class=&quot;preview-tips&quot;&gt;视频预览&lt;/view&gt; &lt;video wx:if=&quot;&#123;&#123;videoSrc&#125;&#125;&quot; class=&quot;video&quot; src=&quot;&#123;&#123;videoSrc&#125;&#125;&quot;&gt;&lt;/video&gt; &lt;view wx:if=&quot;&#123;&#123;videoSrc&#125;&#125;&quot; class=&quot;btn-area&quot;&gt; &lt;button type=&quot;primary&quot; bindtap=&quot;uploadRecord&quot; style=&apos;background-color:#31859c;&apos;&gt;上传该录像&lt;/button&gt; &lt;/view&gt; &lt;/view&gt;&lt;/view&gt; js: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363var app = getApp();Page(&#123; data: &#123; src:&quot;&quot;, fengmian:&quot;&quot;, videoSrc:&quot;&quot;, who:&quot;&quot;, openid: &quot;&quot;, token: &quot;&quot;, windowWidth: 0, trackshow: &quot;进行人脸追踪&quot;, canvasshow:true &#125;, onLoad() &#123; var that = this //屏幕宽度 var sysInfo = wx.getSystemInfoSync() that.setData(&#123; windowWidth: sysInfo.windowWidth, &#125;) that.ctx = wx.createCameraContext() console.log(&quot;onLoad&quot;), that.setData(&#123; openid: app.globalData.openid, token: app.globalData.token &#125;); &#125;, onReady: function () &#123; // this.takePhoto() // this.interval = setInterval(this.takePhoto, 500) &#125;, track (e)&#123; var that =this if (e.target.dataset.trackshow ==&quot;进行人脸追踪&quot;)&#123; that.setData(&#123; trackshow: &quot;停止人脸追踪&quot;, canvasshow: true &#125;) that.takePhoto() that.interval = setInterval(this.takePhoto, 500) &#125;else&#123; clearInterval(that.interval) that.setData(&#123; trackshow: &quot;进行人脸追踪&quot;, canvasshow: false &#125;) &#125; &#125;, takePhoto() &#123; console.log(&quot;takePhoto&quot;) var that = this var takephonewidth var takephoneheight that.ctx.takePhoto(&#123; quality: &apos;low&apos;, success: (res) =&gt; &#123; // console.log(res.tempImagePath), // 获取图片真实宽高 wx.getImageInfo(&#123; src: res.tempImagePath, success: function (res) &#123; takephonewidth= res.width, takephoneheight = res.height &#125; &#125;) wx.getFileSystemManager().readFile(&#123; filePath: res.tempImagePath, //选择图片返回的相对路径 encoding: &apos;base64&apos;, //编码格式 success: res =&gt; &#123; //成功的回调 // console.log(&apos;data:image/png;base64,&apos; + res.data), wx.request(&#123; url: &quot;https://aip.baidubce.com/rest/2.0/face/v3/detect?access_token=【填自己的】&quot;, data: &#123; image:res.data, image_type:&quot;BASE64&quot;, max_face_num:10 &#125;, method: &apos;POST&apos;, dataType: &quot;json&quot;, header: &#123; &apos;content-type&apos;: &apos;application/json&apos; &#125;, success: function (res) &#123; console.log(res.data); if (res.data.error_code === 0) &#123; var ctx = wx.createContext() ctx.setStrokeStyle(&apos;#31859c&apos;) ctx.lineWidth = 3 for (let j = 0; j &lt; res.data.result.face_num; j++)&#123; var cavansl = res.data.result.face_list[j].location.left / takephonewidth * that.data.windowWidth var cavanst = res.data.result.face_list[j].location.top / takephoneheight * 300 var cavansw = res.data.result.face_list[j].location.width / takephonewidth * that.data.windowWidth var cavansh = res.data.result.face_list[j].location.height / takephoneheight * 300 ctx.strokeRect(cavansl, cavanst, cavansw, cavansh) &#125; wx.drawCanvas(&#123; canvasId: &apos;canvas&apos;, actions: ctx.getActions() &#125;) &#125;else&#123; var ctx = wx.createContext() ctx.setStrokeStyle(&apos;#31859c&apos;) ctx.lineWidth = 3 wx.drawCanvas(&#123; canvasId: &apos;canvas&apos;, actions: ctx.getActions() &#125;) &#125; &#125;, &#125;) &#125; &#125;) &#125; &#125;) &#125;, search()&#123; var that = this var takephonewidth var takephoneheight that.ctx.takePhoto(&#123; quality: &apos;heigh&apos;, success: (res) =&gt; &#123; // console.log(res.tempImagePath), // 获取图片真实宽高 wx.getImageInfo(&#123; src: res.tempImagePath, success: function (res) &#123; takephonewidth = res.width, takephoneheight = res.height &#125; &#125;) that.setData(&#123; src: res.tempImagePath &#125;), wx.getFileSystemManager().readFile(&#123; filePath: that.data.src, //选择图片返回的相对路径 encoding: &apos;base64&apos;, //编码格式 success: res =&gt; &#123; wx.request(&#123; url: &quot;https://aip.baidubce.com/rest/2.0/face/v3/multi-search?access_token=【填自己的】&quot;, data: &#123; image: res.data, image_type: &quot;BASE64&quot;, group_id_list: &quot;camera000001&quot;, max_face_num: 10, match_threshold: 60, &#125;, method: &apos;POST&apos;, dataType: &quot;json&quot;, header: &#123; &apos;content-type&apos;: &apos;application/json&apos; &#125;, success: function (res) &#123; console.log(res.data); var ctx = wx.createContext() if (res.data.error_code === 0) &#123; ctx.setStrokeStyle(&apos;#31859c&apos;) ctx.setFillStyle(&apos;#31859c&apos;); ctx.lineWidth = 3 for (let j = 0; j &lt; res.data.result.face_num; j++) &#123; var cavansl = res.data.result.face_list[j].location.left / takephonewidth * that.data.windowWidth var cavanst = res.data.result.face_list[j].location.top / takephoneheight * 300 var cavansw = res.data.result.face_list[j].location.width / takephonewidth * that.data.windowWidth var cavansh = res.data.result.face_list[j].location.height / takephoneheight * 300 var cavanstext = res.data.result.face_list[j].user_list[0].user_id + &quot; &quot; + res.data.result.face_list[j].user_list[0].score.toFixed(0) + &quot;%&quot; ctx.setFontSize(14); ctx.fillText(cavanstext, cavansl, cavanst-2) ctx.strokeRect(cavansl, cavanst, cavansw, cavansh) &#125; wx.drawCanvas(&#123; canvasId: &apos;canvasresult&apos;, actions: ctx.getActions() &#125;) &#125; else &#123; var ctx = wx.createContext() ctx.setStrokeStyle(&apos;#31859c&apos;) ctx.lineWidth = 3 wx.drawCanvas(&#123; canvasId: &apos;canvasresult&apos;, actions: ctx.getActions() &#125;) &#125; &#125;, &#125;) &#125; &#125;) &#125; &#125;) &#125;, startRecord() &#123; this.ctx.startRecord(&#123; success: (res) =&gt; &#123; console.log(&apos;startRecord&apos;) &#125;, &#125;) &#125;, stopRecord() &#123; this.ctx.stopRecord(&#123; success: (res) =&gt; &#123; console.log(res) this.setData(&#123; fengmian: res.tempThumbPath, videoSrc: res.tempVideoPath &#125;) &#125; &#125;) &#125;, uploadRecord() &#123; var that = this; wx.showLoading(&#123; title: &apos;上传中&apos;, &#125;) //获取摄像头信息 wx.request(&#123; url: app.globalData.urlHeader + &apos;/login/cameralist&apos;, data: &#123; openid: app.globalData.openid, token: app.globalData.token &#125;, method: &apos;POST&apos;, header: &#123; &apos;content-type&apos;: &apos;application/json&apos; &#125;, success: function (res) &#123; if (res.data.code === 0) &#123; if (res.data.data.cameras == null) &#123; wx.request(&#123; url: app.globalData.urlHeader + &apos;/login/addcamera&apos;, data: &#123; openid: app.globalData.openid, token: app.globalData.token, camera: &quot;phone&quot; &#125;, method: &apos;POST&apos;, header: &#123; &apos;content-type&apos;: &apos;application/json&apos; &#125;, success: function (res) &#123; if (res.data.code === 0) &#123; console.log(&apos;添加成功&apos;) &#125; else &#123; console.log(res.data.error) &#125; &#125; &#125;) &#125; else &#123; var cameras = res.data.data.cameras if (cameras.includes(&quot;phone&quot;)) &#123; return false &#125; else &#123; wx.request(&#123; url: app.globalData.urlHeader + &apos;/login/addcamera&apos;, data: &#123; openid: app.globalData.openid, token: app.globalData.token, camera: &quot;phone&quot; &#125;, method: &apos;POST&apos;, header: &#123; &apos;content-type&apos;: &apos;application/json&apos; &#125;, success: function (res) &#123; if (res.data.code === 0) &#123; console.log(&apos;添加成功&apos;) &#125; else &#123; console.log(res.data.error) &#125; &#125; &#125;) &#125; &#125; &#125; else &#123; wx.hideLoading() console.log(&apos;获取摄像头列表失败！&apos; + res.data.error) wx.showToast(&#123; title: &apos;获取摄像头列表失败！&apos;, image: &apos;../../img/about.png&apos;, duration: 1000 &#125;) &#125; &#125; &#125;) wx.uploadFile(&#123; url: app.globalData.urlHeader + &apos;/upload&apos;, filePath: that.data.videoSrc, name: &apos;file&apos;, formData: &#123; &apos;cameraid&apos;:&apos;phone&apos;, &apos;openid&apos;: that.data.openid, &apos;token&apos;: that.data.token, &#125;, success: function (res) &#123; console.log(res.data); var result = JSON.parse(res.data).data.filename console.log(result); wx.uploadFile(&#123; url: app.globalData.urlHeader + &apos;/upload/fengmian&apos;, filePath: that.data.fengmian, name: &apos;file&apos;, formData: &#123; &apos;openid&apos;: that.data.openid, &apos;token&apos;: that.data.token, &apos;name&apos;: result &#125;, success(res) &#123; console.log( res.data); that.setData(&#123; fengmian: &quot;&quot;, videoSrc:&quot;&quot; &#125;), wx.hideLoading() wx.showToast(&#123; title: &apos;上传成功&apos;, icon: &apos;success&apos;, duration: 2000 &#125;) &#125;, fail(res) &#123; wx.hideLoading() wx.showToast(&#123; title: &apos;上传失败&apos;, image: &apos;../../img/about.png&apos;, duration: 2000 &#125;) &#125; &#125;) &#125;, fail(res) &#123; wx.hideLoading() wx.showToast(&#123; title: &apos;上传失败&apos;, image: &apos;../../img/about.png&apos;, duration: 2000 &#125;) &#125; &#125;) &#125;, onUnload: function () &#123; var that=this clearInterval(that.interval) &#125;, error(e) &#123; console.log(e.detail) &#125;&#125;) wxss： 123456789101112131415161718192021222324252627282930313233343536373839.preview-tips &#123; margin: 50rpx 0 30rpx; &#125;.video &#123; margin: 20rpx auto; width: 100%; height: 300px;&#125;page &#123; background-color: #F8F8F8; height: 100%; font-size: 32rpx; line-height: 1.6;&#125;.page-body &#123; padding: 20rpx 0;&#125;.page-body-wrapper &#123; display: flex; flex-direction: column; align-items: center; width: 100%;&#125;.btn-area &#123; margin-top: 40rpx; box-sizing: border-box; width: 100%; padding: 0 30rpx;&#125;.result-img&#123;width:100%;height:300px;&#125; 自己的access_token获取看这里：http://ai.baidu.com/docs#/Face-Detect-V3/top在手机端的主页效果，第二个即为演示图上传的视频更新：改了识别结果显示方式，增加了多人识别。效果：类似的骨架提取：]]></content>
      <categories>
        <category>微信小程序</category>
      </categories>
      <tags>
        <tag>微信小程序</tag>
        <tag>人脸追踪</tag>
        <tag>视频上传</tag>
        <tag>百度AI sdk</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matterport mask_rcnn配置运行 Ubuntu Jupyter&pycharm]]></title>
    <url>%2F2018%2F12%2F17%2F8%2F</url>
    <content type="text"><![CDATA[mask_rcnn github地址：https://github.com/matterport/Mask_RCNN 一、配置1、安装 AnacondaAnaconda下载地址：https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/ 2、python 3.6#创建名为tensorflow的python36环境 12conda create -n tensorflow python=3.6 //创建环境source activate tensorflow //激活环境 3、配置环境（1）安装Tensorflow(CPU) 1pip install tensorflow （2）安装keras 1pip install keras （3）安装opencv 1pip install opencv-python （4）安装其他依赖包 1234pip install cythonpip install scikit-imagepip install theanopip install jupyter （5）安装pycocotools#下载地址：https://github.com/waleedka/coco 解压后，cd到PythonAPI里面，输入 1make -j4 二、运行1、下载Mask-RCNNgithub地址：https://github.com/matterport/Mask_RCNN 2、下载coco权重文件 文件mask_rcnn_coco.h5 （246MB）下载地址：https://github.com/matterport/Mask_RCNN/releases若下载过慢，百度网盘： https://pan.baidu.com/s/18OYXRM3Fpqmk1cs9vH5WNA下载完成以后放在Mask_RCNN目录下 3、运行Mask_RCNN的demo（1）Jupyter运行Mask_RCNN根目录下输入： 1jupyter notebook 稍等片刻，会在浏览器打开页面，点击进入 samples目录，点击demo.ipynp进入代码运行页面 把pycocotools路径加到系统路径 1sys.path.append('/home/zhoujie/下载/coco-master/PythonAPI') 修改后点击保存按钮，再点击重启服务然后运行整个代码出现结果： （2）pycharm运行Mask_RCNN根目录下输入： 1jupyter notebook 稍等片刻，会在浏览器打开页面，点击进入 samples目录，点击demo.ipynp进入代码运行页面点击文件-下载-Python（.py），将demo.py保存到samples文件夹 把pycocotools路径加到系统路径 1sys.path.append('/home/zhoujie/下载/coco-master/PythonAPI') 再注释掉get_ipython().run_line_magic(&#39;matplotlib&#39;, &#39;inline&#39;)就可以运行demo.py了运行结果：]]></content>
      <categories>
        <category>算法</category>
        <category>配置</category>
      </categories>
      <tags>
        <tag>mask_rcnn</tag>
        <tag>Jupyter</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微信小程序跳一跳html版复原（three.js+tween.js）]]></title>
    <url>%2F2018%2F12%2F13%2F7%2F</url>
    <content type="text"><![CDATA[一、浏览器运行需要安装Tomcat后以类似 http://localhost:8080/zjgame/zjgame.html的方式访问，否则本地图片无法加载。注：本项目基于Three.js，是对微信小游戏跳一跳的html版改写，只供研究学习使用。 效果图GitHub：https://github.com/zj19941113/You_Jump_I_Jump ,只供个人研究学习使用。Three.js+tween.js 基础(一)：https://blog.csdn.net/ffcjjhv/article/details/86632320配置腾讯云服务器 通过域名访问自己的网页tomcat：https://blog.csdn.net/ffcjjhv/article/details/85140212 二、在桌面运行 electron打包1、安装 node.js#下载：https://nodejs.org/en/cmd输入node -v 和npm -v查看是否安装成功 2、安装 electron命令行下载淘宝镜像命令工具 cnpm 1npm install cnpm -g --registry=http://registry.npm.taobao.org 用 cnpm 命令安装 electron 1cnpm install electron -g cmd输入electron -v查看是否安装成功 3、运行下载：https://github.com/zj19941113/You_Jump_I_Jump ，并解压，将zjgame.html重命名为index.html。 复制粘贴下面的 package.json 和 main.js文件，最终目录如图在package.json中： 12345&#123; "name" : "your-app", "version" : "0.1.0", "main" : "main.js"&#125; 在main.js中 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455const &#123;app, BrowserWindow&#125; = require('electron')const path = require('path')const url = require('url')// Keep a global reference of the window object, if you don't, the window will// be closed automatically when the JavaScript object is garbage collected.let winfunction createWindow () &#123; // Create the browser window. win = new BrowserWindow(&#123;width: 800, height: 600&#125;) // and load the index.html of the app. win.loadURL(url.format(&#123; pathname: path.join(__dirname, 'index.html'), protocol: 'file:', slashes: true &#125;)) // Open the DevTools. win.webContents.openDevTools() // Emitted when the window is closed. win.on('closed', () =&gt; &#123; // Dereference the window object, usually you would store windows // in an array if your app supports multi windows, this is the time // when you should delete the corresponding element. win = null &#125;)&#125;// This method will be called when Electron has finished// initialization and is ready to create browser windows.// Some APIs can only be used after this event occurs.app.on('ready', createWindow)// Quit when all windows are closed.app.on('window-all-closed', () =&gt; &#123; // On macOS it is common for applications and their menu bar // to stay active until the user quits explicitly with Cmd + Q if (process.platform !== 'darwin') &#123; app.quit() &#125;&#125;)app.on('activate', () =&gt; &#123; // On macOS it's common to re-create a window in the app when the // dock icon is clicked and there are no other windows open. if (win === null) &#123; createWindow() &#125;&#125;)// In this file you can include the rest of your app's specific main process// code. You can also put them in separate files and require them here. 在项目目录下运行 1electron . 4、electron-packager打包为.exe（1）全局安装electron-packager1npm install electron-packager -g （2）运行打包命令1electron-packager . YOU_JUMP_I_JUMP --win --out outApp --arch=x64 --app-version 1.0.0 --electron-version 5.0.0 --overwrite --ignore=node_modules 在outApp\YOU_JUMP_I_JUMP-win32-x64\文件夹下生成可执行文件YOU_JUMP_I_JUMP.exe 5、源码加密在YOU_JUMP_I_JUMP-win32-x64\resources\app里有写的源码。写的代码完全暴露在用户电脑上是非常不安全的，可以通过electron 自带的加密功能解决这个问题。 （1）全局安装 asar1npm install asar -g （2）使用asar指令进行加密在resources目录下使用asar指令进行加密 1asar pack ./app app.asar 将原来的app文件夹删除，这样生成的app.asar就加密了之前的源代码双击YOU_JUMP_I_JUMP.exe运行]]></content>
      <categories>
        <category>微信小程序</category>
        <category>three</category>
      </categories>
      <tags>
        <tag>微信小程序</tag>
        <tag>跳一跳</tag>
        <tag>three</tag>
        <tag>tween</tag>
        <tag>electron</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu下编译C++与python版Dlib库，附新建样例程序（dlib+opencv）]]></title>
    <url>%2F2018%2F11%2F30%2F6%2F</url>
    <content type="text"><![CDATA[Win10+python36+opencv编译配置Dlib（anoconda）实时人脸识别看这里：https://blog.csdn.net/ffcjjhv/article/details/84992881 下载Dlib进入官网：http://dlib.net/ ，点击左下角Download dlib ver.19.16 ，下载后解压。 安装cmake运行 sudo apt-get install cmake 安装cmake ，如果提醒需要apt-get update，那就先sudo apt-get update，然后再执行 sudo apt-get install cmake 1. Python 编译dlib +opencv（只编译C++版，可跳过此步）编译dlibdlib根目录下运行python setup.py install 安装opencv运行 pip install opencv-python 测试自己的程序下载： https://github.com/zj19941113/Face_Recognition_dlib运行python facerec_68point.py 得到识别结果all-face-result.jpg 在anoconda下编译dlib +opencv在dlib根目录下运行：conda create -n py36 python=3.6source activate py36python setup.py installpip install opencv-python 下载： https://github.com/zj19941113/Face_Recognition_dlib 解压后，在当前项目根目录运行： source activate py36python facerec_68point.py 2. C++ 编译dlib +opencv（在dlib根目录下已编译过python版也不影响）仍然先进入dlib根目录下mkdir build //如果已经编译过python版，此步略过cd buildcmake .. -DDLIB_USE_CUDA=0cmake --build . --config Releasesudo make install 到此dlib已经编译好了 opencv的编译参考 https://blog.csdn.net/cocoaqin/article/details/78163171 测试自己的程序新建文件夹dlib_test dlib_test.cpp文件：打开 http://dlib.net/face_landmark_detection_ex.cpp.html ,拷贝全文粘贴到dlib_test.cpp shape_predictor_68_face_landmarks.dat文件：点击：http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 下载面部特征点检测模型shape_predictor_68_face_landmarks.dat.bz2 ，运行bunzip2 shape_predictor_68_face_landmarks.dat.bz2 进行解压 CMakeLists.txt文件：12345678910111213141516cmake_minimum_required(VERSION 2.8.4) PROJECT(dlib_test) SET(CMAKE_CXX_FLAGS "$&#123;CMAKE_CXX_FLAGS&#125; -std=c++11 -O2")IF(CMAKE_CXX_COMPILER_ID STREQUAL "Clang") SET(CMAKE_CXX_FLAGS "$&#123;CMAKE_CXX_FLAGS&#125; -Weverything") ELSEIF(CMAKE_CXX_COMPILER_ID STREQUAL "GNU") SET(CMAKE_CXX_FLAGS "$&#123;CMAKE_CXX_FLAGS&#125; -Wall -Wextra") ENDIF() INCLUDE(/home/zhoujie/dlib-19.16/dlib/cmake) //需修改为自己的路径ADD_EXECUTABLE(dlib_test dlib_test.cpp) TARGET_LINK_LIBRARIES(dlib_test dlib) 注：INCLUDE(/home/zhoujie/dlib-19.16/dlib/cmake) 需修改为自己的路径 运行：在 dlib_test 文件夹根目录运行cmake .make./dlib_test shape_predictor_68_face_landmarks.dat all-face.jpg 运行结果：]]></content>
      <categories>
        <category>算法</category>
        <category>人脸识别</category>
      </categories>
      <tags>
        <tag>dlib</tag>
        <tag>c++</tag>
        <tag>ubuntu</tag>
        <tag>python</tag>
        <tag>人脸识别</tag>
        <tag>配置</tag>
        <tag>cmake</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu+dlib+opencv摄像头实时人脸识别（含训练人脸库）]]></title>
    <url>%2F2018%2F11%2F29%2F5%2F</url>
    <content type="text"><![CDATA[环境 Ubuntu 16.04 opencv 3.0 for python3.6 pip install opencv-python dlib 19.16 模型下载人脸关键点检测器 predictor_path=&quot;shape_predictor_68_face_landmarks.dat人脸识别模型 face_rec_model_path = &quot;dlib_face_recognition_resnet_model_v1.dat含人脸库candidate-face中人脸不同表情的测试数据集 test-face.zip 解压后与上述文件均置于根目录下下载地址 ： 百度云盘 https://pan.baidu.com/s/1h01sfvf5KWU6_7c2-i5HTQ 运行运行python candidate_train.py 获得人脸库特征信息，存储在candidates.npy 与 candidates.txt 中 。 candidate_train.py文件： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182# -*- coding: UTF-8 -*-import os,dlib,numpyimport cv2# 1.人脸关键点检测器predictor_path = "shape_predictor_68_face_landmarks.dat"# 2.人脸识别模型face_rec_model_path = "dlib_face_recognition_resnet_model_v1.dat"# 3.候选人脸文件夹faces_folder_path = "candidate-face"# 4.需识别的人脸img_path = "test-face/0001_IR_allleft.jpg"# 5.识别结果存放文件夹faceRect_path = "faceRec"# 1.加载正脸检测器detector = dlib.get_frontal_face_detector()# 2.加载人脸关键点检测器sp = dlib.shape_predictor(predictor_path)# 3. 加载人脸识别模型facerec = dlib.face_recognition_model_v1(face_rec_model_path)# 候选人脸描述子listcandidates = []filelist = os.listdir(faces_folder_path)count = 0for fn in filelist: count = count+1descriptors = numpy.zeros(shape=(count,128))n = 0for file in filelist: f = os.path.join(faces_folder_path,file) #if os.path.splitext(file)[1] == ".jpg" #文件扩展名 print("Processing file: &#123;&#125;".format(f)) img = cv2.imread(f) # 1.人脸检测 dets = detector(img, 1) for k, d in enumerate(dets): # 2.关键点检测 shape = sp(img, d) # 3.描述子提取，128D向量 face_descriptor = facerec.compute_face_descriptor(img, shape) # 转换为numpy array v = numpy.array(face_descriptor) descriptors[n] = v # descriptors.append(v) candidates.append(os.path.splitext(file)[0]) n += 1 for d in dets: # print("faceRec locate:",d) # print(type(d)) # 使用opencv在原图上画出人脸位置 left_top = (dlib.rectangle.left(d), dlib.rectangle.top(d)) right_bottom = (dlib.rectangle.right(d), dlib.rectangle.bottom(d)) cv2.rectangle(img, left_top, right_bottom, (0, 255, 0), 2, cv2.LINE_AA) # cv2.imwrite(os.path.join(faceRect_path,file), img)numpy.save('candidates.npy',descriptors)file= open('candidates.txt', 'w')for candidate in candidates: file.write(candidate) file.write('\n')file.close() 运行 python facerec_68point.py 得到识别结果all-face-result.jpg。 facerec_68point.py文件： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071# -*- coding: UTF-8 -*-import dlibimport cv2import numpy# 待检测图片img_path = "all-face.jpg"# 人脸关键点检测器predictor_path="shape_predictor_68_face_landmarks.dat"# 人脸识别模型face_rec_model_path = "dlib_face_recognition_resnet_model_v1.dat"# 候选人文件candidate_npydata_path = "candidates.npy"candidate_path = "candidates.txt"# 加载正脸检测器detector = dlib.get_frontal_face_detector()# 加载人脸关键点检测器sp = dlib.shape_predictor(predictor_path)# 加载人脸识别模型facerec = dlib.face_recognition_model_v1(face_rec_model_path)# 候选人脸描述子list# 读取候选人数据npy_data=numpy.load(candidate_npydata_path)descriptors=npy_data.tolist()# 候选人名单candidate = []file=open(candidate_path, 'r')list_read = file.readlines()for name in list_read: name = name.strip('\n') candidate.append(name)print("Processing file: &#123;&#125;".format(img_path))img = cv2.imread(img_path)# 1.人脸检测dets = detector(img, 1)print("Number of faces detected: &#123;&#125;".format(len(dets)))for k, d in enumerate(dets): # 2.关键点检测 shape = sp(img, d) face_descriptor = facerec.compute_face_descriptor(img, shape) d_test2 = numpy.array(face_descriptor) # 计算欧式距离 dist = [] for i in descriptors: dist_ = numpy.linalg.norm(i - d_test2) dist.append(dist_) num = dist.index(min(dist)) # 返回最小值 left_top = (dlib.rectangle.left(d), dlib.rectangle.top(d)) right_bottom = (dlib.rectangle.right(d), dlib.rectangle.bottom(d)) cv2.rectangle(img, left_top, right_bottom, (0, 255, 0), 2, cv2.LINE_AA) text_point = (dlib.rectangle.left(d), dlib.rectangle.top(d) - 5) cv2.putText(img, candidate[num], text_point, cv2.FONT_HERSHEY_PLAIN, 2.0, (255, 255, 255), 2, 1) # 标出facecv2.imwrite('all-face-result.jpg', img)# cv2.imshow("img",img) # 转成ＢＧＲ显示## cv2.waitKey(0)# cv2.destroyAllWindows() 运行 this_is_who_camera.py 打开摄像头进行实时的人脸识别 this_is_who_camera.py文件： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596# -*- coding: UTF-8 -*-import dlib,numpy import cv2 import time# 1.人脸关键点检测器predictor_path = "shape_predictor_68_face_landmarks.dat"# 2.人脸识别模型face_rec_model_path = "dlib_face_recognition_resnet_model_v1.dat"# 3.候选人文件candidate_npydata_path = "candidates.npy"candidate_path = "candidates.txt"# 4.储存截图目录path_screenshots = "screenShots/"# 加载正脸检测器detector = dlib.get_frontal_face_detector()# 加载人脸关键点检测器sp = dlib.shape_predictor(predictor_path)# 加载人脸识别模型facerec = dlib.face_recognition_model_v1(face_rec_model_path)# 候选人脸描述子list# 读取候选人数据npy_data=numpy.load(candidate_npydata_path)descriptors=npy_data.tolist()# 候选人名单candidate = []file=open(candidate_path, 'r')list_read = file.readlines()for name in list_read: name = name.strip('\n') candidate.append(name)# 创建 cv2 摄像头对象cv2.namedWindow("camera", 1)cap = cv2.VideoCapture(0)cap.set(3, 480)# 截图 screenshots 的计数器cnt = 0while (cap.isOpened()): #isOpened() 检测摄像头是否处于打开状态 ret, img = cap.read() #把摄像头获取的图像信息保存之img变量 if ret == True: #如果摄像头读取图像成功 # 添加提示 cv2.putText(img, "press 'S': screenshot", (20, 400), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1, cv2.LINE_AA) cv2.putText(img, "press 'Q': quit", (20, 450), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1, cv2.LINE_AA) # img_gray = cv2.cvtColor(im_rd, cv2.COLOR_RGB2GRAY) dets = detector(img, 1) if len(dets) != 0: # 检测到人脸 for k, d in enumerate(dets): # 关键点检测 shape = sp(img, d) # 遍历所有点圈出来 for pt in shape.parts(): pt_pos = (pt.x, pt.y) cv2.circle(img, pt_pos, 2, (0, 255, 0), 1) face_descriptor = facerec.compute_face_descriptor(img, shape) d_test2 = numpy.array(face_descriptor) # 计算欧式距离 dist = [] for i in descriptors: dist_ = numpy.linalg.norm(i - d_test2) dist.append(dist_) num = dist.index(min(dist)) # 返回最小值 left_top = (dlib.rectangle.left(d), dlib.rectangle.top(d)) right_bottom = (dlib.rectangle.right(d), dlib.rectangle.bottom(d)) cv2.rectangle(img, left_top, right_bottom, (0, 255, 0), 2, cv2.LINE_AA) text_point = (dlib.rectangle.left(d), dlib.rectangle.top(d) - 5) cv2.putText(img, candidate[num][0:4], text_point, cv2.FONT_HERSHEY_PLAIN, 2.0, (255, 255, 255), 1, 1) # 标出face cv2.putText(img, "facesNum: " + str(len(dets)), (20, 50), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 0, 0), 2, cv2.LINE_AA) else: # 没有检测到人脸 cv2.putText(img, "facesNum:0", (20, 50), cv2.FONT_HERSHEY_PLAIN, 1.5, (0, 0, 0), 2, cv2.LINE_AA) k = cv2.waitKey(1) # 按下 's' 键保存 if k == ord('s'): cnt += 1 print(path_screenshots + "screenshot" + "_" + str(cnt) + "_" + time.strftime("%Y-%m-%d-%H-%M-%S", time.localtime()) + ".jpg") cv2.imwrite(path_screenshots + "screenshot" + "_" + str(cnt) + "_" + time.strftime("%Y-%m-%d-%H-%M-%S", time.localtime()) + ".jpg", img) # 按下 'q' 键退出 if k == ord('q'): break cv2.imshow("camera", img)# 释放摄像头cap.release()cv2.destroyAllWindows() 补充 每次人脸库candidate-face中加入新的人脸数据，均需运行python candidate_train.py python facerec_68point.py检测的是与人脸库中最相似的 提供 this_is_who.py 进行在test-face文件夹中的批量测试，测试结果存于faceRec文件夹，识别错误结果存于faceRec_ERROR 最近的项目是在红外人脸图像上进行的，人脸不太清晰，如果是正常摄像头效果应该会更好运行结果python facerec_68point.py在单张上的测试结果：this_is_who.py在test-face文件夹中的批量测试结果：this_is_who_camera.py实时检测效果.py摄像头截图：项目地址 ：https://github.com/zj19941113/Face_Recognition_dlib]]></content>
      <categories>
        <category>算法</category>
        <category>人脸识别</category>
      </categories>
      <tags>
        <tag>dlib</tag>
        <tag>ubuntu</tag>
        <tag>人脸识别</tag>
        <tag>配置</tag>
        <tag>opencv</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人脸深度图活体检测_SVM（Ubuntu+opencv3）]]></title>
    <url>%2F2018%2F11%2F19%2F4%2F</url>
    <content type="text"><![CDATA[环境Ubuntu ，opencv3 数据准备原始深度图（已标定过人脸位置） 百度云盘：https://pan.baidu.com/s/1Hi85o521oIGaAfDoavOXeA使用MATLAB进行数据采集与处理： 1、运行dataGet_MATLAB/position_process.m，进行深度图片的人脸位置的快速批量标定，鼠标框出人脸位置，自动生成同名包含位置信息的txt文件。（百度云盘中已包含人脸位置信息，可跳过此步） 12345678910111213141516171819clcclearRAW_PATH = '/home/zhoujie/liveness detection/zjraw/face/';file =dir([RAW_PATH ,'*.raw']);for num=1:length(file)f1 = fopen([RAW_PATH,file(num).name], 'r');data0 = fread(f1, 'uint16');fclose(f1);img1 = reshape(data0, 400, 345);dep_img = img1';imshow(dep_img,[350,800]); mouse=imrect; pos=getPosition(mouse)% x1 y1 w h pos=round(pos);txtname = strrep(file(num).name,'.raw','.txt');fp=fopen([RAW_PATH,txtname],'a');fprintf(fp,'%i %i %i %i\n',pos); fclose(fp);end 2、运行dataGet_MATLAB/faceGet_process.m，进行人脸深度图的批量处理 ，需提前新建/data/face与/data/non-face空文件夹用来存放生成的正负训练样本 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657clcclearRAW_PATH = '/home/zhoujie/liveness detection/zjraw/face/';jpg_path = '/home/zhoujie/liveness detection/svm/data/face/';file =dir([RAW_PATH ,'*.raw']);for j=1:length(file) try f1 = fopen([RAW_PATH,file(j).name], 'r'); txtname = strrep(file(j).name,'.raw','.txt'); [par1,par2,par3,par4] = textread([RAW_PATH,txtname],'%d%d%d%d',1); data0 = fread(f1, 'uint16'); fclose(f1); img1 = reshape(data0, 400, 345); dep_img = img1'; try face = dep_img(par2 :par2 +par4-2,par1 :par1 + par3-2); catch if par2 +par4-2 &gt;345 face_height = 345; else face_height = par2 +par4-2; end if par1 +par3-2 &gt;400 face_weight = 400; else face_weight = par1 +par3-2; end face = dep_img(par2 :face_height,par1 :face_weight); end [m,n]=size(face); faceData = reshape(face, 1, m*n); faceData(find(faceData==0))=[]; able = 0;total = 0; for i =1:1000 num = randperm(length(faceData),1); facePlane = faceData(num); distance = abs([-1,faceData(num)]*[faceData;ones(1,length(faceData))]); total=sum(distance&lt;30); if total&gt;able able=total; bestfacePlane=facePlane; end end xmax = bestfacePlane+50; xmin = bestfacePlane-50; face(find(face &gt; xmax ))=xmax; face(find(face &lt; xmin ))=xmin; ymax=255;ymin=0; OutImg = round((ymax-ymin)*(face-xmin)/(xmax-xmin) + ymin); %归一化并取整 Outface=uint8(OutImg); Outface = imresize(Outface, [40 40]); jpgname = [jpg_path,num2str(j+538),'.jpg']; imwrite(Outface,jpgname); catch disp(file(j).name) endend 新建文件夹data/train_image/1、data/train_image/0、data/test_image/1、data/test_image/0data/face文件夹中的五分之四复制到data/train_image/1，剩下的复制到data/test_image/1data/non-face文件夹中的五分之四复制到data/train_image/0，剩下的复制到data/test_image/0 模型训练123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103#include &lt;iostream&gt; #include &lt;string.h&gt;#include&lt;time.h&gt;#include &lt;opencv2/highgui/highgui.hpp&gt; #include &lt;opencv2/ml/ml.hpp&gt; #include &lt;dirent.h&gt;using namespace std; using namespace cv;using namespace cv::ml;void getFiles( string path, vector&lt;string&gt;&amp; files); void get_num(int num, Mat&amp; trainingImages, vector&lt;int&gt;&amp; trainingLabels); int main() &#123; //获取训练数据 Mat classes; Mat trainingData; Mat trainingImages; vector&lt;int&gt; trainingLabels; get_num(0, trainingImages, trainingLabels); get_num(1, trainingImages, trainingLabels); Mat(trainingImages).copyTo(trainingData); trainingData.convertTo(trainingData, CV_32FC1); Mat(trainingLabels).copyTo(classes); //配置SVM训练器参数 Ptr&lt;SVM&gt; svm = SVM::create(); svm-&gt;setType(SVM::C_SVC); svm-&gt;setKernel(SVM::LINEAR); Ptr&lt;TrainData&gt; tData =TrainData::create(trainingData, ROW_SAMPLE, classes); cout &lt;&lt; "SVM: start train ..." &lt;&lt; endl; clock_t start,finish; double totaltime; start=clock(); svm-&gt;trainAuto(tData); svm-&gt;save("svm.xml"); cout&lt;&lt;"SVM: TRAIN SUCCESS !"&lt;&lt;endl; finish=clock(); totaltime=(double)(finish-start)/CLOCKS_PER_SEC; cout&lt;&lt;"TRAIN TIME : "&lt;&lt;totaltime&lt;&lt;" S ！"&lt;&lt;endl; // getchar(); return 0; &#125; void getFiles( string path, vector&lt;string&gt;&amp; files ) &#123; DIR *dir; struct dirent *ptr; if ((dir=opendir(path.c_str())) == NULL) &#123; perror("Open path error..."); exit(1); &#125; while ((ptr=readdir(dir)) != NULL) &#123; if(strcmp(ptr-&gt;d_name,".")==0 || strcmp(ptr-&gt;d_name,"..")==0) ///current dir OR parrent dir continue; else if(ptr-&gt;d_type == 8) ///file &#123; files.push_back(ptr-&gt;d_name); &#125; else if(ptr-&gt;d_type == 10) ///link file &#123;continue; &#125; else if(ptr-&gt;d_type == 4) ///dir &#123; files.push_back(ptr-&gt;d_name); &#125; &#125; closedir(dir); sort(files.begin(), files.end());&#125;void get_num(int num, Mat&amp; trainingImages, vector&lt;int&gt;&amp; trainingLabels) &#123; string numpath = "/home/zhoujie/liveness detection/svm/data/train_image/"; char char_num[2]; sprintf(char_num,"%d",num); string str_num = char_num; string str = numpath + str_num; const char* filePath = str.data(); string base; vector&lt;string&gt; files; getFiles(filePath, files); int number = files.size(); for (int i = 0;i &lt; number;i++) &#123; // cout &lt;&lt; "*************************** n = " &lt;&lt; i &lt;&lt; " ************************************ "&lt;&lt; endl; base = str + "/" + files[i]; Mat SrcImage=imread(base.c_str()); SrcImage= SrcImage.reshape(1, 1); // cout &lt;&lt; SrcImage &lt;&lt; endl; trainingImages.push_back(SrcImage); trainingLabels.push_back(num); &#125; &#125; 运行 svm_train.cpp g++ svm_train.cpp `pkg-config –cflags –libs opencv` -o svm_train./ svm_train 生成的SVM模型存储在根目录的svm.xml中 模型测试12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485#include &lt;iostream&gt; #include &lt;opencv2/highgui/highgui.hpp&gt; #include &lt;opencv2/ml/ml.hpp&gt; #include &lt;string.h&gt;#include &lt;dirent.h&gt;using namespace std; using namespace cv; void getFiles( string path, vector&lt;string&gt;&amp; files ); int main() &#123; for (int num = 0; num &lt; 2; num ++) &#123; int response; int result = 0; float accuracy; string numpath = "/home/zhoujie/liveness detection/svm/data/test_image/"; char char_num[2]; sprintf(char_num,"%d",num); string str_num = char_num; string str = numpath + str_num; const char* filePath = str.data(); string base; vector&lt;string&gt; files; getFiles(filePath, files ); int number = files.size(); cout &lt;&lt;"文件夹"&lt;&lt; num &lt;&lt;" 共有测试图片 " &lt;&lt;number &lt;&lt;" 张"&lt;&lt; endl; Ptr&lt;ml::SVM&gt;svm = ml::SVM::load("svm.xml"); for (int i = 0;i &lt; number;i++) &#123; base = str + "/" + files[i]; Mat inMat = imread(base.c_str()); Mat p = inMat.reshape(1, 1); p.convertTo(p, CV_32FC1); response = (int)svm-&gt;predict(p); // 核心代码，将检测的图片的标签返回回来，结果保存在response中 // cout &lt;&lt; "识别的数字为：" &lt;&lt; response &lt;&lt; endl; if (response == num) &#123; result++; &#125; // else // &#123; // cout &lt;&lt; base.c_str() &lt;&lt; " ERROR ! " &lt;&lt; endl; // &#125; &#125; accuracy = result*1.0/number; cout &lt;&lt; "识别正确 " &lt;&lt; result &lt;&lt;" 张，准确率： "&lt;&lt; accuracy &lt;&lt; endl; &#125; return 0; &#125; void getFiles( string path, vector&lt;string&gt;&amp; files ) &#123; DIR *dir; struct dirent *ptr; if ((dir=opendir(path.c_str())) == NULL) &#123; perror("Open path error..."); exit(1); &#125; while ((ptr=readdir(dir)) != NULL) &#123; if(strcmp(ptr-&gt;d_name,".")==0 || strcmp(ptr-&gt;d_name,"..")==0) ///current dir OR parrent dir continue; else if(ptr-&gt;d_type == 8) ///file &#123; files.push_back(ptr-&gt;d_name); &#125; else if(ptr-&gt;d_type == 10) ///link file &#123; continue; &#125; else if(ptr-&gt;d_type == 4) ///dir &#123; files.push_back(ptr-&gt;d_name); &#125; &#125; closedir(dir); sort(files.begin(), files.end());&#125; 运行 svm_test.cpp g++ svm_test.cpp `pkg-config –cflags –libs opencv` -o svm_test./ svm_test 运行结果项目地址 ：https://github.com/zj19941113/FaceLivenessDetection_SVM]]></content>
      <categories>
        <category>活体检测</category>
      </categories>
      <tags>
        <tag>活体检测</tag>
        <tag>近红外人脸定位</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++ 时间类型及相互转换详解 time_t与tm]]></title>
    <url>%2F2018%2F10%2F25%2F3%2F</url>
    <content type="text"><![CDATA[时间常见的有两种存储方式:time_t :整数类型 用来存储从1970年到现在经过了多少秒tm :结构类型 把日期和时间以 C 结构的形式保存，tm 结构的定义如下： struct tm { int tm_sec; // 秒，正常范围从 0 到 59，但允许至 61 int tm_min; // 分，范围从 0 到 59 int tm_hour; // 小时，范围从 0 到 23 int tm_mday; // 一月中的第几天，范围从 1 到 31 int tm_mon; // 月，范围从 0 到 11 int tm_year; // 自 1900 年起的年数 int tm_wday; // 一周中的第几天，范围从 0 到 6，从星期日算起 int tm_yday; // 一年中的第几天，范围从 0 到 365，从 1 月 1 日算起 int tm_isdst; // 夏令时} 12345678#include &lt;time.h&gt;#include &lt;stdio.h&gt; int main() &#123; time_t timep; time(&amp;timep); /*获取time_t类型的当前时间*/ printf("%s", asctime(gmtime(&amp;timep))); return 0; &#125; 输出为UTC时间： Thu Oct 25 01:49:20 2018 用gmtime将time_t类型的时间转换为struct tm类型的时间，按没有经过时区转换的UTC时间，然后再用asctime转换为我们常见的格式 Thu Oct 25 01:49:20 2018 修改时间输出格式： 123456789101112131415#include &lt;stdio.h&gt; #include &lt;time.h&gt;int main()&#123; char *wday[] = &#123;"Sun", "Mon", "Tue", "Wed", "Thu", "Fri", "Sat"&#125;; time_t timep; struct tm *p; time(&amp;timep); /*获得time_t结构的时间，UTC时间*/ p = localtime(&amp;timep); /*转换为struct tm结构的本地时间*/ printf("%d/%d/%d ", 1900 + p-&gt;tm_year, 1 + p-&gt;tm_mon, p-&gt;tm_mday); printf("%s %d:%d:%d\n", wday[p-&gt;tm_wday], p-&gt;tm_hour, p-&gt;tm_min, p-&gt;tm_sec); return 0;&#125; 输出为本地时间： 2018/10/25 Thu 09:49:20 常用时间函数：time_t time(time_t* t);//取得从1970年1月1日至今的秒数 char *asctime(const struct tm* timeptr);//将结构中的信息转换为真实世界的时间，以字符串的形式显示 char *ctime(const time_t* timep);//将timep转换为真是世界的时间，以字符串显示 struct tm* gmtime(const time_t* timep);//将time_t表示的时间转换为没有经过时区转换的UTC时间，是一个struct tm结构指针 struct tm* localtime(const time_t* timep);//和gmtime类似，但是它是经过时区转换的时间 time_t mktime(struct tm* timeptr);//将struct tm 结构的时间转换为从1970年至今的秒数 C语言中，#include &lt; time.h &gt; time_t timep;time(&amp;timep); /* 获取 time_t 类型的当前时间 */ C++中，#include &lt; ctime &gt; time_t now = time(0); // 基于当前系统的当前日期/时间 转换关系如下图：]]></content>
      <categories>
        <category>算法语言</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>语法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++ 近红外人脸定位与深度图人脸活体检测（.raw深度图像）]]></title>
    <url>%2F2018%2F10%2F22%2F2%2F</url>
    <content type="text"><![CDATA[通过深度相机的红外图进行人脸位置定位，传给深度图进行活体检测注：利用rgb传来的位置对应到深度图上偏差过大，直接利用深度相机产生的红外图像进行人脸位置的确定更加精准。 运行dlib_test.cpp 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188#include &lt;dlib/image_processing/frontal_face_detector.h&gt;#include &lt;dlib/gui_widgets.h&gt;#include &lt;dlib/image_io.h&gt;#include &lt;iostream&gt;#include &lt;time.h&gt;#include &lt;stdio.h&gt;#include &lt;math.h&gt;#include &lt;stdbool.h&gt;#include &lt;stdlib.h&gt;using namespace dlib;using namespace std;/* 函数声明 */int *face_location(char *imgFile);bool liveness_detection(char *DeepFile, int rec_face[4]); const int IMG_HEIGHT = 720;const int IMG_WIDTH = 1280;int main()&#123; char *imgFile = "/home/zhoujie/cProject/dlib_test/0001_IR_frontface.jpg"; int *rec_face; /* 调用函数得到人脸位置 */ // Eg:rec_face = &#123;157 ,66 ,172 ,198 &#125;, 行取66：66+198,列取157：157+172 rec_face = face_location(imgFile); //深度图与红外图是水平翻转的 rec_face[0] = IMG_WIDTH - rec_face[0] -rec_face[2]; cout &lt;&lt; rec_face[0] &lt;&lt; endl; cout &lt;&lt; rec_face[1] &lt;&lt; endl; cout &lt;&lt; rec_face[2] &lt;&lt; endl; cout &lt;&lt; rec_face[3] &lt;&lt; endl; char *DeepFile = "/home/zhoujie/cProject/dlib_test/raw_0001_frontface.raw"; bool IS_FACE; /* 调用函数判断是否为活体 */ IS_FACE = liveness_detection( DeepFile, rec_face); printf("RESULT : %d\n", IS_FACE); delete rec_face;&#125;/* 函数 输出人脸位置 */int *face_location(char* imgFile)&#123; int *rec_face = new int[4]; frontal_face_detector detector = get_frontal_face_detector(); cout &lt;&lt; "processing image " &lt;&lt; imgFile &lt;&lt; endl; clock_t start,finish; double totaltime; start=clock(); array2d&lt;unsigned char&gt; img; load_image(img, imgFile); std::vector&lt;rectangle&gt; dets = detector(img); cout &lt;&lt; "Number of faces detected: " &lt;&lt; dets.size() &lt;&lt; endl; rec_face[0] = dets[0].left(); rec_face[1] = dets[0].top(); rec_face[2] = dets[0].right() - dets[0].left() + 1; rec_face[3] = dets[0].bottom() - dets[0].top() + 1; finish=clock(); totaltime=(double)(finish-start)/CLOCKS_PER_SEC; cout&lt;&lt;"\n此程序的运行时间为"&lt;&lt;totaltime&lt;&lt;"秒！"&lt;&lt;endl; // delete rec_face; return rec_face;&#125;/* 函数判断是否为活体 */bool liveness_detection(char *DeepFile, int rec_face[4])&#123; const int ITER = 10000; // 随机取点次数 const float PLANE_OR_NOT = 0.1; // 判断是否为平面的分界线 const int SIGMA = 1; typedef unsigned short UNIT16; // 从.raw读取二进制16位数据到MatDATA UNIT16 MatDATA[IMG_HEIGHT*IMG_WIDTH]; FILE *fp = NULL; fp = fopen( DeepFile, "rb" ); fread(MatDATA,sizeof(UNIT16),IMG_HEIGHT*IMG_WIDTH,fp); fclose(fp); int n = 0; int i,j; int COL = rec_face[0],ROW = rec_face[1],FACE_WIDTH = rec_face[2],FACE_HEIGHT = rec_face[3]; //位置信息 // txt :157 66 172 198 , 取行66：66+198,列取157：157+172 int faceno0_num = FACE_HEIGHT*FACE_WIDTH -1; int FaceDATA[3][160000]; n = 0; for(i = 1;i&lt; FACE_HEIGHT+1;i++) &#123; for(j= 1;j&lt; FACE_WIDTH+1;j++) &#123; if (MatDATA[IMG_WIDTH*(ROW+i-2)+COL+j-2] == 0) &#123; faceno0_num -= 1; // 非零深度点个数为 faceno0_num+1 continue; &#125; FaceDATA[1][n] = i; FaceDATA[0][n] = j; FaceDATA[2][n] = MatDATA[IMG_WIDTH*(ROW+i-2)+COL+j-2]; n += 1; &#125; &#125; int pretotal = 0; // 符合拟合模型的数据的个数 int x[3],y[3],z[3]; // 随机取三个点 srand((unsigned)time(NULL)); float a,b,c; // 拟合平面方程 z=ax+by+c // float besta,bestb,bestc; // 最佳参数 int rand_num[3]; float check,distance; int total = 0; for(i = 0; i &lt; ITER; i++) &#123; do&#123; rand_num[0] = std::rand()%faceno0_num; rand_num[1] = std::rand()%faceno0_num; rand_num[2] = std::rand()%faceno0_num; &#125;while(rand_num[0] == rand_num[1] || rand_num[0] == rand_num[2] || rand_num[1] == rand_num[2]); for(n = 0; n &lt; 3; n++ ) &#123; x[n] = FaceDATA[0][rand_num[n]]; y[n] = FaceDATA[1][rand_num[n]]; z[n] = FaceDATA[2][rand_num[n]]; // printf("%d,%d,%d,%d\n", x[n],y[n],z[n],n); &#125; check = (x[0]-x[1])*(y[0]-y[2]) - (x[0]-x[2])*(y[0]-y[1]); if ( check == 0) // 防止提示浮点数例外 (核心已转储) &#123; i -= 1; continue; &#125; a = ( (z[0]-z[1])*(y[0]-y[2]) - (z[0]-z[2])*(y[0]-y[1]) )/( (x[0]-x[1])*(y[0]-y[2]) - (x[0]-x[2])*(y[0]-y[1]) ); if (y[0] == y[2]) // 防止提示浮点数例外 (核心已转储) &#123; i -= 1; continue; &#125; b = ((z[0] - z[2]) - a * (x[0] - x[2]))/(y[0]-y[2]); c = z[0]- a * x[0] - b * y[0]; // printf("%f,%f,%f\n",a,b,c); total = 0; for(n = 0; n &lt; faceno0_num +1 ; n++ ) &#123; distance = fabs(a*FaceDATA[0][n] + b*FaceDATA[1][n] - 1*FaceDATA[2][n] + c*1); if (distance &lt; SIGMA) &#123; total +=1; &#125; &#125; // printf("%d,%f,%d\n",i,distance,total); if (total &gt; pretotal) // 找到符合拟合平面数据最多的拟合平面 &#123; pretotal=total; &#125; &#125; float pretotal_ary = pretotal *1.0/ faceno0_num ; printf("%d,%f\n", pretotal,pretotal_ary); bool IS_FACE; if (pretotal_ary &lt; PLANE_OR_NOT) &#123; IS_FACE = true; &#125; else &#123; IS_FACE = false; &#125; return IS_FACE;&#125; Ubuntu下编译Dlib库参考这里 https://blog.csdn.net/ffcjjhv/article/details/84660869修改CMakeLists.txt相应路径cmake .make./dlib_test 运行结果项目地址：https://github.com/zj19941113/Face-Liveness_detection 活检部分原理Func_liveness_detection.c 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205#include &lt;stdio.h&gt;#include &lt;stdlib.h&gt;#include &lt;dirent.h&gt;#include &lt;string.h&gt;#include &lt;time.h&gt;#include &lt;math.h&gt;#include &lt;stdbool.h&gt;const int IMG_HEIGHT = 345;const int IMG_WIDTH = 400;const int ITER = 10000; // 随机取点次数const float PLANE_OR_NOT = 0.1; // 判断是否为平面的分界线typedef unsigned short UNIT16;int readFileList(char *basePath)&#123; DIR *dir; struct dirent *ptr; char base[8]; char title[4]; char *p=".raw"; //需要的子串; char *p2=".txt"; char *padd="/"; int len; char Deepfile_raw[100]; char Deepfile_txt[100]; UNIT16 MatDATA[IMG_HEIGHT*IMG_WIDTH]; FILE *fp = NULL; int DeepDATA[3][IMG_HEIGHT*IMG_WIDTH]; int length; int n; int i,j; FILE *fp2 = NULL; char buf[20]; char *ptr2; int rec_face[4]; int COL ,ROW ,FACE_WIDTH ,FACE_HEIGHT ; int FaceDATA[3][40000]; int faceno0_num ; int sigma = 1; int pretotal; // 符合拟合模型的数据的个数 int x[3],y[3],z[3]; // 随机取三个点 float a,b,c; // 拟合平面方程 z=ax+by+c // float besta,bestb,bestc; // 最佳参数 int rand_num[3]; float check,distance; int total; if ((dir=opendir(basePath)) == NULL) &#123; perror("Open dir error..."); exit(1); &#125; while ((ptr=readdir(dir)) != NULL) &#123; strcpy(base, ptr-&gt;d_name); if(strstr(base,p)) &#123; len = strlen(base); memset(title, '\0', sizeof(title)); strncpy(title, base, len -4); strcpy(Deepfile_raw, basePath); strcat(Deepfile_raw, padd); strcat(Deepfile_raw, title); strcpy(Deepfile_txt, Deepfile_raw); strcat(Deepfile_raw, p); strcat(Deepfile_txt, p2); // printf("%s\n",Deepfile_raw); // printf("%s\n",Deepfile_txt); // 从.raw读取二进制16位数据到MatDATA fp = fopen( Deepfile_raw, "rb" ); fread(MatDATA,sizeof(UNIT16),IMG_HEIGHT*IMG_WIDTH,fp); fclose(fp); // length = sizeof(MatDATA) / sizeof(UNIT16); // printf("数组的长度为: %d\n",length); //length 应为IMG_HEIGHT*IMG_WIDTH n = 0; // DeepDATA三行分别为深度图行数，列数，深度信息 for(i=1;i&lt; IMG_HEIGHT+1 ;i++) &#123; for(j=1;j&lt; IMG_WIDTH+1 ;j++) &#123; DeepDATA[0][n] = i; DeepDATA[1][n] = j; DeepDATA[2][n] = MatDATA[n]; n += 1; &#125; &#125; // int test1 = 110194 ; // printf("%d,%d,%d\n",DeepDATA[0][test1],DeepDATA[1][test1],DeepDATA[2][test1]); // FaceDATA为深度图DeepDATA裁剪后且去除零深度信息后的人脸部分 n = 0; fp2 = fopen(Deepfile_txt, "r"); fgets(buf, 20, fp2); // printf("%s\n", buf ); ptr2 = strtok(buf, " "); for(n = 0; n &lt; 4; n++) &#123; rec_face[n] = atoi(ptr2); ptr2 = strtok(NULL, " "); &#125; fclose(fp2); COL = rec_face[0],ROW = rec_face[1],FACE_WIDTH = rec_face[2],FACE_HEIGHT = rec_face[3]; //位置信息 // txt :157 66 172 198 , 取行66：66+198,列取157：157+172 faceno0_num = FACE_HEIGHT*FACE_WIDTH -1; n = 0; for(i = 1;i&lt; FACE_HEIGHT+1;i++) &#123; for(j= 1;j&lt; FACE_WIDTH+1;j++) &#123; if (MatDATA[IMG_WIDTH*(ROW+i-2)+COL+j-2] == 0) &#123; faceno0_num -= 1; // 非零深度点个数为 faceno0_num+1 continue; &#125; FaceDATA[1][n] = i; FaceDATA[0][n] = j; FaceDATA[2][n] = MatDATA[IMG_WIDTH*(ROW+i-2)+COL+j-2]; n += 1; &#125; &#125; // int test = 6804; // printf("%d,%d,%d,%d\n",test,FaceDATA[0][test],FaceDATA[1][test],FaceDATA[2][test]); srand((unsigned)time(NULL)); pretotal = 0; total = 0; for(i = 0; i &lt; ITER; i++) &#123; do&#123; rand_num[0] = rand()%faceno0_num; rand_num[1] = rand()%faceno0_num; rand_num[2] = rand()%faceno0_num; &#125;while(rand_num[0] == rand_num[1] || rand_num[0] == rand_num[2] || rand_num[1] == rand_num[2]); for(n = 0; n &lt; 3; n++ ) &#123; x[n] = FaceDATA[0][rand_num[n]]; y[n] = FaceDATA[1][rand_num[n]]; z[n] = FaceDATA[2][rand_num[n]]; // printf("%d,%d,%d,%d\n", x[n],y[n],z[n],n); &#125; check = (x[0]-x[1])*(y[0]-y[2]) - (x[0]-x[2])*(y[0]-y[1]); if ( check == 0) // 防止提示浮点数例外 (核心已转储) &#123; i -= 1; continue; &#125; a = ( (z[0]-z[1])*(y[0]-y[2]) - (z[0]-z[2])*(y[0]-y[1]) )*1.0/( (x[0]-x[1])*(y[0]-y[2]) - (x[0]-x[2])*(y[0]-y[1]) ); if (y[0] == y[2]) // 防止提示浮点数例外 (核心已转储) &#123; i -= 1; continue; &#125; b = ((z[0] - z[2]) - a * (x[0] - x[2]))*1.0/(y[0]-y[2]); c = z[0]- a * x[0] - b * y[0]; // printf("%f,%f,%f\n",a,b,c); total = 0; for(n = 0; n &lt; faceno0_num +1 ; n++ ) &#123; distance = fabs(a*FaceDATA[0][n] + b*FaceDATA[1][n] - 1*FaceDATA[2][n] + c*1); if (distance &lt; sigma) &#123; total +=1; &#125; &#125; // printf("%d,%f,%d\n",i,distance,total); if (total &gt; pretotal) // 找到符合拟合平面数据最多的拟合平面 &#123; pretotal=total; // besta = a; // bestb = b; // bestc = c; &#125; &#125; float pretotal_ary = pretotal *1.0/ faceno0_num ; printf("pretotal = %d,_ary = %f,",pretotal,pretotal_ary); printf("%s",base); bool IS_FACE; if (pretotal_ary&gt;PLANE_OR_NOT) &#123; IS_FACE = false; printf("不是人脸\n"); &#125; else &#123; IS_FACE = true; printf("是人脸\n"); &#125; &#125; &#125; closedir(dir);&#125;int main(void)&#123; DIR *dir; char *basePath = "/home/zhoujie/liveness detection/raw文件/non-face"; readFileList(basePath); return 0;&#125; github地址：https://github.com/zj19941113/Face-Liveness_detection上面这个项目使用的数据集标定的不太准，PLANE_OR_NOT参数选为 0.1 这个数据集是自己标定的，数量较少但是比较准确： https://pan.baidu.com/s/161xSbayGW7tKg0tKfTW1mw ，PLANE_OR_NOT参数选为 0.2 快速鼠标标定深度图人脸位置： https://blog.csdn.net/ffcjjhv/article/details/83270002]]></content>
      <categories>
        <category>活体检测</category>
      </categories>
      <tags>
        <tag>深度图</tag>
        <tag>活体检测</tag>
        <tag>C++</tag>
        <tag>近红外人脸定位</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++ warning!warning!warning!]]></title>
    <url>%2F2018%2F10%2F22%2F9%2F</url>
    <content type="text"><![CDATA[1、warning: ISO C++ forbids converting a string constant to ‘char*’ [-Wwrite-strings] char *imgFile = “data/IR_62_78_150_151.jpg”; 分析 ：char *背后的含义是：这个字符串，我要修改它。而传给函数的字面常量是没法被修改的。修正：把参数类型修改为const char * const char \*imgFile = &quot;data/IR_62_78_150_151.jpg&quot;; 2、warning: comparison between signed and unsigned integer expressions [-Wsign-compare]for (int j = 0; j &lt; shape.num_parts(); ++j) 分析：signed 和unsigned两种不同类型的比较，防止一个负的符号型的数据转化为无符号型时会产生一个很大的数据，signed 和unsigned数据的范围也不同。修正：使用size_t 类型。 for (std::size_t j = 0; j &lt; shape.num_parts(); ++j) 3、warning: unused variable ‘j’ [-Wunused-variable] int i,j;分析：代码写太长了，定义的j后面没用到。修正：删掉j。 int i; 4、warning: ignoring return value of ‘size_t fread(void, size_t, size_t, FILE)’, declared with attribute warn_unused_result [-Wunused-result] fread(MatDATA,sizeof(UNIT16),IMG_HEIGHT*IMG_WIDTH,fp);分析：使用fread(void*, size_t, size_t, FILE*)会返回一个size_t类型的值，假如命名为sizeRead，通过判断if (sizeRead != IMG_HEIGHT*IMG_WIDTH){printf(“error!\n”);}可以知道是否读取成功。修正：取返回值。 1234size_t sizeRead = fread(MatDATA,sizeof(UNIT16),IMG_HEIGHT*IMG_WIDTH,fp);if (sizeRead != IMG_HEIGHT*IMG_WIDTH) &#123; printf("error!\n");&#125; 5、warning: control reaches end of non-void function [-Wreturn-type]} 分析：控制到达非void函数的结尾。本应带有返回值的函数到达结尾后可能并没有返回任何值。修正：根据定义的类型加上返回值。return 0; } 6、error: stray ‘\240’ in program &amp;&amp; error: stray ‘\302’ in program分析：源代码中含有一些隐藏的非ascii字符。修正：将程序中出错行前面的空格删除，重新插入空格或tab键，重新编译，错误消失]]></content>
      <categories>
        <category>程序语言</category>
      </categories>
      <tags>
        <tag>C++</tag>
        <tag>warning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matlab 深度图人脸活体检测（.raw深度图像）]]></title>
    <url>%2F2018%2F10%2F22%2F1%2F</url>
    <content type="text"><![CDATA[github地址：https://github.com/zj19941113/Deep-Img-Liveness-Detection上面这个项目使用的数据集标定的不太准，PLANE_OR_NOT参数选为 0.1 这个数据集是自己标定的，数量较少但是比较准确： https://pan.baidu.com/s/161xSbayGW7tKg0tKfTW1mw ，PLANE_OR_NOT参数选为 0.2 快速鼠标标定深度图人脸位置： https://blog.csdn.net/ffcjjhv/article/details/83270002 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556RAW_PATH = '/home/zhoujie/liveness detection/zjraw/non-face/';PLANE_OR_NOT = 0.2; %根据标定位置的准确程度修改阈值ITER = 10000; %10000次结果基本已经稳定，不用修改file =dir([RAW_PATH ,'*.raw']);for num=1:length(file)f1 = fopen([RAW_PATH,file(num).name], 'r');txtname = strrep(file(num).name,'.raw','.txt');[par1,par2,par3,par4] = textread([RAW_PATH,txtname],'%d%d%d%d',1);data0 = fread(f1, 'uint16');fclose(f1);img1 = reshape(data0, 400, 345);dep_img = img1';% dep_img(find(dep_img &gt; 600))= 0;% figure(1),imshow(dep_img,[400,580]); face = dep_img(par2 :par2 +par4,par1 :par1 + par3); % figure(2),imshow(face,[400,580]); %%%三维平面拟合[X Y]=meshgrid(1:size(face,2),1:size(face,1));zz=face(:);xx=X(:);yy=Y(:);data=[xx';yy';zz'];id = data(3,:) == 0;data(:,id) = [];number = size(data,2); % 总点数sigma = 1;pretotal=0; %符合拟合模型的数据的个数for i=1:ITER %%% 随机选择三个点 idx = randperm(number,3); sample = data(:,idx); %%%拟合直线方程 z=ax+by+c plane = zeros(1,3); x = sample(1,:); y = sample(2,:); z = sample(3,:); a = ((z(1)-z(2))*(y(1)-y(3)) - (z(1)-z(3))*(y(1)-y(2)))/((x(1)-x(2))*(y(1)-y(3)) - (x(1)-x(3))*(y(1)-y(2))); b = ((z(1) - z(3)) - a * (x(1) - x(3)))/(y(1)-y(3)); c = z(1) - a * x(1) - b * y(1); plane = [a b -1 c]; mask=abs(plane*[data; ones(1,size(data,2))]); %求每个数据到拟合平面的距离 total=sum(mask&lt;sigma); %计算数据距离平面小于一定阈值的数据的个数 if total&gt;pretotal %找到符合拟合平面数据最多的拟合平面 pretotal=total;% bestplane=plane; %找到最好的拟合平面 end endpretotal_ary = pretotal/number;% potable(num) = pretotal_ary;if(pretotal_ary&gt;PLANE_OR_NOT) descrip = '不是人脸';elsedescrip = '是人脸'; enddisp(['pretotal=',num2str(pretotal),',','_ary=',num2str(pretotal_ary),',',file(num).name,descrip]);end]]></content>
      <categories>
        <category>活体检测</category>
      </categories>
      <tags>
        <tag>matlab</tag>
        <tag>深度图</tag>
        <tag>活体检测</tag>
        <tag>人脸</tag>
        <tag>raw</tag>
      </tags>
  </entry>
</search>
